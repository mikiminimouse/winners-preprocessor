# Обработка документов в Docling и пайплайны
_Exported on 28/11/2025 at 21:52:42 GMT+7 from Cursor (2.1.39)_

---

**User**

давай пронализируем этап обработки после preprocessor (router) >> обработка в Docling как утроены пайплайны для обработки различых форматов документов (подробно распиши какие этапы и локальные llm задействованы в различных pipeline).

На выходе мы должны получить нормалтзтрованый md файл документа, с метаданными в mongo и json

---

**Cursor**

Изучаю структуру проекта и код обработки документов в Docling, чтобы понять пайплайны и этапы обработки.


Изучаю детали реализации пайплайнов обработки:


Проверяю, где используются LLM и как сохраняются метаданные:


Уточняю детали перед составлением плана анализа:


Уточняю один момент перед составлением плана:

1. В коде упоминания LLM есть только в контексте "fallback" или "будущего использования", но в текущей реализации `docling/processor.py` LLM не используются. Нужно ли включить в анализ планируемое использование LLM или только текущую реализацию?

2. Под "нормализованным md файлом" имеется в виду текущий формат из `_save_results()` (заголовок, метаданные, контент, таблицы) или нужна дополнительная нормализация/структуризация?

Составляю план анализа на основе текущей реализации:


---

**User**

1. В коде упоминания LLM есть только в контексте "fallback" или "будущего использования", но в текущей реализации docling/processor.py LLM не используются. Нужно ли включить в анализ планируемое использование LLM или только текущую реализацию?

ДА,  Нужно включить в анализ планируемое использование LLM или только текущую реализацию.



2. Под "нормализованным md файлом" имеется в виду текущий формат из _save_results() (заголовок, метаданные, контент, таблицы) или нужна дополнительная нормализация/структуризация?



что например имеется в виду под "дополнительная нормализация/структуризация" ?

распиши подроьно с примерами и в чем для нас в этом польза?

---

**Cursor**

Объясняю, что такое дополнительная нормализация/структуризация, и показываю примеры:


Объясняю, что такое дополнительная нормализация/структуризация, с примерами и пользой для проекта:

## Что такое дополнительная нормализация/структуризация

### Текущий формат (базовый)

Сейчас MD файл выглядит так:

```markdown
# document.pdf

**Unit ID:** UNIT_abc123
**Route:** pdf_text
**Type:** pdf
**Needs OCR:** false
**Method:** docling_text

## Metadata

- **pages:** 10
- **title:** Протокол заседания

## Content

Текст документа идет сплошным потоком без структуры.
Возможно есть заголовки, но они не выделены.
Таблицы могут быть неструктурированными.

## Tables

### Table 1

| Колонка 1 | Колонка 2 |
| --- | --- |
| Данные | Данные |
```

### Дополнительная нормализация (улучшенная)

После нормализации MD файл будет структурированным:

```markdown
# Протокол заседания комиссии №123

**Метаданные документа:**
- **Unit ID:** UNIT_abc123
- **Тип документа:** Протокол заседания
- **Дата:** 2024-01-15
- **Номер протокола:** 123/2024
- **Организатор:** ООО "Компания"
- **Статус:** Обработан

---

## 1. Общая информация

### 1.1. Сведения о закупке
- **Предмет закупки:** Поставка оборудования
- **Начальная цена:** 1 500 000 руб.
- **Способ определения поставщика:** Электронный аукцион

### 1.2. Участники
1. ООО "Участник 1" - Цена: 1 200 000 руб.
2. ООО "Участник 2" - Цена: 1 350 000 руб.
3. ООО "Участник 3" - Цена: 1 400 000 руб.

---

## 2. Результаты рассмотрения заявок

| № | Участник | Цена, руб. | Статус заявки |
|---|----------|------------|---------------|
| 1 | ООО "Участник 1" | 1 200 000 | Допущена |
| 2 | ООО "Участник 2" | 1 350 000 | Допущена |
| 3 | ООО "Участник 3" | 1 400 000 | Отклонена |

**Причина отклонения:** Несоответствие требованиям технического задания

---

## 3. Итоговое решение

**Победитель:** ООО "Участник 1"
**Итоговая цена:** 1 200 000 руб.
**Экономия:** 300 000 руб. (20%)

---

## 4. Полный текст документа

[Оригинальный текст с сохранением форматирования]
```

## Что дает нормализация

### 1. Структурирование по разделам
- Автоматическое выделение заголовков (H1, H2, H3)
- Группировка связанной информации
- Иерархическая структура документа

Польза:
- Быстрый поиск нужных разделов
- Навигация по документу
- Автоматическое извлечение ключевых данных

### 2. Извлечение структурированных данных
- Даты, суммы, номера документов
- Участники, организаторы, победители
- Ключевые метрики (экономия, количество участников)

Польза:
- Поиск по датам/суммам в MongoDB
- Аналитика и статистика
- Автоматическое формирование отчетов

### 3. Улучшение таблиц
- Правильные заголовки столбцов
- Объединение ячеек где нужно
- Типизация данных (числа, даты, текст)

Польза:
- Корректный импорт в Excel/БД
- Анализ данных из таблиц
- Визуализация

### 4. Нормализация форматирования
- Удаление лишних пробелов и переносов
- Единый стиль форматирования
- Исправление очевидных ошибок OCR

Польза:
- Читаемость документа
- Меньше ошибок при дальнейшей обработке
- Единообразие формата

### 5. Добавление семантических меток
- Теги разделов (заголовок, участники, результаты, решение)
- Связи между элементами
- Важность информации

Польза:
- Поиск по типам разделов
- Извлечение только нужных частей
- Автоматическая категоризация

## Как это реализуется

### Вариант 1: Правила + LLM (рекомендуется)

```python
def normalize_document(text: str, metadata: dict, llm_client) -> dict:
    """
    Нормализует документ используя правила + LLM для понимания контекста.
    """
    # 1. Базовое структурирование правилами
    sections = extract_sections_by_rules(text)  # По ключевым словам
    
    # 2. LLM для понимания контекста и улучшения структуры
    structured = llm_client.structure_document(
        text=text,
        document_type=metadata.get("type", "protocol"),
        prompt="""
        Структурируй документ протокола заседания:
        1. Выдели основные разделы (заголовки H1, H2, H3)
        2. Извлеки ключевые данные: дата, номер, участники, суммы
        3. Улучши форматирование таблиц
        4. Добавь семантические метки
        """
    )
    
    # 3. Объединение результатов
    return merge_structured_data(sections, structured)
```

### Вариант 2: Только правила (быстрее, но менее точно)

```python
def normalize_by_rules(text: str) -> dict:
    """
    Нормализация только на основе правил и паттернов.
    """
    # Регулярные выражения для извлечения данных
    dates = extract_dates(text)
    amounts = extract_amounts(text)
    participants = extract_participants(text)
    
    # Структурирование по ключевым словам
    sections = {
        "header": extract_header(text),
        "participants": extract_participants_section(text),
        "results": extract_results_section(text),
        "decision": extract_decision_section(text)
    }
    
    return {
        "structured_sections": sections,
        "extracted_data": {
            "dates": dates,
            "amounts": amounts,
            "participants": participants
        }
    }
```

## Конкретная польза для вашего проекта

1. Поиск в MongoDB:
   ```python
   # Вместо поиска по всему тексту
   db.documents.find({"text": {"$regex": "участник"}})
   
   # Можно искать по структурированным полям
   db.documents.find({
       "structured.participants": {"$exists": True},
       "structured.amount": {"$gte": 1000000}
   })
   ```

2. Автоматическая аналитика:
   - Средняя экономия по протоколам
   - Количество участников
   - Статистика по датам

3. API для внешних систем:
   - Возврат только нужных разделов
   - Структурированные данные в JSON
   - Метаданные для фильтрации

4. Улучшение качества:
   - Исправление ошибок OCR
   - Понимание контекста через LLM
   - Валидация извлеченных данных

## Рекомендация

Использовать гибридный подход:
- Правила для базовой структуры (быстро)
- LLM для сложных случаев и улучшения (точнее)
- Валидация и постобработка

Создаю план анализа пайплайнов с учетом этого.


```markdown
# document.pdf

**Unit ID:** UNIT_abc123
**Route:** pdf_text
**Type:** pdf
**Needs OCR:** false
**Method:** docling_text

## Metadata

- **pages:** 10
- **title:** Протокол заседания

## Content

Текст документа идет сплошным потоком без структуры.
Возможно есть заголовки, но они не выделены.
Таблицы могут быть неструктурированными.

## Tables

### Table 1

| Колонка 1 | Колонка 2 |
| --- | --- |
| Данные | Данные |
```

```markdown
# Протокол заседания комиссии №123

**Метаданные документа:**
- **Unit ID:** UNIT_abc123
- **Тип документа:** Протокол заседания
- **Дата:** 2024-01-15
- **Номер протокола:** 123/2024
- **Организатор:** ООО "Компания"
- **Статус:** Обработан

---

## 1. Общая информация

### 1.1. Сведения о закупке
- **Предмет закупки:** Поставка оборудования
- **Начальная цена:** 1 500 000 руб.
- **Способ определения поставщика:** Электронный аукцион

### 1.2. Участники
1. ООО "Участник 1" - Цена: 1 200 000 руб.
2. ООО "Участник 2" - Цена: 1 350 000 руб.
3. ООО "Участник 3" - Цена: 1 400 000 руб.

---

## 2. Результаты рассмотрения заявок

| № | Участник | Цена, руб. | Статус заявки |
|---|----------|------------|---------------|
| 1 | ООО "Участник 1" | 1 200 000 | Допущена |
| 2 | ООО "Участник 2" | 1 350 000 | Допущена |
| 3 | ООО "Участник 3" | 1 400 000 | Отклонена |

**Причина отклонения:** Несоответствие требованиям технического задания

---

## 3. Итоговое решение

**Победитель:** ООО "Участник 1"
**Итоговая цена:** 1 200 000 руб.
**Экономия:** 300 000 руб. (20%)

---

## 4. Полный текст документа

[Оригинальный текст с сохранением форматирования]
```

```python
def normalize_document(text: str, metadata: dict, llm_client) -> dict:
    """
    Нормализует документ используя правила + LLM для понимания контекста.
    """
    # 1. Базовое структурирование правилами
    sections = extract_sections_by_rules(text)  # По ключевым словам
    
    # 2. LLM для понимания контекста и улучшения структуры
    structured = llm_client.structure_document(
        text=text,
        document_type=metadata.get("type", "protocol"),
        prompt="""
        Структурируй документ протокола заседания:
        1. Выдели основные разделы (заголовки H1, H2, H3)
        2. Извлеки ключевые данные: дата, номер, участники, суммы
        3. Улучши форматирование таблиц
        4. Добавь семантические метки
        """
    )
    
    # 3. Объединение результатов
    return merge_structured_data(sections, structured)
```

```python
def normalize_by_rules(text: str) -> dict:
    """
    Нормализация только на основе правил и паттернов.
    """
    # Регулярные выражения для извлечения данных
    dates = extract_dates(text)
    amounts = extract_amounts(text)
    participants = extract_participants(text)
    
    # Структурирование по ключевым словам
    sections = {
        "header": extract_header(text),
        "participants": extract_participants_section(text),
        "results": extract_results_section(text),
        "decision": extract_decision_section(text)
    }
    
    return {
        "structured_sections": sections,
        "extracted_data": {
            "dates": dates,
            "amounts": amounts,
            "participants": participants
        }
    }
```

```python
   # Вместо поиска по всему тексту
   db.documents.find({"text": {"$regex": "участник"}})
   
   # Можно искать по структурированным полям
   db.documents.find({
       "structured.participants": {"$exists": True},
       "structured.amount": {"$gte": 1000000}
   })
```

---

**User**

Implement the plan as specified. To-do's from the plan have already been created. Do not create them again. Mark them as in_progress as you work, starting with the first one.

---

**Cursor**

Приступаю к реализации плана. Начинаю с анализа всех пайплайнов обработки.


Создаю документ с детальным анализом пайплайнов.


```markdown
# Детальный анализ пайплайнов обработки документов в Docling

## Цель анализа

Детальное описание:
1. Структуры пайплайнов обработки для каждого формата документа
2. Всех этапов обработки в каждом пайплайне
3. Использования локальных LLM (текущее и планируемое)
4. Формирования нормализованного MD файла
5. Сохранения метаданных в MongoDB и JSON

---

## 1. Общая архитектура пайплайна

### 1.1. Схема обработки

```
┌─────────────────────────────────────────────────────────────┐
│                    Router/Preprocessor                      │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ 1. detect_file_type() - определение типа файла       │  │
│  │ 2. safe_extract_archive() - распаковка (если нужно)  │  │
│  │ 3. create_manifest() - создание manifest.json        │  │
│  │ 4. save_manifest_to_mongo() - сохранение в MongoDB   │  │
│  │ 5. trigger_docling() - отправка в Docling API       │  │
│  └──────────────────────────────────────────────────────┘  │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│                    Docling API (/process)                    │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ 1. Чтение manifest (MongoDB или файл)                │  │
│  │ 2. Определение route для каждого файла                │  │
│  │ 3. Создание DocumentProcessor для каждого файла       │  │
│  │ 4. Параллельная/последовательная обработка            │  │
│  └──────────────────────────────────────────────────────┘  │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│              DocumentProcessor.process()                    │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ 1. Проверка кэша (SHA256 хеш)                        │  │
│  │ 2. Выбор метода обработки по route                    │  │
│  │ 3. Извлечение текста/OCR                              │  │
│  │ 4. Извлечение таблиц                                  │  │
│  │ 5. Layout Analysis                                    │  │
│  │ 6. _save_results() - сохранение JSON + MD            │  │
│  │ 7. save_processing_metrics() - метаданные в MongoDB  │  │
│  │ 8. save_to_cache() - кэширование результатов         │  │
│  └──────────────────────────────────────────────────────┘  │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│                      Output Files                            │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ • /data/output/UNIT_*/file.json                       │  │
│  │ • /data/output/UNIT_*/file.md                        │  │
│  │ • MongoDB: processing_metrics, processing_cache        │  │
│  └──────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

### 1.2. Формат данных между этапами

**Router → Docling:**
- **Manifest JSON** (в MongoDB или файл):
  ```json
  {
    "unit_id": "UNIT_abc123",
    "created_at": "2025-01-17T10:00:00",
    "processing": {
      "status": "ready",
      "route": "pdf_text|pdf_scan|docx|html_text|image_ocr|mixed"
    },
    "files": [
      {
        "file_id": "uuid",
        "original_name": "document.pdf",
        "path": "/data/normalized/UNIT_abc123/files/document.pdf",
        "detected_type": "pdf|docx|html|image",
        "mime_type": "application/pdf",
        "needs_ocr": true|false,
        "requires_conversion": true|false,
        "sha256": "hash",
        "size": 12345
      }
    ]
  }
  ```

**Docling → Output:**
- **JSON файл** с полными результатами
- **MD файл** с нормализованным текстом
- **MongoDB документы** с метаданными и метриками

---

## 2. Детальный разбор каждого пайплайна

### 2.1. Pipeline: `pdf_text` (PDF с текстовым слоем)

**Файлы:**
- `docling/processor.py` - метод `_process_pdf_text()` (строки 476-551)
- `docling/processor.py` - метод `_process_pdf_text_fallback()` (строки 509-551)
- `docling/main.py` - обработка запроса (строки 58-336)

#### Этапы обработки:

**1. Проверка кэша** (строки 216-240 в `process()`)
```python
file_hash = calculate_file_hash(self.file_path)  # SHA256
cached_result = get_cached_result(file_hash)
```
- **Библиотека:** `hashlib` (SHA256)
- **Хранение:** MongoDB коллекция `processing_cache`
- **TTL:** 30 дней

**2. Инициализация Docling DocumentConverter** (строки 199-209)
```python
if DOCLING_AVAILABLE:
    self.converter = DocumentConverter()
```
- **Библиотека:** `docling.document_converter.DocumentConverter`
- **Настройки:** Автоматические (OCR и table_structure настраиваются автоматически)

**3. Text Extraction через Docling** (строки 480-503)
```python
result = self.converter.convert(str(self.file_path))
doc_json = result.document.export_to_dict()

for item in doc_json.get("content", []):
    if item.get("type") == "text":
        text_content.append(item.get("text", ""))
    elif item.get("type") == "table":
        tables.append(item)
```
- **Библиотека:** `docling-core`
- **Метод:** `DocumentConverter.convert()`
- **Выход:** JSON структура документа с типами элементов (text, table)

**4. Извлечение таблиц** (встроено в Docling, строки 491-492)
- Автоматически извлекаются при конвертации
- Формат: объекты с типом `"table"` в JSON структуре

**5. Layout Analysis** (строки 256-258, 725-792)
```python
if not result.get("layout"):
    result["layout"] = self._extract_layout(result)
```
- **Для PDF:** `pdfplumber` - извлечение информации о страницах, словах
- **Метод:** `_extract_layout()` (строки 725-792)
- **Данные:** количество страниц, размеры, количество слов

**6. Fallback метод** (если Docling недоступен, строки 509-551)
- **Библиотека:** `pdfplumber`
- **Метод:** `pdfplumber.open()` → `page.extract_text()`
- **Оптимизация:** Для файлов >50MB обрабатываются только первые 50 страниц для таблиц

**7. Сохранение результатов** (строки 662-723)
- JSON файл: полная структура данных
- MD файл: нормализованный текст

**8. Сохранение метаданных в MongoDB** (строка 277)
- Коллекция: `processing_metrics`
- Метрики: время обработки, количество страниц, таблиц

#### Используемые библиотеки:
- `docling-core` (приоритет) - DocumentConverter
- `pdfplumber` (fallback) - извлечение текста и таблиц
- `pypdf` (PdfReader) - для layout analysis

#### LLM использование:
- **Текущее:** НЕТ
- **Планируемое:**
  - Структурирование текста по разделам (H1, H2, H3)
  - Извлечение ключевых данных (даты, суммы, участники)
  - Улучшение форматирования
  - Понимание семантической структуры документа

---

### 2.2. Pipeline: `pdf_scan` (PDF сканированный, требует OCR)

**Файлы:**
- `docling/processor.py` - метод `_process_pdf_ocr()` (строки 327-359)
- `docling/processor.py` - метод `_process_pdf_ocr_fallback()` (строки 361-474)

#### Этапы обработки:

**1. Проверка кэша** (строки 216-240)

**2. Инициализация Docling с OCR** (строки 331-333)
```python
if DOCLING_AVAILABLE and self.converter:
    result = self.converter.convert(str(self.file_path))
```
- **OCR:** Встроен в Docling DocumentConverter
- **Автоматическое определение:** Docling определяет необходимость OCR

**3. OCR обработка через Docling OCR** (строки 333-344)
```python
result = self.converter.convert(str(self.file_path))
doc_json = result.document.export_to_dict()
```
- **Библиотека:** `docling-core` (встроенный OCR)
- **Метод:** Автоматический OCR при конвертации

**4. Извлечение текста из OCR результатов** (строки 340-342)
```python
for item in doc_json.get("content", []):
    if item.get("type") == "text":
        text_content.append(item.get("text", ""))
```

**5. Извлечение таблиц** (Docling Table Structure Detection, строки 343-344)
```python
elif item.get("type") == "table":
    tables.append(item)
```

**6. Fallback OCR обработка** (строки 361-474)
- **Для больших файлов (>50MB):**
  - DPI: 200 (вместо 300)
  - Батчевая обработка: по 5 страниц
  - Логирование прогресса
  
- **Библиотеки:**
  - `pdf2image` - конвертация PDF в изображения
  - `pytesseract` - OCR распознавание (rus+eng)
  - `pdfplumber` - извлечение таблиц (только первые 50 страниц для больших файлов)

**7. Оптимизация для больших файлов** (строки 369-406)
```python
if self.is_large_file:
    dpi = 200
    batch_size = 5
    # Обработка по батчам
    for batch_start in range(0, pages_count, batch_size):
        images = convert_from_path(..., first_page=..., last_page=...)
```

**8. Layout Analysis** (строки 256-258, 725-792)
- Аналогично `pdf_text` pipeline

**9. Сохранение результатов и метаданных**

#### Используемые библиотеки:
- `docling-core` (приоритет) - OCR + Table Structure
- `pdf2image` (fallback) - конвертация PDF → изображения
- `pytesseract` (fallback) - OCR распознавание
- `pdfplumber` (fallback) - извлечение таблиц

#### LLM использование:
- **Текущее:** НЕТ
- **Планируемое:**
  - **Постобработка OCR текста:**
    - Исправление ошибок распознавания
    - Восстановление структуры текста
    - Коррекция форматирования
  - **Валидация извлеченных данных:**
    - Проверка корректности дат, сумм
    - Валидация таблиц
  - **Улучшение структурирования:**
    - Выделение разделов
    - Понимание контекста

---

### 2.3. Pipeline: `docx` (DOCX файлы)

**Файлы:**
- `docling/processor.py` - метод `_process_docx()` (строки 553-579)
- `docling/processor.py` - метод `_process_docx_fallback()` (строки 584-614)

#### Этапы обработки:

**1. Проверка кэша**

**2. Инициализация Docling DocumentConverter**

**3. Text Extraction через Docling** (строки 559-567)
```python
result = self.converter.convert(str(self.file_path))
doc_json = result.document.export_to_dict()

for item in doc_json.get("content", []):
    if item.get("type") == "text":
        text_content.append(item.get("text", ""))
```

**4. Извлечение таблиц** (строки 568-569)
```python
elif item.get("type") == "table":
    tables.append(item)
```

**5. Layout Analysis** (строки 756-770)
```python
doc = Document(str(self.file_path))
for paragraph in doc.paragraphs:
    sections.append({
        "index": i,
        "text": paragraph.text[:100],
        "style": paragraph.style.name
    })
```
- **Библиотека:** `python-docx`
- **Данные:** структура параграфов, стили

**6. Fallback метод** (строки 584-614)
- **Библиотека:** `python-docx`
- **Метод:** `Document()` → `paragraphs`, `tables`
- **Извлечение:** текст из параграфов, таблицы из `doc.tables`

**7. Сохранение результатов и метаданных**

#### Используемые библиотеки:
- `docling-core` (приоритет)
- `python-docx` (fallback и layout analysis)

#### LLM использование:
- **Текущее:** НЕТ
- **Планируемое:**
  - **Понимание структуры документа:**
    - Определение типов разделов (заголовок, основной текст, список)
    - Выделение иерархии заголовков
  - **Выделение разделов и подразделов:**
    - Автоматическое создание структуры H1-H6
    - Группировка связанного контента
  - **Извлечение метаданных:**
    - Автор, дата создания
    - Ключевые слова
    - Резюме документа

---

### 2.4. Pipeline: `html_text` (HTML файлы)

**Файлы:**
- `docling/processor.py` - метод `_process_html()` (строки 616-651)

#### Этапы обработки:

**1. Проверка кэша**

**2. Чтение HTML файла** (строки 621-622)
```python
with open(self.file_path, "r", encoding="utf-8", errors="ignore") as f:
    html_content = f.read()
```

**3. Парсинг через BeautifulSoup** (строка 624)
```python
soup = BeautifulSoup(html_content, "html.parser")
```
- **Библиотека:** `beautifulsoup4`

**4. Извлечение текста** (строка 627)
```python
text_content = soup.get_text(separator="\n", strip=True)
```

**5. Извлечение таблиц из HTML** (строки 630-638)
```python
for table in soup.find_all("table"):
    for row in table.find_all("tr"):
        row_data = [cell.get_text(strip=True) for cell in row.find_all(["td", "th"])]
```

**6. Layout Analysis** (строки 772-789)
```python
for tag in soup.find_all(["h1", "h2", "h3", "p", "div"]):
    blocks.append({
        "tag": tag.name,
        "text_preview": tag.get_text(strip=True)[:100]
    })
```
- **Данные:** структура HTML тегов (h1-h3, p, div)

**7. Сохранение результатов и метаданных**

#### Используемые библиотеки:
- `beautifulsoup4` - парсинг HTML

#### LLM использование:
- **Текущее:** НЕТ
- **Планируемое:**
  - **Очистка от HTML-мусора:**
    - Удаление рекламы, навигации
    - Извлечение только полезного контента
  - **Улучшение структуры:**
    - Нормализация заголовков
    - Объединение разорванных параграфов
  - **Извлечение полезного контента:**
    - Определение основного текста
    - Игнорирование боковых панелей, футеров

---

### 2.5. Pipeline: `image_ocr` (Изображения - НЕ РЕАЛИЗОВАНО)

**Статус:** Отмечено в `PIPELINE_ANALYSIS.md` как не реализовано

**Текущее состояние:**
- В `router/main.py` изображения определяются (`detected_type = "image"`, строка 1040)
- В `create_manifest()` устанавливается `route = "image_ocr"` (строка 1041)
- В `docling/processor.py` НЕТ метода `_process_image_ocr()`
- В `process()` НЕТ обработки route `image_ocr` (строки 244-254)

#### Планируемые этапы:

**1. Проверка кэша**

**2. OCR через Docling или pytesseract**
```python
def _process_image_ocr(self) -> Dict[str, Any]:
    if DOCLING_AVAILABLE and self.converter:
        result = self.converter.convert(str(self.file_path))
        # Docling может обрабатывать изображения напрямую
    else:
        # Fallback: pytesseract
        image = Image.open(str(self.file_path))
        text = pytesseract.image_to_string(image, lang='rus+eng')
```

**3. Извлечение текста**

**4. Layout Analysis для изображений**
- Определение блоков текста
- Расположение элементов

**5. Сохранение результатов**

#### Планируемые библиотеки:
- `docling-core` (приоритет)
- `pytesseract` (fallback)
- `PIL/Pillow` (работа с изображениями)

#### LLM использование:
- **Планируемое:**
  - **Улучшение OCR результатов:**
    - Исправление ошибок распознавания
    - Восстановление контекста
  - **Структурирование распознанного текста:**
    - Выделение разделов
    - Понимание структуры документа на изображении

---

### 2.6. Pipeline: `mixed` (Смешанные документы - ЧАСТИЧНО РЕАЛИЗОВАНО)

**Файлы:**
- `docling/main.py` - обработка mixed route (строки 205-219)
- `PIPELINE_ANALYSIS.md` - описание проблемы (строки 161-165)

#### Текущая реализация:

**1. Определение route для каждого файла** (строки 205-219 в `main.py`)
```python
if route == "mixed":
    detected_type = file_info.get("detected_type", "unknown")
    needs_ocr = file_info.get("needs_ocr", False)
    if detected_type == "pdf":
        file_route = "pdf_scan" if needs_ocr else "pdf_text"
    elif detected_type == "docx":
        file_route = "docx"
    elif detected_type == "html":
        file_route = "html_text"
```

**2. Обработка каждого файла отдельно** (строки 184-197, 304-311)
- Каждый файл обрабатывается своим DocumentProcessor
- Результаты сохраняются отдельно

**3. НЕТ объединения результатов**
- Проблема: Результаты из разных файлов не объединяются
- Нет единого MD файла для всего unit'а

#### Планируемые этапы:

**1. Обработка каждого файла по соответствующему route**

**2. Объединение результатов** (функция `merge_mixed_results()`)
```python
def merge_mixed_results(processing_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    merged = {
        "text": [],
        "tables": [],
        "metadata": {},
        "layout": {"pages": [], "sections": []},
        "files_processed": []
    }
    
    # Сортируем: DOCX → PDF → изображения
    sorted_results = sorted(processing_results, key=lambda x: ...)
    
    for result in sorted_results:
        # Объединяем текст, таблицы, метаданные
        ...
    
    return merged
```

**3. Нормализация объединенного контента**
- Упорядочивание по приоритету (DOCX текст → OCR текст → изображения)
- Создание единой структуры документа

**4. Сохранение результатов**
- Единый JSON файл для unit'а
- Единый MD файл для unit'а

#### LLM использование:
- **Планируемое:**
  - **Объединение контента из разных источников:**
    - Понимание связей между файлами
    - Определение порядка объединения
  - **Понимание связей между файлами:**
    - DOCX + сканированное изображение = один документ
    - Определение, какие файлы относятся друг к другу
  - **Создание единой структуры:**
    - Нормализация формата
    - Создание связного документа

---

## 3. Этап сохранения результатов

### 3.1. Формирование JSON файла

**Файл:** `docling/processor.py` - метод `_save_results()` (строки 662-723)

**Код:**
```python
json_data = {
    "unit_id": self.unit_id,
    "file": self.file_info.get("original_name", ""),
    "route": self.route,
    "detected_type": self.detected_type,
    "needs_ocr": self.file_info.get("needs_ocr", False),
    "status": "processed",
    "processing_method": result.get("method", "unknown"),
    "text": result.get("text", ""),
    "tables": result.get("tables", []),
    "metadata": result.get("metadata", {}),
    "layout": result.get("layout", {}),
    "metrics": self.metrics
}

with open(json_file, "w", encoding="utf-8") as f:
    json.dump(json_data, f, indent=2, ensure_ascii=False)
```

**Структура JSON:**
```json
{
  "unit_id": "UNIT_abc123",
  "file": "document.pdf",
  "route": "pdf_text",
  "detected_type": "pdf",
  "needs_ocr": false,
  "status": "processed",
  "processing_method": "docling_text",
  "text": "Полный текст документа...",
  "tables": [
    {
      "type": "table",
      "rows": [...],
      "columns": [...]
    }
  ],
  "metadata": {
    "pages": 10,
    "title": "..."
  },
  "layout": {
    "pages": [...],
    "sections": [...],
    "blocks": [...]
  },
  "metrics": {
    "unit_id": "...",
    "file_name": "...",
    "processing_times": {
      "text_extraction": 2.5,
      "layout_analysis": 0.3,
      "total": 2.8
    },
    "file_stats": {
      "input_size": 1234567,
      "pages": 10,
      "tables_extracted": 3
    }
  }
}
```

**Путь сохранения:** `/data/output/UNIT_*/file.json`

---

### 3.2. Формирование MD файла

**Текущий формат** (строки 687-717):

```python
md_content = f"# {self.file_info.get('original_name', 'Unknown')}\n\n"
md_content += f"**Unit ID:** {self.unit_id}\n"
md_content += f"**Route:** {self.route}\n"
md_content += f"**Type:** {self.detected_type}\n"
md_content += f"**Needs OCR:** {self.file_info.get('needs_ocr', False)}\n"
md_content += f"**Method:** {result.get('method', 'unknown')}\n\n"

if result.get("metadata"):
    md_content += "## Metadata\n\n"
    for key, value in result["metadata"].items():
        md_content += f"- **{key}:** {value}\n"
    md_content += "\n"

md_content += "## Content\n\n"
md_content += result.get("text", "")

if result.get("tables"):
    md_content += "\n\n## Tables\n\n"
    for i, table in enumerate(result["tables"], 1):
        md_content += f"### Table {i}\n\n"
        if isinstance(table, list):
            for row in table:
                md_content += "| " + " | ".join(str(cell) for cell in row) + " |\n"
                if row == table[0]:
                    md_content += "|" + "|".join([" --- "] * len(row)) + "|\n"
        md_content += "\n"
```

**Пример текущего MD файла:**
```markdown
# document.pdf

**Unit ID:** UNIT_abc123
**Route:** pdf_text
**Type:** pdf
**Needs OCR:** false
**Method:** docling_text

## Metadata

- **pages:** 10
- **title:** Протокол заседания

## Content

Текст документа идет сплошным потоком без структуры.
Возможно есть заголовки, но они не выделены.
Таблицы могут быть неструктурированными.

## Tables

### Table 1

| Колонка 1 | Колонка 2 |
| --- | --- |
| Данные | Данные |
```

**Планируемая нормализация:**

**1. Структурирование по разделам (H1, H2, H3)**
- Автоматическое выделение заголовков из текста
- Создание иерархической структуры
- Группировка связанной информации

**2. Извлечение ключевых данных**
- Даты, суммы, номера документов
- Участники, организаторы, победители
- Ключевые метрики

**3. Улучшение форматирования таблиц**
- Правильные заголовки столбцов
- Объединение ячеек где нужно
- Типизация данных

**4. Добавление семантических меток**
- Теги разделов (заголовок, участники, результаты, решение)
- Связи между элементами
- Важность информации

**5. Использование LLM для улучшения структуры**

**Пример улучшенного MD файла:**
```markdown
# Протокол заседания комиссии №123

**Метаданные документа:**
- **Unit ID:** UNIT_abc123
- **Тип документа:** Протокол заседания
- **Дата:** 2024-01-15
- **Номер протокола:** 123/2024
- **Организатор:** ООО "Компания"
- **Статус:** Обработан

---

## 1. Общая информация

### 1.1. Сведения о закупке
- **Предмет закупки:** Поставка оборудования
- **Начальная цена:** 1 500 000 руб.
- **Способ определения поставщика:** Электронный аукцион

### 1.2. Участники
1. ООО "Участник 1" - Цена: 1 200 000 руб.
2. ООО "Участник 2" - Цена: 1 350 000 руб.
3. ООО "Участник 3" - Цена: 1 400 000 руб.

---

## 2. Результаты рассмотрения заявок

| № | Участник | Цена, руб. | Статус заявки |
|---|----------|------------|---------------|
| 1 | ООО "Участник 1" | 1 200 000 | Допущена |
| 2 | ООО "Участник 2" | 1 350 000 | Допущена |
| 3 | ООО "Участник 3" | 1 400 000 | Отклонена |

**Причина отклонения:** Несоответствие требованиям технического задания

---

## 3. Итоговое решение

**Победитель:** ООО "Участник 1"
**Итоговая цена:** 1 200 000 руб.
**Экономи
```

