#!/usr/bin/env python3
import os
import sys
import json
import time
import base64
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from PIL import Image
import io

try:
    from openai import OpenAI
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False
    print("‚ö†Ô∏è  openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")

try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False
    print("‚ö†Ô∏è  pdf2image –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdf2image")
    print("   –¢–∞–∫–∂–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è: sudo apt-get install poppler-utils")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_KEY = "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"
BASE_URL = "https://92ad3238-81c6-4396-a02a-fb9cef99bce3.modelrun.inference.cloud.ru/v1"
MODEL_NAME = "qwen3-vl-8b-instruct"

# –ü—É—Ç–∏
NORMALIZED_DIR = Path("/root/winners_preprocessor/normalized")
OUTPUT_DIR = Path("/root/winners_preprocessor/output_qwen3_all_pages")
TEST_UNITS_FILE = Path("/root/winners_preprocessor/test_ocr_units_fixed.json")

class Qwen3AllPagesProcessor:
    def __init__(self):
        if not OPENAI_SDK_AVAILABLE:
            raise ImportError("openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        self.client = OpenAI(api_key=API_KEY, base_url=BASE_URL, timeout=120.0)
        self.model = MODEL_NAME
        OUTPUT_DIR.mkdir(exist_ok=True)
        self.metrics = {
            "total_units": 0,
            "successful_units": 0,
            "total_files": 0,
            "successful_files": 0,
            "total_processing_time": 0.0,
            "total_tokens_used": 0,
            "unit_results": []
        }
        self.winners_analysis = []

    def test_connection(self) -> bool:
        try:
            print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç"}],
                max_tokens=10,
                temperature=0.5
            )
            print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ! –û—Ç–≤–µ—Ç: {response.choices[0].message.content.strip()}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            return False

    def image_to_base64(self, image_path: Path) -> str:
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode('utf-8')

    def pdf_to_all_pages_images_base64(self, pdf_path: Path, unit_output_dir: Path) -> List[str]:
        if not PDF2IMAGE_AVAILABLE:
            raise ImportError("pdf2image not installed")
        
        try:
            # Convert all pages of PDF to PIL images
            pil_images = convert_from_path(str(pdf_path), dpi=200)
            if not pil_images:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
            
            base64_images = []
            for i, img in enumerate(pil_images):
                # Save image to disk
                image_filename = f"{pdf_path.stem}_page_{i+1}.png"
                image_path = unit_output_dir / image_filename
                img.save(image_path, format='PNG')
                print(f"         üñºÔ∏è  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {i+1}: {image_path.name}")
                
                # Convert PIL image to base64
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                base64_img = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
                base64_images.append(base64_img)
                
            return base64_images
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            raise

    def create_prompt(self, file_type: str, page_number: Optional[int] = None) -> str:
        if file_type == "pdf":
            page_info = f" (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number})" if page_number else ""
            return f"""
            –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É{page_info} PDF-–¥–æ–∫—É–º–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–º –∑–∞–∫—É–ø–∫–∏.
            –ò–∑–≤–ª–µ–∫–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ.
            –ï—Å–ª–∏ –ø–æ–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —É–∫–∞–∂–∏—Ç–µ null.
            
            –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:
            - "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "32515314610-01")
            - "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–õ–æ—Ç 1", "1", "null" –µ—Å–ª–∏ –Ω–µ—Ç)
            - "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è)
            - "–ò–ù–ù": string (–ò–ù–ù –ø–æ–±–µ–¥–∏—Ç–µ–ª—è, —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
            - "–ö–ü–ü": string (–ö–ü–ü –ø–æ–±–µ–¥–∏—Ç–µ–ª—è, —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
            - "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": string (—Å—É–º–º–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, "10 025.00")
            - "–≤–∞–ª—é—Ç–∞": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "RUB", "—Ä—É–±.")
            - "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": string (–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞ –∑–∞–∫—É–ø–∫–∏)
            - "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–∑–∞–∫–∞–∑—á–∏–∫": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞)
            - "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞)
            - "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": array of strings (—Å–ø–∏—Å–æ–∫ –§–ò–û —á–ª–µ–Ω–æ–≤ –∫–æ–º–∏—Å—Å–∏–∏)
            - "—É—á–∞—Å—Ç–Ω–∏–∫–∏": array of objects (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–±–æ –≤—Å–µ—Ö —É—á–∞—Å—Ç–Ω–∏–∫–∞—Ö –∑–∞–∫—É–ø–∫–∏)
              –ö–∞–∂–¥—ã–π —É—á–∞—Å—Ç–Ω–∏–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
              {{
                "–Ω–æ–º–µ—Ä_–∑–∞—è–≤–∫–∏": string,
                "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": string,
                "—Å—É–º–º–∞_–±–µ–∑_–Ω–¥—Å": string,
                "—Å—É–º–º–∞_—Å_–Ω–¥—Å": string,
                "—Å—Ç–∞—Ç—É—Å": string
              }}
            
            –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ, –∏–∑–≤–ª–µ–∫–∏—Ç–µ –ø–æ–ª–Ω—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã.
            –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ:
            {{
                "text": "–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "tables": [
                    {{
                        "type": "table",
                        "rows": [["Header1", "Header2"], ["Value1", "Value2"]],
                        "bbox": [x1, y1, x2, y2]
                    }}
                ],
                "layout": {{
                    "pages": [
                        {{
                            "page_num": {page_number if page_number else 1},
                            "blocks": [
                                {{"type": "title", "text": "...", "bbox": [...]}}",
                                {{"type": "paragraph", "text": "...", "bbox": [...]}}",
                                {{"type": "table", "bbox": [...]}}"
                            ]
                        }}
                    ]
                }},
                "metadata": {{
                    "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": "...",
                    "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": "...",
                    "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": "...",
                    "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": "...",
                    "–ò–ù–ù": "...",
                    "–ö–ü–ü": "...",
                    "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": "...",
                    "–≤–∞–ª—é—Ç–∞": "...",
                    "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": "...",
                    "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": "...",
                    "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": "...",
                    "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "...",
                    "–∑–∞–∫–∞–∑—á–∏–∫": "...",
                    "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "...",
                    "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": ["...", "..."],
                    "—É—á–∞—Å—Ç–Ω–∏–∫–∏": [
                      {{
                        "–Ω–æ–º–µ—Ä_–∑–∞—è–≤–∫–∏": "...",
                        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": "...",
                        "—Å—É–º–º–∞_–±–µ–∑_–Ω–¥—Å": "...",
                        "—Å—É–º–º–∞_—Å_–Ω–¥—Å": "...",
                        "—Å—Ç–∞—Ç—É—Å": "..."
                      }}
                    ]
                }}
            }}
            """
        elif file_type == "image":
            return """
            –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —è–≤–ª—è–µ—Ç—Å—è —Å–∫–∞–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∑–∞–∫—É–ø–∫–∏).
            –ò–∑–≤–ª–µ–∫–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ.
            –ï—Å–ª–∏ –ø–æ–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —É–∫–∞–∂–∏—Ç–µ null.
            
            –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:
            - "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "32515314610-01")
            - "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–õ–æ—Ç 1", "1", "null" –µ—Å–ª–∏ –Ω–µ—Ç)
            - "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è)
            - "–ò–ù–ù": string (–ò–ù–ù –ø–æ–±–µ–¥–∏—Ç–µ–ª—è, —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
            - "–ö–ü–ü": string (–ö–ü–ü –ø–æ–±–µ–¥–∏—Ç–µ–ª—è, —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
            - "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": string (—Å—É–º–º–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, "10 025.00")
            - "–≤–∞–ª—é—Ç–∞": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "RUB", "—Ä—É–±.")
            - "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": string (–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞ –∑–∞–∫—É–ø–∫–∏)
            - "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–∑–∞–∫–∞–∑—á–∏–∫": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞)
            - "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞)
            - "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": array of strings (—Å–ø–∏—Å–æ–∫ –§–ò–û —á–ª–µ–Ω–æ–≤ –∫–æ–º–∏—Å—Å–∏–∏)
            - "—É—á–∞—Å—Ç–Ω–∏–∫–∏": array of objects (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–±–æ –≤—Å–µ—Ö —É—á–∞—Å—Ç–Ω–∏–∫–∞—Ö –∑–∞–∫—É–ø–∫–∏)
              –ö–∞–∂–¥—ã–π —É—á–∞—Å—Ç–Ω–∏–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
              {
                "–Ω–æ–º–µ—Ä_–∑–∞—è–≤–∫–∏": string,
                "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": string,
                "—Å—É–º–º–∞_–±–µ–∑_–Ω–¥—Å": string,
                "—Å—É–º–º–∞_—Å_–Ω–¥—Å": string,
                "—Å—Ç–∞—Ç—É—Å": string
              }
            
            –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ, –∏–∑–≤–ª–µ–∫–∏—Ç–µ –ø–æ–ª–Ω—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã.
            –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ:
            {
                "text": "–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "tables": [
                    {
                        "type": "table",
                        "rows": [["Header1", "Header2"], ["Value1", "Value2"]],
                        "bbox": [x1, y1, x2, y2]
                    }
                ],
                "layout": {
                    "pages": [
                        {
                            "page_num": 1,
                            "blocks": [
                                {"type": "title", "text": "...", "bbox": [...]},
                                {"type": "paragraph", "text": "...", "bbox": [...]},
                                {"type": "table", "bbox": [...]}
                            ]
                        }
                    ]
                },
                "metadata": {
                    "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": "...",
                    "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": "...",
                    "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": "...",
                    "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": "...",
                    "–ò–ù–ù": "...",
                    "–ö–ü–ü": "...",
                    "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": "...",
                    "–≤–∞–ª—é—Ç–∞": "...",
                    "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": "...",
                    "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": "...",
                    "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": "...",
                    "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "...",
                    "–∑–∞–∫–∞–∑—á–∏–∫": "...",
                    "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "...",
                    "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": ["...", "..."],
                    "—É—á–∞—Å—Ç–Ω–∏–∫–∏": [
                      {
                        "–Ω–æ–º–µ—Ä_–∑–∞—è–≤–∫–∏": "...",
                        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": "...",
                        "—Å—É–º–º–∞_–±–µ–∑_–Ω–¥—Å": "...",
                        "—Å—É–º–º–∞_—Å_–Ω–¥—Å": "...",
                        "—Å—Ç–∞—Ç—É—Å": "..."
                      }
                    ]
                }
            }
            """
        else:
            return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Ä–∞—Å–ø–æ–∑–Ω–∞–π—Ç–µ —Ä–∞–∑–º–µ—Ç–∫—É —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏—Ç–µ —Ç–µ–∫—Å—Ç, —Ç–∞–±–ª–∏—Ü—ã, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Docling JSON."

    def process_single_page(self, base64_image: str, prompt_text: str, page_num: int) -> Dict[str, Any]:
        """Process a single page/image with the Qwen3-VL model"""
        try:
            messages_content = [
                {"type": "text", "text": prompt_text},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
            ]
            
            print(f"      ‚û°Ô∏è  –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Qwen3-VL-8B –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}...")
            response_start_time = time.time()
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": messages_content}],
                max_tokens=5000,
                temperature=0.0, # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                response_format={"type": "json_object"} # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º JSON
            )
            response_time = time.time() - response_start_time
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            print(f"      ‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {response_time:.2f} —Å–µ–∫—É–Ω–¥")
            print(f"         –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response.choices[0].message.content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # Parse JSON response
            try:
                llm_output = json.loads(response.choices[0].message.content)
                print(f"      üì¶ –ü–∞—Ä—Å–∏–Ω–≥ JSON...")
                return {
                    "success": True,
                    "data": llm_output,
                    "response_time": response_time,
                    "tokens_used": tokens_used
                }
            except json.JSONDecodeError as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
                return {
                    "success": False,
                    "error": f"JSON Decode Error: {e}",
                    "raw_response": response.choices[0].message.content,
                    "response_time": response_time,
                    "tokens_used": tokens_used
                }
                
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def merge_page_results(self, page_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Merge results from multiple pages into a single document result"""
        if not page_results:
            return {}
        
        # Initialize merged result with data from first successful page
        merged = {
            "text": "",
            "tables": [],
            "layout": {"pages": []},
            "metadata": {},
            "processing_info": {
                "total_pages": len(page_results),
                "successful_pages": 0,
                "failed_pages": 0,
                "total_response_time": 0,
                "total_tokens_used": 0
            }
        }
        
        for i, result in enumerate(page_results):
            page_num = i + 1
            if result.get("success"):
                merged["processing_info"]["successful_pages"] += 1
                merged["processing_info"]["total_response_time"] += result.get("response_time", 0)
                merged["processing_info"]["total_tokens_used"] += result.get("tokens_used", 0)
                
                page_data = result.get("data", {})
                
                # Merge text
                if page_data.get("text"):
                    merged["text"] += f"\n\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} ---\n\n" + page_data.get("text", "")
                
                # Merge tables
                if page_data.get("tables"):
                    merged["tables"].extend(page_data.get("tables", []))
                
                # Merge layout
                if page_data.get("layout", {}).get("pages"):
                    merged["layout"]["pages"].extend(page_data["layout"]["pages"])
                else:
                    # Create a default page entry if none exists
                    merged["layout"]["pages"].append({
                        "page_num": page_num,
                        "blocks": []
                    })
                
                # Merge metadata (take from first page or merge if needed)
                if not merged["metadata"] and page_data.get("metadata"):
                    merged["metadata"] = page_data["metadata"]
                elif page_data.get("metadata"):
                    # Merge participants from all pages
                    if "—É—á–∞—Å—Ç–Ω–∏–∫–∏" in page_data["metadata"]:
                        if "—É—á–∞—Å—Ç–Ω–∏–∫–∏" not in merged["metadata"]:
                            merged["metadata"]["—É—á–∞—Å—Ç–Ω–∏–∫–∏"] = []
                        merged["metadata"]["—É—á–∞—Å—Ç–Ω–∏–∫–∏"].extend(page_data["metadata"]["—É—á–∞—Å—Ç–Ω–∏–∫–∏"])
            else:
                merged["processing_info"]["failed_pages"] += 1
        
        return merged

    def analyze_winners(self, merged_result: Dict[str, Any], unit_id: str) -> Dict[str, Any]:
        """Analyze winner information from the merged result"""
        metadata = merged_result.get("metadata", {})
        participants = metadata.get("—É—á–∞—Å—Ç–Ω–∏–∫–∏", [])
        
        winner_analysis = {
            "unit_id": unit_id,
            "winner_found": False,
            "winner_info": {},
            "total_participants": len(participants),
            "participants": participants,
            "procurement_info": {
                "procedure_number": metadata.get("–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã"),
                "lot_number": metadata.get("–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞"),
                "procurement_subject": metadata.get("–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏"),
                "protocol_date": metadata.get("–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞")
            }
        }
        
        # Check for explicit winner information
        winner_name = metadata.get("–ø–æ–±–µ–¥–∏—Ç–µ–ª—å")
        if winner_name:
            winner_analysis["winner_found"] = True
            winner_analysis["winner_info"] = {
                "name": winner_name,
                "inn": metadata.get("–ò–ù–ù"),
                "kpp": metadata.get("–ö–ü–ü"),
                "price": metadata.get("—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è"),
                "currency": metadata.get("–≤–∞–ª—é—Ç–∞")
            }
        else:
            # Try to determine winner from participants list
            # Look for participant with status indicating winner
            for participant in participants:
                status = participant.get("—Å—Ç–∞—Ç—É—Å", "").lower()
                if "–ø–æ–±–µ–¥" in status or "winner" in status or status == "–¥–æ–ø—É—â–µ–Ω":
                    winner_analysis["winner_found"] = True
                    winner_analysis["winner_info"] = {
                        "name": participant.get("–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"),
                        "inn": None,  # Would need to extract from participant data
                        "kpp": None,
                        "price": participant.get("—Å—É–º–º–∞_—Å_–Ω–¥—Å"),
                        "currency": "RUB"
                    }
                    break
        
        # Add to global winners analysis
        self.winners_analysis.append(winner_analysis)
        
        return winner_analysis

    def process_unit(self, unit_info: Dict[str, Any], unit_index: int, total_units: int) -> Optional[Dict[str, Any]]:
        unit_id = unit_info["unit_id"]
        unit_dir = Path(unit_info["unit_dir"])
        files_in_unit = unit_info["files"]
        
        unit_output_dir = OUTPUT_DIR / unit_id
        unit_output_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n{'='*80}")
        print(f"[{unit_index+1}/{total_units}] –û–±—Ä–∞–±–æ—Ç–∫–∞ UNIT: {unit_id}")
        print(f"{'='*80}")

        unit_start_time = time.time()
        unit_total_tokens = 0
        unit_successful_files = 0
        
        unit_result = {
            "unit_id": unit_id,
            "route": unit_info.get("route", "unknown"),
            "files_processed": [],
            "total_unit_time": 0.0,
            "total_unit_tokens": 0,
            "status": "failed",
            "error": None
        }

        for file_index, file_info in enumerate(files_in_unit):
            file_path = Path(file_info["path"])
            original_name = file_info["original_name"]
            detected_type = file_info["detected_type"]

            print(f"   üìÑ [{file_index+1}/{len(files_in_unit)}] –§–∞–π–ª: {original_name} ({detected_type})")

            file_start_time = time.time()
            file_tokens_used = 0
            
            try:
                if detected_type == "image":
                    print(f"\n      üì∑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {original_name}")
                    print(f"         –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / (1024*1024):.1f} MB")
                    base64_image = self.image_to_base64(file_path)
                    print(f"         –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ base64...")
                    print(f"         Base64 –¥–ª–∏–Ω–∞: {len(base64_image)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    prompt_text = self.create_prompt(detected_type)
                    page_result = self.process_single_page(base64_image, prompt_text, 1)
                    
                    if page_result.get("success"):
                        llm_output = page_result["data"]
                        file_tokens_used = page_result.get("tokens_used", 0)
                        self.metrics["total_tokens_used"] += file_tokens_used
                        
                        # Save results
                        output_filename = f"{file_path.stem}_qwen3_result.json"
                        output_path = unit_output_dir / output_filename
                        with open(output_path, "w", encoding="utf-8") as f:
                            json.dump(llm_output, f, indent=2, ensure_ascii=False)
                        print(f"      üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
                        
                        # Analyze winners
                        winner_analysis = self.analyze_winners(llm_output, unit_id)
                        print(f"      üèÜ –ü–æ–±–µ–¥–∏—Ç–µ–ª—å –Ω–∞–π–¥–µ–Ω: {'–î–∞' if winner_analysis['winner_found'] else '–ù–µ—Ç'}")
                        
                        unit_successful_files += 1
                        unit_result["files_processed"].append({
                            "file_name": original_name,
                            "detected_type": detected_type,
                            "status": "success",
                            "response_time": page_result.get("response_time", 0),
                            "tokens_used": file_tokens_used,
                            "output_path": str(output_path),
                            "winner_found": winner_analysis["winner_found"],
                            "winner_info": winner_analysis["winner_info"]
                        })
                    else:
                        unit_result["files_processed"].append({
                            "file_name": original_name,
                            "detected_type": detected_type,
                            "status": "failed",
                            "error": page_result.get("error", "Unknown error")
                        })
                        
                elif detected_type == "pdf":
                    print(f"\n      üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF: {original_name}")
                    print(f"         –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / 1024:.1f} KB")
                    if not PDF2IMAGE_AVAILABLE:
                        raise ImportError("pdf2image not installed")
                    
                    print(f"         –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
                    base64_images = self.pdf_to_all_pages_images_base64(file_path, unit_output_dir)
                    print(f"         –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(base64_images)}")
                    
                    # Process each page
                    page_results = []
                    for i, base64_image in enumerate(base64_images):
                        page_num = i + 1
                        print(f"         üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} –∏–∑ {len(base64_images)}")
                        prompt_text = self.create_prompt(detected_type, page_num)
                        page_result = self.process_single_page(base64_image, prompt_text, page_num)
                        page_results.append(page_result)
                        if page_result.get("success"):
                            file_tokens_used += page_result.get("tokens_used", 0)
                    
                    self.metrics["total_tokens_used"] += file_tokens_used
                    
                    # Merge results from all pages
                    merged_result = self.merge_page_results(page_results)
                    
                    # Save merged results
                    output_filename = f"{file_path.stem}_qwen3_merged_result.json"
                    output_path = unit_output_dir / output_filename
                    with open(output_path, "w", encoding="utf-8") as f:
                        json.dump(merged_result, f, indent=2, ensure_ascii=False)
                    print(f"      üíæ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
                    
                    # Analyze winners
                    winner_analysis = self.analyze_winners(merged_result, unit_id)
                    print(f"      üèÜ –ü–æ–±–µ–¥–∏—Ç–µ–ª—å –Ω–∞–π–¥–µ–Ω: {'–î–∞' if winner_analysis['winner_found'] else '–ù–µ—Ç'}")
                    
                    # Save individual page results for reference
                    page_results_path = unit_output_dir / f"{file_path.stem}_page_results.json"
                    with open(page_results_path, "w", encoding="utf-8") as f:
                        json.dump(page_results, f, indent=2, ensure_ascii=False)
                    print(f"      üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {page_results_path}")
                    
                    unit_successful_files += 1
                    unit_result["files_processed"].append({
                        "file_name": original_name,
                        "detected_type": detected_type,
                        "status": "success",
                        "total_pages": len(base64_images),
                        "successful_pages": merged_result.get("processing_info", {}).get("successful_pages", 0),
                        "failed_pages": merged_result.get("processing_info", {}).get("failed_pages", 0),
                        "total_response_time": merged_result.get("processing_info", {}).get("total_response_time", 0),
                        "tokens_used": file_tokens_used,
                        "output_path": str(output_path),
                        "winner_found": winner_analysis["winner_found"],
                        "winner_info": winner_analysis["winner_info"],
                        "total_participants": winner_analysis["total_participants"]
                    })
                    
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ (docx, html_text) –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º vision API
                    print(f"      ‚ùå –¢–∏–ø —Ñ–∞–π–ª–∞ '{detected_type}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è Vision API. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                    continue

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                unit_result["files_processed"].append({
                    "file_name": original_name,
                    "detected_type": detected_type,
                    "status": "failed",
                    "error": str(e)
                })
        
        unit_result["total_unit_time"] = time.time() - unit_start_time
        unit_result["total_unit_tokens"] = unit_total_tokens
        if unit_successful_files > 0:
            unit_result["status"] = "success"
            self.metrics["successful_units"] += 1
            self.metrics["successful_files"] += unit_successful_files
        
        self.metrics["total_units"] += 1
        self.metrics["total_files"] += len(files_in_unit)
        self.metrics["total_processing_time"] += unit_result["total_unit_time"]
        self.metrics["unit_results"].append(unit_result)
        
        return unit_result

    def generate_summary_report(self):
        print(f"\n{'='*80}")
        print(f"–ì–ï–ù–ï–†–ê–¶–ò–Ø –ú–ï–¢–†–ò–ö")
        print(f"{'='*80}")

        total_units = self.metrics["total_units"]
        successful_units = self.metrics["successful_units"]
        total_files = self.metrics["total_files"]
        successful_files = self.metrics["successful_files"]
        total_time = self.metrics["total_processing_time"]
        total_tokens = self.metrics["total_tokens_used"]

        avg_time_per_file = total_time / successful_files if successful_files > 0 else 0
        avg_tokens_per_file = total_tokens / successful_files if successful_files > 0 else 0

        # Count winners found
        winners_found = sum(1 for w in self.winners_analysis if w["winner_found"])
        total_analyzed = len(self.winners_analysis)

        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ: {successful_units}/{total_units} ({successful_units/total_units*100:.1f}%)")
        print(f"‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —É—Å–ø–µ—à–Ω—ã–π —Ñ–∞–π–ª: {avg_time_per_file:.2f} —Å–µ–∫")
        print(f"üî¢ –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {total_tokens}")
        print(f"üèÜ –ü–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –Ω–∞–π–¥–µ–Ω–æ: {winners_found}/{total_analyzed} ({winners_found/max(total_analyzed, 1)*100:.1f}%)")

        # Extrapolation
        estimated_100_units_time = avg_time_per_file * 100
        estimated_500_units_time = avg_time_per_file * 500

        print(f"üìà –û—Ü–µ–Ω–∫–∞ –¥–ª—è 100 UNIT'–æ–≤: {estimated_100_units_time / 60:.1f} –º–∏–Ω ({estimated_100_units_time / 3600:.2f} —á)")
        print(f"üìà –û—Ü–µ–Ω–∫–∞ –¥–ª—è 500 UNIT'–æ–≤: {estimated_500_units_time / 60:.1f} –º–∏–Ω ({estimated_500_units_time / 3600:.2f} —á)")

        metrics_summary = {
            "timestamp": datetime.now().isoformat(),
            "total_units_attempted": total_units,
            "successful_units": successful_units,
            "total_files_attempted": total_files,
            "successful_files": successful_files,
            "total_processing_time_seconds": total_time,
            "avg_time_per_successful_file_seconds": avg_time_per_file,
            "total_tokens_used": total_tokens,
            "avg_tokens_per_successful_file": avg_tokens_per_file,
            "winners_analysis": {
                "total_analyzed": total_analyzed,
                "winners_found": winners_found,
                "success_rate": winners_found/max(total_analyzed, 1)
            },
            "extrapolation": {
                "estimated_100_units_minutes": estimated_100_units_time / 60,
                "estimated_100_units_hours": estimated_100_units_time / 3600,
                "estimated_500_units_minutes": estimated_500_units_time / 60,
                "estimated_500_units_hours": estimated_500_units_time / 3600
            }
        }
        metrics_output_path = OUTPUT_DIR / f"metrics_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(metrics_output_path, "w", encoding="utf-8") as f:
            json.dump(metrics_summary, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_output_path}")

        return metrics_summary

    def generate_comparison_report(self):
        print(f"\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
        report_path = OUTPUT_DIR / f"comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"# –û—Ç—á–µ—Ç –æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ UNIT'–æ–≤ —á–µ—Ä–µ–∑ Qwen3-VL-8B (–≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)\n\n")
            f.write(f"**–î–∞—Ç–∞ –æ—Ç—á–µ—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n")
            f.write(f"- –í—Å–µ–≥–æ UNIT'–æ–≤ –≤ —Ç–µ—Å—Ç–µ: {self.metrics['total_units']}\n")
            f.write(f"- –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ UNIT'–æ–≤: {self.metrics['successful_units']}\n")
            f.write(f"- –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.metrics['total_processing_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
            f.write(f"- –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {self.metrics['total_tokens_used']}\n")
            f.write(f"- –ü–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –Ω–∞–π–¥–µ–Ω–æ: {sum(1 for w in self.winners_analysis if w['winner_found'])}/{len(self.winners_analysis)}\n\n")

            f.write(f"## –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ UNIT'–∞–º\n\n")
            for unit_result in self.metrics["unit_results"]:
                f.write(f"### UNIT ID: `{unit_result['unit_id']}`\n")
                f.write(f"- –°—Ç–∞—Ç—É—Å: **{unit_result['status'].upper()}**\n")
                f.write(f"- –ú–∞—Ä—à—Ä—É—Ç: `{unit_result['route']}`\n")
                f.write(f"- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ UNIT'–∞: {unit_result['total_unit_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
                if unit_result['error']:
                    f.write(f"- –û—à–∏–±–∫–∞: `{unit_result['error']}`\n")
                f.write(f"\n")

                for file_proc_result in unit_result["files_processed"]:
                    f.write(f"#### –§–∞–π–ª: `{file_proc_result['file_name']}`\n")
                    f.write(f"- –¢–∏–ø: `{file_proc_result['detected_type']}`\n")
                    f.write(f"- –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏: **{file_proc_result['status'].upper()}**\n")
                    if file_proc_result['status'] == 'success':
                        if file_proc_result['detected_type'] == 'pdf':
                            f.write(f"- –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {file_proc_result.get('total_pages', 'N/A')}\n")
                            f.write(f"- –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {file_proc_result.get('successful_pages', 'N/A')}\n")
                            f.write(f"- –ù–µ—É–¥–∞—á–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {file_proc_result.get('failed_pages', 'N/A')}\n")
                            f.write(f"- –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü: {file_proc_result.get('total_response_time', 0):.2f} —Å–µ–∫—É–Ω–¥\n")
                        else:
                            f.write(f"- –í—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞ –∫ API: {file_proc_result['response_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
                        
                        f.write(f"- –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {file_proc_result['tokens_used']}\n")
                        f.write(f"- –ü—É—Ç—å –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É JSON: `{file_proc_result['output_path']}`\n")
                        f.write(f"- –ü–æ–±–µ–¥–∏—Ç–µ–ª—å –Ω–∞–π–¥–µ–Ω: **{'–î–∞' if file_proc_result.get('winner_found', False) else '–ù–µ—Ç'}**\n")
                        
                        if file_proc_result.get('winner_found'):
                            winner_info = file_proc_result.get('winner_info', {})
                            f.write(f"- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ:\n")
                            f.write(f"  - –ù–∞–∑–≤–∞–Ω–∏–µ: {winner_info.get('name', 'N/A')}\n")
                            f.write(f"  - –ò–ù–ù: {winner_info.get('inn', 'N/A')}\n")
                            f.write(f"  - –ö–ü–ü: {winner_info.get('kpp', 'N/A')}\n")
                            f.write(f"  - –¶–µ–Ω–∞: {winner_info.get('price', 'N/A')} {winner_info.get('currency', 'N/A')}\n")
                        
                        # Original document info
                        original_file_path = NORMALIZED_DIR / unit_result['unit_id'] / "files" / file_proc_result['file_name']
                        if original_file_path.exists():
                            f.write(f"\n##### –ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç:\n")
                            f.write(f"```\n")
                            f.write(f"–ü—É—Ç—å: {original_file_path}\n")
                            f.write(f"–¢–∏–ø: {file_proc_result['detected_type']}\n")
                            f.write(f"–†–∞–∑–º–µ—Ä: {original_file_path.stat().st_size / 1024:.1f} KB\n")
                            f.write(f"```\n")
                        else:
                            f.write(f"\n##### –ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ `{original_file_path}`\n")

                        # LLM output preview
                        try:
                            with open(file_proc_result['output_path'], 'r', encoding='utf-8') as json_f:
                                llm_output = json.load(json_f)
                            
                            f.write(f"\n##### –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ Qwen3-VL-8B (Docling AST -> Markdown):\n")
                            f.write(f"```markdown\n")
                            f.write(f"# {llm_output.get('metadata', {}).get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')}\n\n")
                            f.write(f"**–î–∞—Ç–∞:** {llm_output.get('metadata', {}).get('date', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}\n\n")
                            f.write(f"## –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n")
                            text_preview = llm_output.get('text', '–¢–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω')
                            f.write(f"{text_preview[:1000]}{'...' if len(text_preview) > 1000 else ''}\n\n")
                            
                            if llm_output.get('tables'):
                                f.write(f"## –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã\n")
                                for table in llm_output['tables'][:2]:  # Limit to first 2 tables
                                    f.write(f"```\n")
                                    if 'rows' in table and table['rows']:
                                        header = table['rows'][0]
                                        body = table['rows'][1:3] if len(table['rows']) > 1 else table['rows'][1:2]  # Limit rows
                                        f.write("| " + " | ".join(header) + " |\n")
                                        f.write("|" + "---|".join(["---"] * len(header)) + "|\n")
                                        for row in body:
                                            f.write("| " + " | ".join(row) + " |\n")
                                    f.write("```\n\n")
                            
                            f.write(f"## –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n")
                            for key, value in llm_output.get('metadata', {}).items():
                                if key not in ['title', 'date']: # Already shown
                                    if key == '—É—á–∞—Å—Ç–Ω–∏–∫–∏':
                                        f.write(f"- **–£—á–∞—Å—Ç–Ω–∏–∫–∏:** {len(value) if isinstance(value, list) else 'N/A'}\n")
                                    else:
                                        f.write(f"- **{key.replace('_', ' ').capitalize()}:** {value}\n")
                            f.write(f"```\n")

                        except Exception as e:
                            f.write(f"\n##### –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏/—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ JSON: {e}\n")
                    else:
                        f.write(f"- –û—à–∏–±–∫–∞: `{file_proc_result['error']}`\n")
                    f.write(f"\n---\n\n")
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

    def generate_winners_analysis_report(self):
        """Generate a separate report focused on winners analysis"""
        print(f"\nüèÜ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π...")
        winners_report_path = OUTPUT_DIR / f"winners_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # Enrich winners analysis with more details
        enriched_analysis = []
        for analysis in self.winners_analysis:
            # Find corresponding unit result for more details
            unit_result = next((u for u in self.metrics["unit_results"] if u["unit_id"] == analysis["unit_id"]), None)
            if unit_result:
                analysis["unit_details"] = {
                    "route": unit_result.get("route"),
                    "processing_time": unit_result.get("total_unit_time"),
                    "status": unit_result.get("status")
                }
            enriched_analysis.append(analysis)
        
        with open(winners_report_path, "w", encoding="utf-8") as f:
            json.dump(enriched_analysis, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ –û—Ç—á–µ—Ç –ø–æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {winners_report_path}")
        
        # Also generate a summary markdown report
        winners_md_path = OUTPUT_DIR / f"winners_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(winners_md_path, "w", encoding="utf-8") as f:
            f.write(f"# –ê–Ω–∞–ª–∏–∑ –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –ø–æ –≥–æ—Å–∑–∞–∫—É–ø–∫–∞–º\n\n")
            f.write(f"**–î–∞—Ç–∞ –æ—Ç—á–µ—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"## –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n")
            f.write(f"- –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(enriched_analysis)}\n")
            f.write(f"- –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–º: {sum(1 for w in enriched_analysis if w['winner_found'])}\n")
            f.write(f"- –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π: {sum(1 for w in enriched_analysis if w['winner_found'])/max(len(enriched_analysis), 1)*100:.1f}%\n\n")
            
            f.write(f"## –î–æ–∫—É–º–µ–Ω—Ç—ã —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è–º–∏\n\n")
            for analysis in enriched_analysis:
                if analysis["winner_found"]:
                    f.write(f"### UNIT: `{analysis['unit_id']}`\n")
                    f.write(f"- –ù–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã: {analysis['procurement_info'].get('procedure_number', 'N/A')}\n")
                    f.write(f"- –ü—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏: {analysis['procurement_info'].get('procurement_subject', 'N/A')}\n")
                    f.write(f"- –î–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞: {analysis['procurement_info'].get('protocol_date', 'N/A')}\n")
                    f.write(f"- –ü–æ–±–µ–¥–∏—Ç–µ–ª—å: **{analysis['winner_info'].get('name', 'N/A')}**\n")
                    f.write(f"- –ò–ù–ù: {analysis['winner_info'].get('inn', 'N/A')}\n")
                    f.write(f"- –ö–ü–ü: {analysis['winner_info'].get('kpp', 'N/A')}\n")
                    f.write(f"- –¶–µ–Ω–∞: {analysis['winner_info'].get('price', 'N/A')} {analysis['winner_info'].get('currency', 'N/A')}\n")
                    f.write(f"- –í—Å–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {analysis['total_participants']}\n\n")
            
            f.write(f"## –î–æ–∫—É–º–µ–Ω—Ç—ã –±–µ–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è\n\n")
            for analysis in enriched_analysis:
                if not analysis["winner_found"]:
                    f.write(f"### UNIT: `{analysis['unit_id']}`\n")
                    f.write(f"- –ù–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã: {analysis['procurement_info'].get('procedure_number', 'N/A')}\n")
                    f.write(f"- –ü—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏: {analysis['procurement_info'].get('procurement_subject', 'N/A')}\n")
                    f.write(f"- –î–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞: {analysis['procurement_info'].get('protocol_date', 'N/A')}\n")
                    f.write(f"- –í—Å–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {analysis['total_participants']}\n")
                    f.write(f"- –°—Ç–∞—Ç—É—Å UNIT'–∞: {analysis.get('unit_details', {}).get('status', 'N/A')}\n\n")
        
        print(f"‚úÖ –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {winners_md_path}")

    def run(self):
        print(f"\n{'='*80}")
        print(f"–ú–ê–°–°–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê 10 UNIT'–û–í –ß–ï–†–ï–ó QWEN3-VL-8B (–í–°–ï –°–¢–†–ê–ù–ò–¶–´)")
        print(f"{'='*80}")

        if not self.test_connection():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å.")
            return

        try:
            with open(TEST_UNITS_FILE, "r", encoding="utf-8") as f:
                test_units_data = json.load(f)
                # Use only first 10 units instead of all 20
                self.test_units = test_units_data["units"][:10]
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º UNIT'–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {TEST_UNITS_FILE}")
            print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ collect_ocr_units.py –¥–ª—è —Å–±–æ—Ä–∞ UNIT'–æ–≤.")
            return

        print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ UNIT'–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(self.test_units)}")
        print(f"üéØ –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.test_units)} UNIT'–æ–≤")

        for i, unit_info in enumerate(self.test_units):
            self.process_unit(unit_info, i, len(self.test_units))
        
        self.generate_summary_report()
        self.generate_comparison_report()
        self.generate_winners_analysis_report()

        print(f"\n{'='*80}")
        print(f"‚úÖ –ú–ê–°–°–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        print(f"{'='*80}")

if __name__ == "__main__":
    processor = Qwen3AllPagesProcessor()
    processor.run()

