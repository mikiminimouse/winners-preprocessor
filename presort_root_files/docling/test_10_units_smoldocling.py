#!/usr/bin/env python3
import os
import sys
import json
import time
import base64
import re
import requests
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from PIL import Image
import io

try:
    import openai
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False
    print("‚ö†Ô∏è  openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")

try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False
    print("‚ö†Ô∏è  pdf2image –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdf2image")
    print("   –¢–∞–∫–∂–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è: sudo apt-get install poppler-utils")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è SmolDocling (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω)
API_TOKEN = "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"
BASE_URL = "https://d63e30af-085a-49f0-9724-8162da967af2.modelrun.inference.cloud.ru/v1"
MODEL_NAME = "model-run-4qigw-disease"  # –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è SmolDocling

# –ü—É—Ç–∏
NORMALIZED_DIR = Path("/root/winners_preprocessor/normalized")
OUTPUT_DIR = Path("/root/winners_preprocessor/output_smoldocling")
TEST_UNITS_FILE = Path("/root/winners_preprocessor/test_ocr_units_fixed.json")

class SmolDoclingProcessor:
    def __init__(self):
        if not OPENAI_SDK_AVAILABLE:
            raise ImportError("openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenAI –∫–ª–∏–µ–Ω—Ç —Å –Ω–∞—à–∏–º —Ç–æ–∫–µ–Ω–æ–º
        self.client = openai.OpenAI(
            api_key=API_TOKEN,
            base_url=BASE_URL,
            timeout=120.0
        )
        self.model = MODEL_NAME
        OUTPUT_DIR.mkdir(exist_ok=True)
        self.metrics = {
            "total_units": 0,
            "successful_units": 0,
            "total_files": 0,
            "successful_files": 0,
            "total_processing_time": 0.0,
            "total_tokens_used": 0,
            "unit_results": []
        }
        self.winners_analysis = []

    def wait_for_server_ready(self, max_wait_time: int = 300) -> bool:
        """Wait for the inference server to be ready"""
        print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞ SmolDocling (–º–∞–∫—Å–∏–º—É–º {max_wait_time} —Å–µ–∫—É–Ω–¥)...")
        
        start_time = time.time()
        while time.time() - start_time < max_wait_time:
            try:
                # Send a wake-up request to the server
                print("    –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø—Ä–æ–±—É–∂–¥–µ–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞...")
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": "Hello, wake up!"}],
                    max_tokens=10,
                    temperature=0.5
                )
                
                if response.choices[0].message.content:
                    print("‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
                    return True
            except Exception as e:
                print(f"    –°–µ—Ä–≤–µ—Ä –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤... ({str(e)[:50]}...)")
                time.sleep(15)  # Wait 15 seconds before retrying
        
        print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ —Å—Ç–∞–ª –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏")
        return False

    def test_connection(self) -> bool:
        try:
            print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ SmolDocling...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç"}],
                max_tokens=10,
                temperature=0.5
            )
            print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ! –û—Ç–≤–µ—Ç: {response.choices[0].message.content.strip()}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            return False

    def image_to_base64(self, image_path: Path) -> str:
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode('utf-8')

    def pdf_to_all_pages_images_base64(self, pdf_path: Path, unit_output_dir: Path) -> List[str]:
        if not PDF2IMAGE_AVAILABLE:
            raise ImportError("pdf2image not installed")
        
        try:
            # Convert all pages of PDF to PIL images
            pil_images = convert_from_path(str(pdf_path), dpi=200)
            if not pil_images:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
            
            base64_images = []
            for i, img in enumerate(pil_images):
                # Save image to disk
                image_filename = f"{pdf_path.stem}_page_{i+1}.png"
                image_path = unit_output_dir / image_filename
                img.save(image_path, format='PNG')
                print(f"         üñºÔ∏è  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {i+1}: {image_path.name}")
                
                # Convert PIL image to base64
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format='PNG')
                base64_img = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
                base64_images.append(base64_img)
                
            return base64_images
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            raise

    def create_structure_prompt(self, file_type: str, page_number: Optional[int] = None) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ SmolDocling"""
        page_info = f" (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number})" if page_number else ""
        return f"""–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É{page_info} –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ Docling JSON.
        
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ñ–æ—Ä–º–∞—Ç—É –æ—Ç–≤–µ—Ç–∞:
1. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
2. –ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É:

{{
  "text": "–ø–æ–ª–Ω—ã–π –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
  "tables": [
    {{
      "type": "table",
      "rows": [
        ["–ó–∞–≥–æ–ª–æ–≤–æ–∫ 1", "–ó–∞–≥–æ–ª–æ–≤–æ–∫ 2"],
        ["–î–∞–Ω–Ω—ã–µ 1", "–î–∞–Ω–Ω—ã–µ 2"]
      ]
    }}
  ],
  "layout": {{
    "pages": [
      {{
        "page_num": {page_number if page_number else 1},
        "blocks": [
          {{
            "type": "title" | "paragraph" | "list" | "table",
            "text": "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –±–ª–æ–∫–∞ (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)",
            "bbox": [x1, y1, x2, y2]
          }}
        ]
      }}
    ]
  }},
  "metadata": {{
    "pages_count": {1 if not page_number else page_number}
  }}
}}

–í–ê–ñ–ù–û:
- –ò–∑–≤–ª–µ–∫–∏ –í–ï–°–¨ —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –ù–∞–π–¥–∏ –∏ –∏–∑–≤–ª–µ–∫–∏ –í–°–ï —Ç–∞–±–ª–∏—Ü—ã
- –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, —Å–ø–∏—Å–∫–∏)
- –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–º–∏
- –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ []
"""

    def create_metadata_prompt(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        return """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∑–∞–∫—É–ø–∫–∏ –∏ –∏–∑–≤–ª–µ–∫–∏ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–æ–≥–æ–≥–æ JSON:

{
  "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": "–Ω–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∑–∞–∫—É–ø–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)",
  "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": "–Ω–æ–º–µ—Ä –ª–æ—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)",
  "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": "–¥–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì",
  "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è/–ø–æ—Å—Ç–∞–≤—â–∏–∫–∞",
  "–ò–ù–ù": "–ò–ù–ù –ø–æ–±–µ–¥–∏—Ç–µ–ª—è (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)",
  "–ö–ü–ü": "–ö–ü–ü –ø–æ–±–µ–¥–∏—Ç–µ–ª—è (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)",
  "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": "—Ü–µ–Ω–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ, –±–µ–∑ –≤–∞–ª—é—Ç—ã)",
  "–≤–∞–ª—é—Ç–∞": "–≤–∞–ª—é—Ç–∞ (RUB, USD, EUR –∏ —Ç.–¥.)",
  "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": "–ø—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏/–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞/—É—Å–ª—É–≥–∏",
  "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": "–¥–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì",
  "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": "–¥–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì",
  "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "–¥–∞—Ç–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì",
  "–∑–∞–∫–∞–∑—á–∏–∫": "–ø–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞",
  "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "–ø–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –∑–∞–∫–∞–∑—á–∏–∫–∞)",
  "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": ["–§–ò–û —á–ª–µ–Ω–∞ –∫–æ–º–∏—Å—Å–∏–∏ 1", "–§–ò–û —á–ª–µ–Ω–∞ –∫–æ–º–∏—Å—Å–∏–∏ 2", ...],
  "—É—á–∞—Å—Ç–Ω–∏–∫–∏": [
    {
      "–Ω–æ–º–µ—Ä_–∑–∞—è–≤–∫–∏": "–Ω–æ–º–µ—Ä –∑–∞—è–≤–∫–∏ —É—á–∞—Å—Ç–Ω–∏–∫–∞",
      "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–∞",
      "—Å—É–º–º–∞_–±–µ–∑_–Ω–¥—Å": "—Å—É–º–º–∞ –±–µ–∑ –ù–î–°",
      "—Å—É–º–º–∞_—Å_–Ω–¥—Å": "—Å—É–º–º–∞ —Å –ù–î–°",
      "—Å—Ç–∞—Ç—É—Å": "—Å—Ç–∞—Ç—É—Å —É—á–∞—Å—Ç–Ω–∏–∫–∞"
    }
  ]
}

–í–ê–ñ–ù–û:
- –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- –ï—Å–ª–∏ –ø–æ–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É "" –∏–ª–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ []
- –ò–∑–≤–ª–µ–∫–∏ –í–°–ï–• —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∑–∞–∫—É–ø–∫–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
- –°–æ—Å—Ç–∞–≤ –∫–æ–º–∏—Å—Å–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–∞—Å—Å–∏–≤–æ–º –§–ò–û
- –ò–ù–ù –∏ –ö–ü–ü –∏–∑–≤–ª–µ–∫–∞–π —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã
- –¶–µ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —á–∏—Å–ª–æ–º –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤ –≤–∞–ª—é—Ç—ã
"""

    def process_single_page_structure(self, base64_image: str, prompt_text: str, page_num: int) -> Dict[str, Any]:
        """Process a single page/image with SmolDocling for structure extraction"""
        try:
            messages_content = [
                {"type": "text", "text": prompt_text},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
            ]
            
            print(f"      ‚û°Ô∏è  –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ SmolDocling –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}...")
            response_start_time = time.time()
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": messages_content}],
                max_tokens=5000,
                temperature=0.0,  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                response_format={"type": "json_object"}  # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º JSON
            )
            response_time = time.time() - response_start_time
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            print(f"      ‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {response_time:.2f} —Å–µ–∫—É–Ω–¥")
            print(f"         –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response.choices[0].message.content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # Parse JSON response
            try:
                docling_output = json.loads(response.choices[0].message.content)
                print(f"      üì¶ –ü–∞—Ä—Å–∏–Ω–≥ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
                return {
                    "success": True,
                    "data": docling_output,
                    "response_time": response_time,
                    "tokens_used": tokens_used
                }
            except json.JSONDecodeError as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")
                return {
                    "success": False,
                    "error": f"JSON Decode Error: {e}",
                    "raw_response": response.choices[0].message.content,
                    "response_time": response_time,
                    "tokens_used": tokens_used
                }
                
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} —á–µ—Ä–µ–∑ SmolDocling: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def extract_winners_metadata(self, structured_text: str, tables_data: List) -> Dict[str, Any]:
        """Extract winners metadata from structured text using LLM"""
        try:
            # Combine text and tables for metadata extraction
            combined_content = f"–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{structured_text}\n\n–¢–∞–±–ª–∏—Ü—ã:\n"
            for i, table in enumerate(tables_data):
                combined_content += f"\n–¢–∞–±–ª–∏—Ü–∞ {i+1}:\n"
                if 'rows' in table:
                    for row in table['rows']:
                        combined_content += f"  {row}\n"
            
            prompt = self.create_metadata_prompt()
            messages = [
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –∑–∞–∫—É–ø–æ–∫. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Ç–æ—á–Ω–æ –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤."},
                {"role": "user", "content": f"{prompt}\n\n–î–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{combined_content}"}
            ]
            
            print(f"      ‚û°Ô∏è  –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π...")
            response_start_time = time.time()
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=4000,
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            response_time = time.time() - response_start_time
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            print(f"      ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã –∑–∞ {response_time:.2f} —Å–µ–∫—É–Ω–¥")
            
            try:
                metadata = json.loads(response.choices[0].message.content)
                return {
                    "success": True,
                    "data": metadata,
                    "response_time": response_time,
                    "tokens_used": tokens_used
                }
            except json.JSONDecodeError as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
                return {
                    "success": False,
                    "error": f"JSON Decode Error: {e}",
                    "raw_response": response.choices[0].message.content,
                    "response_time": response_time,
                    "tokens_used": tokens_used
                }
                
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def merge_page_results(self, page_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Merge results from multiple pages into a single document result"""
        if not page_results:
            return {}
        
        # Initialize merged result with data from first successful page
        merged = {
            "text": "",
            "tables": [],
            "layout": {"pages": []},
            "metadata": {},
            "processing_info": {
                "total_pages": len(page_results),
                "successful_pages": 0,
                "failed_pages": 0,
                "total_response_time": 0,
                "total_tokens_used": 0
            }
        }
        
        for i, result in enumerate(page_results):
            page_num = i + 1
            if result.get("success"):
                merged["processing_info"]["successful_pages"] += 1
                merged["processing_info"]["total_response_time"] += result.get("response_time", 0)
                merged["processing_info"]["total_tokens_used"] += result.get("tokens_used", 0)
                
                page_data = result.get("data", {})
                
                # Merge text
                if page_data.get("text"):
                    merged["text"] += f"\n\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} ---\n\n" + page_data.get("text", "")
                
                # Merge tables
                if page_data.get("tables"):
                    merged["tables"].extend(page_data.get("tables", []))
                
                # Merge layout
                if page_data.get("layout", {}).get("pages"):
                    merged["layout"]["pages"].extend(page_data["layout"]["pages"])
                else:
                    # Create a default page entry if none exists
                    merged["layout"]["pages"].append({
                        "page_num": page_num,
                        "blocks": []
                    })
                
                # Merge metadata (take from first page or merge if needed)
                if not merged["metadata"] and page_data.get("metadata"):
                    merged["metadata"] = page_data["metadata"]
                elif page_data.get("metadata"):
                    # Update pages_count
                    if "pages_count" in page_data["metadata"]:
                        merged["metadata"]["pages_count"] = page_data["metadata"]["pages_count"]
            else:
                merged["processing_info"]["failed_pages"] += 1
        
        return merged

    def analyze_winners(self, metadata: Dict[str, Any], unit_id: str) -> Dict[str, Any]:
        """Analyze winner information from the metadata"""
        participants = metadata.get("—É—á–∞—Å—Ç–Ω–∏–∫–∏", [])
        
        winner_analysis = {
            "unit_id": unit_id,
            "winner_found": False,
            "winner_info": {},
            "total_participants": len(participants),
            "participants": participants,
            "procurement_info": {
                "procedure_number": metadata.get("–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã"),
                "lot_number": metadata.get("–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞"),
                "procurement_subject": metadata.get("–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏"),
                "protocol_date": metadata.get("–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞")
            }
        }
        
        # Check for explicit winner information
        winner_name = metadata.get("–ø–æ–±–µ–¥–∏—Ç–µ–ª—å")
        if winner_name:
            winner_analysis["winner_found"] = True
            winner_analysis["winner_info"] = {
                "name": winner_name,
                "inn": metadata.get("–ò–ù–ù"),
                "kpp": metadata.get("–ö–ü–ü"),
                "price": metadata.get("—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è"),
                "currency": metadata.get("–≤–∞–ª—é—Ç–∞")
            }
        else:
            # Try to determine winner from participants list
            # Look for participant with status indicating winner
            for participant in participants:
                status = participant.get("—Å—Ç–∞—Ç—É—Å", "").lower()
                if "–ø–æ–±–µ–¥" in status or "winner" in status or status == "–¥–æ–ø—É—â–µ–Ω":
                    winner_analysis["winner_found"] = True
                    winner_analysis["winner_info"] = {
                        "name": participant.get("–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"),
                        "inn": None,  # Would need to extract from participant data
                        "kpp": None,
                        "price": participant.get("—Å—É–º–º–∞_—Å_–Ω–¥—Å"),
                        "currency": "RUB"
                    }
                    break
        
        # Add to global winners analysis
        self.winners_analysis.append(winner_analysis)
        
        return winner_analysis

    def process_unit(self, unit_info: Dict[str, Any], unit_index: int, total_units: int) -> Optional[Dict[str, Any]]:
        unit_id = unit_info["unit_id"]
        unit_dir = Path(unit_info["unit_dir"])
        files_in_unit = unit_info["files"]
        
        unit_output_dir = OUTPUT_DIR / unit_id
        unit_output_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n{'='*80}")
        print(f"[{unit_index+1}/{total_units}] –û–±—Ä–∞–±–æ—Ç–∫–∞ UNIT: {unit_id}")
        print(f"{'='*80}")

        unit_start_time = time.time()
        unit_total_tokens = 0
        unit_successful_files = 0
        
        unit_result = {
            "unit_id": unit_id,
            "route": unit_info.get("route", "unknown"),
            "files_processed": [],
            "total_unit_time": 0.0,
            "total_unit_tokens": 0,
            "status": "failed",
            "error": None
        }

        for file_index, file_info in enumerate(files_in_unit):
            file_path = Path(file_info["path"])
            original_name = file_info["original_name"]
            detected_type = file_info["detected_type"]

            print(f"   üìÑ [{file_index+1}/{len(files_in_unit)}] –§–∞–π–ª: {original_name} ({detected_type})")

            file_start_time = time.time()
            file_tokens_used = 0
            
            try:
                if detected_type == "image":
                    print(f"\n      üì∑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {original_name}")
                    print(f"         –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / (1024*1024):.1f} MB")
                    base64_image = self.image_to_base64(file_path)
                    print(f"         –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ base64...")
                    print(f"         Base64 –¥–ª–∏–Ω–∞: {len(base64_image)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    # Stage 1: Extract document structure
                    structure_prompt = self.create_structure_prompt(detected_type)
                    page_result = self.process_single_page_structure(base64_image, structure_prompt, 1)
                    
                    if page_result.get("success"):
                        docling_output = page_result["data"]
                        file_tokens_used = page_result.get("tokens_used", 0)
                        self.metrics["total_tokens_used"] += file_tokens_used
                        
                        # Save structure results
                        output_filename = f"{file_path.stem}_smoldocling_structure.json"
                        output_path = unit_output_dir / output_filename
                        with open(output_path, "w", encoding="utf-8") as f:
                            json.dump(docling_output, f, indent=2, ensure_ascii=False)
                        print(f"      üíæ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
                        
                        # Stage 2: Extract winners metadata from structure
                        structured_text = docling_output.get("text", "")
                        tables_data = docling_output.get("tables", [])
                        metadata_result = self.extract_winners_metadata(structured_text, tables_data)
                        
                        if metadata_result.get("success"):
                            metadata = metadata_result["data"]
                            metadata_tokens = metadata_result.get("tokens_used", 0)
                            self.metrics["total_tokens_used"] += metadata_tokens
                            file_tokens_used += metadata_tokens
                            
                            # Save metadata results
                            metadata_filename = f"{file_path.stem}_smoldocling_metadata.json"
                            metadata_path = unit_output_dir / metadata_filename
                            with open(metadata_path, "w", encoding="utf-8") as f:
                                json.dump(metadata, f, indent=2, ensure_ascii=False)
                            print(f"      üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
                            
                            # Analyze winners
                            winner_analysis = self.analyze_winners(metadata, unit_id)
                            print(f"      üèÜ –ü–æ–±–µ–¥–∏—Ç–µ–ª—å –Ω–∞–π–¥–µ–Ω: {'–î–∞' if winner_analysis['winner_found'] else '–ù–µ—Ç'}")
                            
                            unit_successful_files += 1
                            unit_result["files_processed"].append({
                                "file_name": original_name,
                                "detected_type": detected_type,
                                "status": "success",
                                "structure_response_time": page_result.get("response_time", 0),
                                "metadata_response_time": metadata_result.get("response_time", 0),
                                "tokens_used": file_tokens_used,
                                "structure_output_path": str(output_path),
                                "metadata_output_path": str(metadata_path),
                                "winner_found": winner_analysis["winner_found"],
                                "winner_info": winner_analysis["winner_info"]
                            })
                        else:
                            unit_result["files_processed"].append({
                                "file_name": original_name,
                                "detected_type": detected_type,
                                "status": "metadata_failed",
                                "error": metadata_result.get("error", "Unknown error"),
                                "structure_output_path": str(output_path) if 'output_path' in locals() else None
                            })
                    else:
                        unit_result["files_processed"].append({
                            "file_name": original_name,
                            "detected_type": detected_type,
                            "status": "structure_failed",
                            "error": page_result.get("error", "Unknown error")
                        })
                        
                elif detected_type == "pdf":
                    print(f"\n      üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF: {original_name}")
                    print(f"         –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / 1024:.1f} KB")
                    if not PDF2IMAGE_AVAILABLE:
                        raise ImportError("pdf2image not installed")
                    
                    print(f"         –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
                    base64_images = self.pdf_to_all_pages_images_base64(file_path, unit_output_dir)
                    print(f"         –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(base64_images)}")
                    
                    # Stage 1: Process each page for structure
                    page_results = []
                    for i, base64_image in enumerate(base64_images):
                        page_num = i + 1
                        print(f"         üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} –∏–∑ {len(base64_images)}")
                        structure_prompt = self.create_structure_prompt(detected_type, page_num)
                        page_result = self.process_single_page_structure(base64_image, structure_prompt, page_num)
                        page_results.append(page_result)
                        if page_result.get("success"):
                            file_tokens_used += page_result.get("tokens_used", 0)
                    
                    self.metrics["total_tokens_used"] += file_tokens_used
                    
                    # Merge results from all pages
                    merged_result = self.merge_page_results(page_results)
                    
                    # Save merged structure results
                    output_filename = f"{file_path.stem}_smoldocling_merged_structure.json"
                    output_path = unit_output_dir / output_filename
                    with open(output_path, "w", encoding="utf-8") as f:
                        json.dump(merged_result, f, indent=2, ensure_ascii=False)
                    print(f"      üíæ –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
                    
                    # Save individual page results for reference
                    page_results_path = unit_output_dir / f"{file_path.stem}_page_structure_results.json"
                    with open(page_results_path, "w", encoding="utf-8") as f:
                        json.dump(page_results, f, indent=2, ensure_ascii=False)
                    print(f"      üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {page_results_path}")
                    
                    # Stage 2: Extract winners metadata from merged structure
                    if merged_result and any(r.get("success") for r in page_results):
                        structured_text = merged_result.get("text", "")
                        tables_data = merged_result.get("tables", [])
                        metadata_result = self.extract_winners_metadata(structured_text, tables_data)
                        
                        if metadata_result.get("success"):
                            metadata = metadata_result["data"]
                            metadata_tokens = metadata_result.get("tokens_used", 0)
                            self.metrics["total_tokens_used"] += metadata_tokens
                            file_tokens_used += metadata_tokens
                            
                            # Save metadata results
                            metadata_filename = f"{file_path.stem}_smoldocling_metadata.json"
                            metadata_path = unit_output_dir / metadata_filename
                            with open(metadata_path, "w", encoding="utf-8") as f:
                                json.dump(metadata, f, indent=2, ensure_ascii=False)
                            print(f"      üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
                            
                            # Analyze winners
                            winner_analysis = self.analyze_winners(metadata, unit_id)
                            print(f"      üèÜ –ü–æ–±–µ–¥–∏—Ç–µ–ª—å –Ω–∞–π–¥–µ–Ω: {'–î–∞' if winner_analysis['winner_found'] else '–ù–µ—Ç'}")
                            
                            unit_successful_files += 1
                            unit_result["files_processed"].append({
                                "file_name": original_name,
                                "detected_type": detected_type,
                                "status": "success",
                                "total_pages": len(base64_images),
                                "successful_pages": merged_result.get("processing_info", {}).get("successful_pages", 0),
                                "failed_pages": merged_result.get("processing_info", {}).get("failed_pages", 0),
                                "total_structure_response_time": merged_result.get("processing_info", {}).get("total_response_time", 0),
                                "structure_tokens_used": file_tokens_used - metadata_tokens,
                                "metadata_response_time": metadata_result.get("response_time", 0),
                                "metadata_tokens_used": metadata_tokens,
                                "tokens_used": file_tokens_used,
                                "structure_output_path": str(output_path),
                                "metadata_output_path": str(metadata_path),
                                "winner_found": winner_analysis["winner_found"],
                                "winner_info": winner_analysis["winner_info"],
                                "total_participants": winner_analysis["total_participants"]
                            })
                        else:
                            unit_result["files_processed"].append({
                                "file_name": original_name,
                                "detected_type": detected_type,
                                "status": "metadata_failed",
                                "error": metadata_result.get("error", "Unknown error"),
                                "structure_output_path": str(output_path) if 'output_path' in locals() else None
                            })
                    else:
                        unit_result["files_processed"].append({
                            "file_name": original_name,
                            "detected_type": detected_type,
                            "status": "structure_failed",
                            "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–∏ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                        })
                    
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ (docx, html_text) –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º vision API
                    print(f"      ‚ùå –¢–∏–ø —Ñ–∞–π–ª–∞ '{detected_type}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è Vision API. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                    continue

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                unit_result["files_processed"].append({
                    "file_name": original_name,
                    "detected_type": detected_type,
                    "status": "failed",
                    "error": str(e)
                })
        
        unit_result["total_unit_time"] = time.time() - unit_start_time
        unit_result["total_unit_tokens"] = unit_total_tokens
        if unit_successful_files > 0:
            unit_result["status"] = "success"
            self.metrics["successful_units"] += 1
            self.metrics["successful_files"] += unit_successful_files
        
        self.metrics["total_units"] += 1
        self.metrics["total_files"] += len(files_in_unit)
        self.metrics["total_processing_time"] += unit_result["total_unit_time"]
        self.metrics["unit_results"].append(unit_result)
        
        return unit_result

    def generate_summary_report(self):
        print(f"\n{'='*80}")
        print(f"–ì–ï–ù–ï–†–ê–¶–ò–Ø –ú–ï–¢–†–ò–ö")
        print(f"{'='*80}")

        total_units = self.metrics["total_units"]
        successful_units = self.metrics["successful_units"]
        total_files = self.metrics["total_files"]
        successful_files = self.metrics["successful_files"]
        total_time = self.metrics["total_processing_time"]
        total_tokens = self.metrics["total_tokens_used"]

        avg_time_per_file = total_time / successful_files if successful_files > 0 else 0
        avg_tokens_per_file = total_tokens / successful_files if successful_files > 0 else 0

        # Count winners found
        winners_found = sum(1 for w in self.winners_analysis if w["winner_found"])
        total_analyzed = len(self.winners_analysis)

        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ: {successful_units}/{total_units} ({successful_units/total_units*100:.1f}%)")
        print(f"‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —É—Å–ø–µ—à–Ω—ã–π —Ñ–∞–π–ª: {avg_time_per_file:.2f} —Å–µ–∫")
        print(f"üî¢ –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {total_tokens}")
        print(f"üèÜ –ü–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –Ω–∞–π–¥–µ–Ω–æ: {winners_found}/{total_analyzed} ({winners_found/max(total_analyzed, 1)*100:.1f}%)")

        # Extrapolation
        estimated_100_units_time = avg_time_per_file * 100
        estimated_500_units_time = avg_time_per_file * 500

        print(f"üìà –û—Ü–µ–Ω–∫–∞ –¥–ª—è 100 UNIT'–æ–≤: {estimated_100_units_time / 60:.1f} –º–∏–Ω ({estimated_100_units_time / 3600:.2f} —á)")
        print(f"üìà –û—Ü–µ–Ω–∫–∞ –¥–ª—è 500 UNIT'–æ–≤: {estimated_500_units_time / 60:.1f} –º–∏–Ω ({estimated_500_units_time / 3600:.2f} —á)")

        metrics_summary = {
            "timestamp": datetime.now().isoformat(),
            "total_units_attempted": total_units,
            "successful_units": successful_units,
            "total_files_attempted": total_files,
            "successful_files": successful_files,
            "total_processing_time_seconds": total_time,
            "avg_time_per_successful_file_seconds": avg_time_per_file,
            "total_tokens_used": total_tokens,
            "avg_tokens_per_successful_file": avg_tokens_per_file,
            "winners_analysis": {
                "total_analyzed": total_analyzed,
                "winners_found": winners_found,
                "success_rate": winners_found/max(total_analyzed, 1)
            },
            "extrapolation": {
                "estimated_100_units_minutes": estimated_100_units_time / 60,
                "estimated_100_units_hours": estimated_100_units_time / 3600,
                "estimated_500_units_minutes": estimated_500_units_time / 60,
                "estimated_500_units_hours": estimated_500_units_time / 3600
            }
        }
        metrics_output_path = OUTPUT_DIR / f"metrics_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(metrics_output_path, "w", encoding="utf-8") as f:
            json.dump(metrics_summary, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_output_path}")

        return metrics_summary

    def generate_comparison_report(self):
        print(f"\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
        report_path = OUTPUT_DIR / f"comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"# –û—Ç—á–µ—Ç –æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ UNIT'–æ–≤ —á–µ—Ä–µ–∑ SmolDocling (–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥)\n\n")
            f.write(f"**–î–∞—Ç–∞ –æ—Ç—á–µ—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n")
            f.write(f"- –í—Å–µ–≥–æ UNIT'–æ–≤ –≤ —Ç–µ—Å—Ç–µ: {self.metrics['total_units']}\n")
            f.write(f"- –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ UNIT'–æ–≤: {self.metrics['successful_units']}\n")
            f.write(f"- –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.metrics['total_processing_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
            f.write(f"- –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {self.metrics['total_tokens_used']}\n")
            f.write(f"- –ü–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –Ω–∞–π–¥–µ–Ω–æ: {sum(1 for w in self.winners_analysis if w['winner_found'])}/{len(self.winners_analysis)}\n\n")

            f.write(f"## –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ UNIT'–∞–º\n\n")
            for unit_result in self.metrics["unit_results"]:
                f.write(f"### UNIT ID: `{unit_result['unit_id']}`\n")
                f.write(f"- –°—Ç–∞—Ç—É—Å: **{unit_result['status'].upper()}**\n")
                f.write(f"- –ú–∞—Ä—à—Ä—É—Ç: `{unit_result['route']}`\n")
                f.write(f"- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ UNIT'–∞: {unit_result['total_unit_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
                if unit_result['error']:
                    f.write(f"- –û—à–∏–±–∫–∞: `{unit_result['error']}`\n")
                f.write(f"\n")

                for file_proc_result in unit_result["files_processed"]:
                    f.write(f"#### –§–∞–π–ª: `{file_proc_result['file_name']}`\n")
                    f.write(f"- –¢–∏–ø: `{file_proc_result['detected_type']}`\n")
                    f.write(f"- –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏: **{file_proc_result['status'].upper()}**\n")
                    if file_proc_result['status'] == 'success':
                        if file_proc_result['detected_type'] == 'pdf':
                            f.write(f"- –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {file_proc_result.get('total_pages', 'N/A')}\n")
                            f.write(f"- –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {file_proc_result.get('successful_pages', 'N/A')}\n")
                            f.write(f"- –ù–µ—É–¥–∞—á–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {file_proc_result.get('failed_pages', 'N/A')}\n")
                            f.write(f"- –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {file_proc_result.get('total_structure_response_time', 0):.2f} —Å–µ–∫—É–Ω–¥\n")
                            f.write(f"- –í—Ä–µ–º—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {file_proc_result.get('metadata_response_time', 0):.2f} —Å–µ–∫—É–Ω–¥\n")
                        else:
                            f.write(f"- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {file_proc_result['structure_response_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
                            f.write(f"- –í—Ä–µ–º—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {file_proc_result['metadata_response_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
                        
                        f.write(f"- –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {file_proc_result['tokens_used']}\n")
                        f.write(f"- –ü—É—Ç—å –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSON: `{file_proc_result['structure_output_path']}`\n")
                        f.write(f"- –ü—É—Ç—å –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö JSON: `{file_proc_result['metadata_output_path']}`\n")
                        f.write(f"- –ü–æ–±–µ–¥–∏—Ç–µ–ª—å –Ω–∞–π–¥–µ–Ω: **{'–î–∞' if file_proc_result.get('winner_found', False) else '–ù–µ—Ç'}**\n")
                        
                        if file_proc_result.get('winner_found'):
                            winner_info = file_proc_result.get('winner_info', {})
                            f.write(f"- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ:\n")
                            f.write(f"  - –ù–∞–∑–≤–∞–Ω–∏–µ: {winner_info.get('name', 'N/A')}\n")
                            f.write(f"  - –ò–ù–ù: {winner_info.get('inn', 'N/A')}\n")
                            f.write(f"  - –ö–ü–ü: {winner_info.get('kpp', 'N/A')}\n")
                            f.write(f"  - –¶–µ–Ω–∞: {winner_info.get('price', 'N/A')} {winner_info.get('currency', 'N/A')}\n")
                        
                        # Original document info
                        original_file_path = NORMALIZED_DIR / unit_result['unit_id'] / "files" / file_proc_result['file_name']
                        if original_file_path.exists():
                            f.write(f"\n##### –ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç:\n")
                            f.write(f"```\n")
                            f.write(f"–ü—É—Ç—å: {original_file_path}\n")
                            f.write(f"–¢–∏–ø: {file_proc_result['detected_type']}\n")
                            f.write(f"–†–∞–∑–º–µ—Ä: {original_file_path.stat().st_size / 1024:.1f} KB\n")
                            f.write(f"```\n")
                        else:
                            f.write(f"\n##### –ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ `{original_file_path}`\n")

                        # Structure output preview
                        try:
                            with open(file_proc_result['structure_output_path'], 'r', encoding='utf-8') as json_f:
                                structure_output = json.load(json_f)
                            
                            f.write(f"\n##### –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ SmolDocling (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞):\n")
                            f.write(f"```json\n")
                            f.write(f"{{\n")
                            f.write(f"  \"text_length\": {len(structure_output.get('text', ''))},\n")
                            f.write(f"  \"tables_count\": {len(structure_output.get('tables', []))},\n")
                            f.write(f"  \"pages_count\": {structure_output.get('metadata', {}).get('pages_count', 'N/A')}\n")
                            f.write(f"}}\n")
                            f.write(f"```\n")

                        except Exception as e:
                            f.write(f"\n##### –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏/—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}\n")
                        
                        # Metadata output preview
                        try:
                            with open(file_proc_result['metadata_output_path'], 'r', encoding='utf-8') as json_f:
                                metadata_output = json.load(json_f)
                            
                            f.write(f"\n##### –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ SmolDocling (–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ):\n")
                            f.write(f"```json\n")
                            f.write(f"{{\n")
                            f.write(f"  \"–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã\": \"{metadata_output.get('–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã', 'N/A')}\",\n")
                            f.write(f"  \"–ø–æ–±–µ–¥–∏—Ç–µ–ª—å\": \"{metadata_output.get('–ø–æ–±–µ–¥–∏—Ç–µ–ª—å', 'N/A')}\",\n")
                            f.write(f"  \"—É—á–∞—Å—Ç–Ω–∏–∫–∏_count\": {len(metadata_output.get('—É—á–∞—Å—Ç–Ω–∏–∫–∏', []))}\n")
                            f.write(f"}}\n")
                            f.write(f"```\n")

                        except Exception as e:
                            f.write(f"\n##### –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏/—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}\n")
                    else:
                        f.write(f"- –û—à–∏–±–∫–∞: `{file_proc_result['error']}`\n")
                    f.write(f"\n---\n\n")
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

    def generate_winners_analysis_report(self):
        """Generate a separate report focused on winners analysis"""
        print(f"\nüèÜ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π...")
        winners_report_path = OUTPUT_DIR / f"winners_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # Enrich winners analysis with more details
        enriched_analysis = []
        for analysis in self.winners_analysis:
            # Find corresponding unit result for more details
            unit_result = next((u for u in self.metrics["unit_results"] if u["unit_id"] == analysis["unit_id"]), None)
            if unit_result:
                analysis["unit_details"] = {
                    "route": unit_result.get("route"),
                    "processing_time": unit_result.get("total_unit_time"),
                    "status": unit_result.get("status")
                }
            enriched_analysis.append(analysis)
        
        with open(winners_report_path, "w", encoding="utf-8") as f:
            json.dump(enriched_analysis, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ –û—Ç—á–µ—Ç –ø–æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {winners_report_path}")
        
        # Also generate a summary markdown report
        winners_md_path = OUTPUT_DIR / f"winners_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(winners_md_path, "w", encoding="utf-8") as f:
            f.write(f"# –ê–Ω–∞–ª–∏–∑ –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π –ø–æ –≥–æ—Å–∑–∞–∫—É–ø–∫–∞–º (SmolDocling)\n\n")
            f.write(f"**–î–∞—Ç–∞ –æ—Ç—á–µ—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"## –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n")
            f.write(f"- –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(enriched_analysis)}\n")
            f.write(f"- –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–º: {sum(1 for w in enriched_analysis if w['winner_found'])}\n")
            f.write(f"- –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π: {sum(1 for w in enriched_analysis if w['winner_found'])/max(len(enriched_analysis), 1)*100:.1f}%\n\n")
            
            f.write(f"## –î–æ–∫—É–º–µ–Ω—Ç—ã —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è–º–∏\n\n")
            for analysis in enriched_analysis:
                if analysis["winner_found"]:
                    f.write(f"### UNIT: `{analysis['unit_id']}`\n")
                    f.write(f"- –ù–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã: {analysis['procurement_info'].get('procedure_number', 'N/A')}\n")
                    f.write(f"- –ü—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏: {analysis['procurement_info'].get('procurement_subject', 'N/A')}\n")
                    f.write(f"- –î–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞: {analysis['procurement_info'].get('protocol_date', 'N/A')}\n")
                    f.write(f"- –ü–æ–±–µ–¥–∏—Ç–µ–ª—å: **{analysis['winner_info'].get('name', 'N/A')}**\n")
                    f.write(f"- –ò–ù–ù: {analysis['winner_info'].get('inn', 'N/A')}\n")
                    f.write(f"- –ö–ü–ü: {analysis['winner_info'].get('kpp', 'N/A')}\n")
                    f.write(f"- –¶–µ–Ω–∞: {analysis['winner_info'].get('price', 'N/A')} {analysis['winner_info'].get('currency', 'N/A')}\n")
                    f.write(f"- –í—Å–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {analysis['total_participants']}\n\n")
            
            f.write(f"## –î–æ–∫—É–º–µ–Ω—Ç—ã –±–µ–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è\n\n")
            for analysis in enriched_analysis:
                if not analysis["winner_found"]:
                    f.write(f"### UNIT: `{analysis['unit_id']}`\n")
                    f.write(f"- –ù–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã: {analysis['procurement_info'].get('procedure_number', 'N/A')}\n")
                    f.write(f"- –ü—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏: {analysis['procurement_info'].get('procurement_subject', 'N/A')}\n")
                    f.write(f"- –î–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞: {analysis['procurement_info'].get('protocol_date', 'N/A')}\n")
                    f.write(f"- –í—Å–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {analysis['total_participants']}\n")
                    f.write(f"- –°—Ç–∞—Ç—É—Å UNIT'–∞: {analysis.get('unit_details', {}).get('status', 'N/A')}\n\n")
        
        print(f"‚úÖ –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {winners_md_path}")

    def run(self):
        print(f"\n{'='*80}")
        print(f"–ú–ê–°–°–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê 10 UNIT'–û–í –ß–ï–†–ï–ó SMOLDOCLING (–ì–ò–ë–†–ò–î–ù–´–ô –ü–û–î–•–û–î)")
        print(f"{'='*80}")

        # Wait for server to be ready
        if not self.wait_for_server_ready():
            print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
            return

        if not self.test_connection():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å.")
            return

        try:
            with open(TEST_UNITS_FILE, "r", encoding="utf-8") as f:
                test_units_data = json.load(f)
                # Use only first 10 units instead of all 20
                self.test_units = test_units_data["units"][:10]
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º UNIT'–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {TEST_UNITS_FILE}")
            print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ collect_ocr_units.py –¥–ª—è —Å–±–æ—Ä–∞ UNIT'–æ–≤.")
            return

        print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ UNIT'–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(self.test_units)}")
        print(f"üéØ –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.test_units)} UNIT'–æ–≤")

        for i, unit_info in enumerate(self.test_units):
            self.process_unit(unit_info, i, len(self.test_units))
        
        self.generate_summary_report()
        self.generate_comparison_report()
        self.generate_winners_analysis_report()

        print(f"\n{'='*80}")
        print(f"‚úÖ –ú–ê–°–°–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        print(f"{'='*80}")

if __name__ == "__main__":
    processor = SmolDoclingProcessor()
    processor.run()
