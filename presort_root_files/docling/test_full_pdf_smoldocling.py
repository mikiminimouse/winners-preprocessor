#!/usr/bin/env python3
"""
–ü–û–õ–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê PDF –ß–ï–†–ï–ó SMOLDOCLING
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞, —Ç–∞–±–ª–∏—Ü –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
"""
import os
import sys
import json
import time
import base64
from pathlib import Path
from datetime import datetime
from PIL import Image
import io

try:
    import openai
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False
    print("‚ö†Ô∏è  openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")

try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False
    print("‚ö†Ô∏è  pdf2image –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdf2image")
    print("   –¢–∞–∫–∂–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è: sudo apt-get install poppler-utils")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è SmolDocling
API_TOKEN = "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"
BASE_URL = "https://d63e30af-085a-49f0-9724-8162da967af2.modelrun.inference.cloud.ru/v1"
MODEL_NAME = "model-run-4qigw-disease"

# –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
TEST_FILE = "/root/winners_preprocessor/normalized/UNIT_43a02eedd2bbca86/files/! –ü—Ä–æ—Ç–æ–∫–æ–ª –≠–ú-17.pdf"
OUTPUT_DIR = Path("/root/winners_preprocessor/output_full_pdf_test")

class FullSmolDoclingProcessor:
    def __init__(self):
        if not OPENAI_SDK_AVAILABLE:
            raise ImportError("openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

        self.client = openai.OpenAI(
            api_key=API_TOKEN,
            base_url=BASE_URL,
            timeout=300.0  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        )
        self.model = MODEL_NAME
        OUTPUT_DIR.mkdir(exist_ok=True)

    def wait_for_server_ready(self, max_wait_time: int = 60) -> bool:
        """Wait for the inference server to be ready"""
        print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞ SmolDocling (–º–∞–∫—Å–∏–º—É–º {max_wait_time} —Å–µ–∫—É–Ω–¥)...")

        start_time = time.time()
        while time.time() - start_time < max_wait_time:
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": "Hello"}],
                    max_tokens=10,
                    temperature=0.5
                )
                if response.choices[0].message.content:
                    print("‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
                    return True
            except Exception as e:
                print(f"    –°–µ—Ä–≤–µ—Ä –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤... ({str(e)[:50]}...)")
                time.sleep(5)

        print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ —Å—Ç–∞–ª –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏")
        return False

    def get_pdf_pages_info(self, pdf_path: Path) -> dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö PDF"""
        print(f"üìÑ –ê–Ω–∞–ª–∏–∑ PDF: {pdf_path.name}")

        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            all_pages = convert_from_path(str(pdf_path), dpi=72)  # –ù–∏–∑–∫–∏–π DPI –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            total_pages = len(all_pages)

            print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")

            # –ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä—ã –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤—ã—Å–æ–∫–∏–º DPI
            if all_pages:
                first_page = convert_from_path(str(pdf_path), dpi=300, first_page=1, last_page=1)[0]
                print(f"   –†–∞–∑–º–µ—Ä –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {first_page.size} (–ø—Ä–∏ DPI=300)")

            return {
                'total_pages': total_pages,
                'first_page_size': first_page.size if all_pages else None,
                'file_size_mb': pdf_path.stat().st_size / (1024 * 1024)
            }

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ PDF: {e}")
            return {'error': str(e)}

    def create_high_quality_thumbnail(self, pdf_path: Path, page_num: int = 1) -> str:
        """–°–æ–∑–¥–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π thumbnail –¥–ª—è –ª—É—á—à–µ–≥–æ OCR"""
        print(f"   –°–æ–∑–¥–∞–Ω–∏–µ HQ thumbnail —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} –∏–∑ PDF: {pdf_path.name}")

        if not PDF2IMAGE_AVAILABLE:
            raise ImportError("pdf2image not installed")

        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –≤—ã—Å–æ–∫–∏–º DPI –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            pil_images = convert_from_path(
                str(pdf_path),
                dpi=300,  # –í—ã—Å–æ–∫–∏–π DPI –¥–ª—è –ª—É—á—à–µ–≥–æ OCR
                first_page=page_num,
                last_page=page_num
            )

            if not pil_images:
                raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}")

            img = pil_images[0]

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ RGB –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
            if img.mode != 'RGB':
                img = img.convert('RGB')

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 1500px –ø–æ –±–æ–ª—å—à–µ–π —Å—Ç–æ—Ä–æ–Ω–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ –∫–∞—á–µ—Å—Ç–≤–∞/—Ä–∞–∑–º–µ—Ä–∞
            max_size = 1500
            if max(img.size) > max_size:
                ratio = max_size / max(img.size)
                new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))
                img = img.resize(new_size, Image.Resampling.LANCZOS)

            print(f"   Thumbnail —Ä–∞–∑–º–µ—Ä: {img.size} (–±—ã–ª –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–æ max {max_size}px)")

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ JPEG —Å –≤—ã—Å–æ–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='JPEG', quality=95, optimize=True)
            base64_img = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')

            print(f"   ‚úÖ HQ Thumbnail —Å–æ–∑–¥–∞–Ω: {len(base64_img)} chars base64")
            return base64_img

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è thumbnail: {e}")
            raise

    def process_page_with_different_prompts(self, base64_thumbnail: str, page_num: int) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
        print(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏...")

        prompts = [
            "Convert this document page to structured docling format with full text extraction and table recognition.",
            "Extract all text, tables, and layout information from this document page in docling format.",
            "Convert this page to docling with complete OCR, table detection, and text extraction.",
            "Process this document page for full text and table extraction using docling format."
        ]

        results = {}

        for i, prompt in enumerate(prompts):
            print(f"     –ü—Ä–æ–º–ø—Ç {i+1}/{len(prompts)}: {prompt[:50]}...")

            try:
                image_url = f"data:image/jpeg;base64,{base64_thumbnail}"

                messages_content = [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": image_url}}
                ]

                start_time = time.time()
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": messages_content}],
                    max_tokens=8000,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    temperature=0.0
                )

                processing_time = time.time() - start_time
                tokens_used = response.usage.total_tokens if response.usage else 0
                doctags = response.choices[0].message.content

                print(f"       ‚úÖ –£–°–ü–ï–•: {len(doctags)} —Å–∏–º–≤–æ–ª–æ–≤, {tokens_used} —Ç–æ–∫–µ–Ω–æ–≤, {processing_time:.2f}—Å–µ–∫")

                results[f'prompt_{i+1}'] = {
                    'doctags': doctags,
                    'tokens_used': tokens_used,
                    'processing_time': processing_time,
                    'success': True
                }

            except Exception as e:
                print(f"       ‚ùå –û—à–∏–±–∫–∞: {e}")
                results[f'prompt_{i+1}'] = {
                    'error': str(e),
                    'success': False
                }

        return results

    def combine_doctags_results(self, results: dict) -> str:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è"""
        print("   –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤...")

        all_doctags = []

        for prompt_key, result in results.items():
            if result.get('success') and 'doctags' in result:
                doctags = result['doctags'].strip()
                if doctags and len(doctags) > 10:  # –ú–∏–Ω–∏–º—É–º 10 —Å–∏–º–≤–æ–ª–æ–≤
                    all_doctags.append(doctags)

        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        combined_doctags = '\n'.join(all_doctags)

        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫
        unique_lines = []
        seen_lines = set()
        for line in combined_doctags.split('\n'):
            line = line.strip()
            if line and line not in seen_lines:
                unique_lines.append(line)
                seen_lines.add(line)

        final_doctags = '\n'.join(unique_lines)
        print(f"   ‚úÖ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–æ: {len(all_doctags)} –ø—Ä–æ–º–ø—Ç–æ–≤ ‚Üí {len(final_doctags)} —Å–∏–º–≤–æ–ª–æ–≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")

        return final_doctags

    def doctags_to_markdown(self, doctags: str, pdf_path: Path, page_num: int = 1) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è DocTags –≤ Markdown —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç–∞–±–ª–∏—Ü"""
        print("   –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è DocTags –≤ Markdown —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏...")

        lines = doctags.strip().split('\n')
        markdown_parts = []
        current_paragraph = []
        table_rows = []

        for line in lines:
            if not line.strip():
                continue

            parts = line.split('>')
            if len(parts) >= 5:
                content = parts[-1].strip()  # –ü–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å - –∫–æ–Ω—Ç–µ–Ω—Ç

                if content:
                    # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Ç–∞–±–ª–∏—Ü (–µ—Å–ª–∏ –º–Ω–æ–≥–æ | –∏–ª–∏ —Ç–∞–±—É–ª—è—Ü–∏–π)
                    if '|' in content or '\t' in content or any(char.isdigit() for char in content[:10]):
                        # –í–æ–∑–º–æ–∂–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
                        if not table_rows or content.count('|') != table_rows[-1].count('|'):
                            # –ù–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
                            if table_rows:
                                # –ó–∞–≤–µ—Ä—à–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ç–∞–±–ª–∏—Ü—É
                                markdown_parts.append(self.format_table(table_rows))
                                table_rows = []

                        table_rows.append(content)
                    else:
                        # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                        if table_rows:
                            # –ó–∞–≤–µ—Ä—à–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É –ø–µ—Ä–µ–¥ —Ç–µ–∫—Å—Ç–æ–º
                            markdown_parts.append(self.format_table(table_rows))
                            table_rows = []

                        current_paragraph.append(content)

        # –ó–∞–≤–µ—Ä—à–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–∞–±–ª–∏—Ü—É
        if table_rows:
            markdown_parts.append(self.format_table(table_rows))

        # –î–æ–±–∞–≤–∏—Ç—å –æ—Å—Ç–∞–≤—à–∏–π—Å—è —Ç–µ–∫—Å—Ç
        if current_paragraph:
            full_text = ' '.join(current_paragraph)
            sentences = full_text.split('. ')
            for sentence in sentences:
                if sentence.strip():
                    markdown_parts.append(sentence.strip() + '.')

        markdown_content = '\n\n'.join(markdown_parts) if markdown_parts else "*–¢–µ–∫—Å—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω*"

        # –î–æ–±–∞–≤–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
        header = f"# –ü–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {pdf_path.name}\n\n"
        header += f"**–°—Ç—Ä–∞–Ω–∏—Ü–∞:** {page_num}\n"
        header += f"**–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–µ—Ä–µ–∑ SmolDocling**\n"
        header += f"**–î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

        return header + markdown_content

    def format_table(self, table_rows: list) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ Markdown —Ç–∞–±–ª–∏—Ü—É"""
        if not table_rows:
            return ""

        # –ü—Ä–æ—Å—Ç–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
        markdown_table = []

        for i, row in enumerate(table_rows):
            # –ó–∞–º–µ–Ω–∏—Ç—å —Ç–∞–±—É–ª—è—Ü–∏–∏ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ |
            clean_row = row.replace('\t', '|').replace('  ', ' ')
            # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –µ—Å—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            if '|' not in clean_row:
                clean_row = clean_row.replace(' ', '|')

            markdown_table.append(clean_row)

            # –î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if i == 0 and len(table_rows) > 1:
                separators = ['---'] * len(clean_row.split('|'))
                markdown_table.append('|'.join(separators))

        return '\n'.join(markdown_table)

    def extract_detailed_winners_info(self, doctags: str) -> dict:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è—Ö"""
        print("   –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è—Ö...")

        text_content = doctags.replace('>', ' ').replace('<', ' ').lower()

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        winners_info = {
            "has_protocol": "–ø—Ä–æ—Ç–æ–∫–æ–ª" in text_content,
            "has_winners": any(word in text_content for word in ["–ø–æ–±–µ–¥–∏—Ç–µ–ª", "–ø–æ–±–µ–¥–∏–ª", "–≤—ã–∏–≥—Ä–∞–ª", "–ø–æ–±–µ–¥–∏—Ç", "–ø–æ–±–µ–¥"]),
            "has_contract": any(word in text_content for word in ["–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "–¥–æ–≥–æ–≤–æ—Ä", "—Å–¥–µ–ª–∫", "–∑–∞–∫—É–ø–∫", "–∫–æ–Ω–∫—É—Ä—Å"]),
            "has_amount": any(word in text_content for word in ["—Ä—É–±–ª", "—Å—É–º–º", "—Å—Ç–æ–∏–º–æ—Å—Ç", "—Ü–µ–Ω", "—Ç—ã—Å—è—á", "–º–∏–ª–ª–∏–æ–Ω"]),
            "has_commission": any(word in text_content for word in ["–∫–æ–º–∏—Å—Å–∏", "–∑–∞—Å–µ–¥–∞–Ω–∏", "—Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏", "—á–ª–µ–Ω"]),
            "has_participants": any(word in text_content for word in ["—É—á–∞—Å—Ç–Ω–∏–∫", "–ø–æ—Å—Ç–∞–≤—â–∏–∫", "–∑–∞—è–≤–∫"]),
            "has_decision": any(word in text_content for word in ["—Ä–µ—à–µ–Ω–∏", "–æ–ø—Ä–µ–¥–µ–ª", "–≤—ã–±–æ—Ä"]),
            "document_type": "protocol" if "–ø—Ä–æ—Ç–æ–∫–æ–ª" in text_content else "tender_docs" if "–∑–∞–∫—É–ø–∫" in text_content else "unknown",
            "extracted_text_length": len(text_content.strip()),
            "confidence_score": self.calculate_confidence(text_content)
        }

        return winners_info

    def calculate_confidence(self, text: str) -> float:
        """–û—Ü–µ–Ω–∏—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è"""
        score = 0.0

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
        protocol_keywords = ["–ø—Ä–æ—Ç–æ–∫–æ–ª", "–∑–∞—Å–µ–¥–∞–Ω–∏", "–∫–æ–º–∏—Å—Å–∏", "—Ä–µ—à–µ–Ω"]
        score += sum(1 for keyword in protocol_keywords if keyword in text) * 0.2

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–∫—É–ø–æ–∫
        tender_keywords = ["–∑–∞–∫—É–ø–∫", "–∫–æ–Ω—Ç—Ä–∞–∫—Ç", "–ø–æ–±–µ–¥–∏—Ç–µ–ª", "—Ç–µ–Ω–¥–µ—Ä"]
        score += sum(1 for keyword in tender_keywords if keyword in text) * 0.3

        # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
        if len(text) > 100:
            score += 0.3
        elif len(text) > 50:
            score += 0.2

        # –ü—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ —Ü–∏—Ñ—Ä (–Ω–æ–º–µ—Ä–∞, —Å—É–º–º—ã)
        if any(char.isdigit() for char in text):
            score += 0.2

        return min(score, 1.0)  # –ú–∞–∫—Å–∏–º—É–º 1.0

    def process_full_document(self, pdf_path: Path) -> dict:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        print(f"\nüîç –ü–û–õ–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–û–ö–£–ú–ï–ù–¢–ê: {pdf_path.name}")

        # –ê–Ω–∞–ª–∏–∑ PDF
        pdf_info = self.get_pdf_pages_info(pdf_path)
        print(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ PDF: {pdf_info}")

        all_results = {}

        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        total_pages = min(pdf_info.get('total_pages', 1), 5)  # –ú–∞–∫—Å–∏–º—É–º 5 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ—Å—Ç–∞

        for page_num in range(1, total_pages + 1):
            print(f"\nüìÑ –û–ë–†–ê–ë–û–¢–ö–ê –°–¢–†–ê–ù–ò–¶–´ {page_num}/{total_pages}")

            try:
                # –°–æ–∑–¥–∞—Ç—å HQ thumbnail
                base64_thumbnail = self.create_high_quality_thumbnail(pdf_path, page_num)

                # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏
                prompt_results = self.process_page_with_different_prompts(base64_thumbnail, page_num)

                # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                combined_doctags = self.combine_doctags_results(prompt_results)

                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ Markdown
                markdown_content = self.doctags_to_markdown(combined_doctags, pdf_path, page_num)

                # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                winners_info = self.extract_detailed_winners_info(combined_doctags)

                page_result = {
                    'page_number': page_num,
                    'pdf_info': pdf_info,
                    'prompt_results': prompt_results,
                    'combined_doctags': combined_doctags,
                    'markdown_content': markdown_content,
                    'winners_info': winners_info,
                    'processing_timestamp': datetime.now().isoformat()
                }

                all_results[f'page_{page_num}'] = page_result

                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                page_dir = OUTPUT_DIR / f"page_{page_num}"
                page_dir.mkdir(exist_ok=True)

                # DocTags
                with open(page_dir / f"{pdf_path.stem}_page_{page_num}_doctags.txt", "w", encoding="utf-8") as f:
                    f.write(combined_doctags)

                # Markdown
                with open(page_dir / f"{pdf_path.stem}_page_{page_num}_content.md", "w", encoding="utf-8") as f:
                    f.write(markdown_content)

                # –ê–Ω–∞–ª–∏–∑
                with open(page_dir / f"{pdf_path.stem}_page_{page_num}_analysis.json", "w", encoding="utf-8") as f:
                    json.dump({
                        'pdf_file': str(pdf_path),
                        'page_result': page_result
                    }, f, indent=2, ensure_ascii=False)

                print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {page_dir}")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                all_results[f'page_{page_num}'] = {'error': str(e)}

        return all_results

    def create_final_summary(self, all_results: dict, pdf_path: Path):
        """–°–æ–∑–¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É –ø–æ –≤—Å–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É"""
        print("\nüìã –°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –°–í–û–î–ö–ò...")

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ Markdown
        all_markdown_parts = []

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        summary_header = f"# –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –î–û–ö–£–ú–ï–ù–¢–ê: {pdf_path.name}\n\n"
        summary_header += f"**–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü:** {len(all_results)}\n"
        summary_header += f"**–î–∞—Ç–∞ –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        all_markdown_parts.append(summary_header)

        # –°–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_text_length = 0
        all_winners_info = []

        for page_key, page_result in all_results.items():
            if 'error' in page_result:
                continue

            page_num = page_result['page_number']
            markdown = page_result['markdown_content']
            winners_info = page_result['winners_info']

            all_markdown_parts.append(f"## –°–¢–†–ê–ù–ò–¶–ê {page_num}\n")
            all_markdown_parts.append(markdown)
            all_markdown_parts.append("\n---\n")

            total_text_length += winners_info.get('extracted_text_length', 0)
            all_winners_info.append(winners_info)

        # –î–æ–±–∞–≤–∏—Ç—å —Å–≤–æ–¥–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        summary_info = "\n## –°–í–û–î–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø\n\n"
        summary_info += f"- **–û–±—â–∏–π –æ–±—ä–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:** {total_text_length} —Å–∏–º–≤–æ–ª–æ–≤\n"

        if all_winners_info:
            avg_confidence = sum(info.get('confidence_score', 0) for info in all_winners_info) / len(all_winners_info)
            summary_info += f"- **–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:** {avg_confidence:.2f}\n"

            # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏
            has_protocol = any(info.get('has_protocol', False) for info in all_winners_info)
            has_contract = any(info.get('has_contract', False) for info in all_winners_info)
            has_winners = any(info.get('has_winners', False) for info in all_winners_info)

            summary_info += f"- **–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞:** {'–ü—Ä–æ—Ç–æ–∫–æ–ª' if has_protocol else '–î–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–∫—É–ø–æ–∫' if has_contract else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π'}\n"
            summary_info += f"- **–°–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è—Ö:** {'–î–∞' if has_winners else '–ù–µ—Ç'}\n"

        all_markdown_parts.append(summary_info)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ–ª–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        final_markdown = '\n'.join(all_markdown_parts)

        final_file = OUTPUT_DIR / f"{pdf_path.stem}_FULL_DOCUMENT.md"
        with open(final_file, "w", encoding="utf-8") as f:
            f.write(final_markdown)

        print(f"üíæ –ü–æ–ª–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {final_file}")
        print(f"üìä –û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {total_text_length} —Å–∏–º–≤–æ–ª–æ–≤")

        return final_markdown

    def run(self):
        print("üöÄ –ü–û–õ–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê PDF –ß–ï–†–ï–ó SMOLDOCLING")
        print(f"–§–∞–π–ª: {TEST_FILE}")

        if not self.wait_for_server_ready():
            print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
            return

        print("‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

        pdf_path = Path(TEST_FILE)
        if not pdf_path.exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
            return

        # –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        all_results = self.process_full_document(pdf_path)

        # –°–æ–∑–¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É
        final_markdown = self.create_final_summary(all_results, pdf_path)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç
        report_file = OUTPUT_DIR / f"{pdf_path.stem}_full_report.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump({
                'pdf_file': str(pdf_path),
                'processing_timestamp': datetime.now().isoformat(),
                'all_results': all_results,
                'final_summary': {
                    'total_pages_processed': len(all_results),
                    'total_text_extracted': sum(
                        result.get('winners_info', {}).get('extracted_text_length', 0)
                        for result in all_results.values()
                        if 'error' not in result
                    )
                }
            }, f, indent=2, ensure_ascii=False)

        print(f"üíæ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")

        print("\nüéâ –ü–û–õ–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –ø–∞–ø–∫–µ:")
        print(f"   {OUTPUT_DIR}")

        # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–µ–≤—å—é —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        preview_length = 1000
        preview = final_markdown[:preview_length] + ("..." if len(final_markdown) > preview_length else "")
        print("\nüìÑ –ü–†–ï–í–¨–Æ –ü–û–õ–ù–û–ì–û –î–û–ö–£–ú–ï–ù–¢–ê:")
        print("-" * 60)
        print(preview)
        print("-" * 60)

if __name__ == "__main__":
    processor = FullSmolDoclingProcessor()
    processor.run()
