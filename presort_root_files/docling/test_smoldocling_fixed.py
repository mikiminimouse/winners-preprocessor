#!/usr/bin/env python3
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ SmolDocling –Ω–∞ 3 PDF —Ñ–∞–π–ª–∞—Ö
—Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""
import os
import sys
import json
import time
import base64
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from PIL import Image
import io

try:
    import openai
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False
    print("‚ö†Ô∏è  openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")

try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False
    print("‚ö†Ô∏è  pdf2image –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdf2image")
    print("   –¢–∞–∫–∂–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è: sudo apt-get install poppler-utils")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è SmolDocling (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω)
API_TOKEN = "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"
BASE_URL = "https://d63e30af-085a-49f0-9724-8162da967af2.modelrun.inference.cloud.ru/v1"
MODEL_NAME = "model-run-4qigw-disease"  # –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è SmolDocling

# –ü—É—Ç–∏ –∫ —Ç–µ—Å—Ç–æ–≤—ã–º —Ñ–∞–π–ª–∞–º
TEST_FILES = [
    {
        "id": "UNIT_13bd07b0fa0ef660",
        "path": "/root/winners_preprocessor/normalized/UNIT_13bd07b0fa0ef660/files/–ü—Ä–æ—Ç–æ–∫–æ–ª —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∫–æ—Ç–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∑–∞—è–≤–æ–∫ ‚Ññ 35 –æ—Ç 19.11.2025.pdf",
        "description": "–°–∫–∞–Ω –∫–Ω–∏–∂–Ω–æ–≥–æ —Ä–∞–∑–≤–æ—Ä–æ—Ç–∞"
    },
    {
        "id": "UNIT_11c6ba8e496155c1",
        "path": "/root/winners_preprocessor/normalized/UNIT_11c6ba8e496155c1/files/tmp1jp9rv31.pdf",
        "description": "–î–æ–∫—É–º–µ–Ω—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞"
    },
    {
        "id": "UNIT_6e44cf32b40a2035",
        "path": "/root/winners_preprocessor/normalized/UNIT_6e44cf32b40a2035/files/–ü—Ä–æ—Ç–æ–∫–æ–ª –¢—Ä—É–±–∞ –ü–≠ 560.pdf",
        "description": "–î–æ–∫—É–º–µ–Ω—Ç –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞"
    }
]

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
OUTPUT_DIR = Path("/root/winners_preprocessor/output_smoldocling_fixed")
OUTPUT_DIR.mkdir(exist_ok=True)

class SmolDoclingFixedTester:
    def __init__(self):
        if not OPENAI_SDK_AVAILABLE:
            raise ImportError("openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π OpenAI –∫–ª–∏–µ–Ω—Ç —Å –Ω–∞—à–∏–º —Ç–æ–∫–µ–Ω–æ–º
        self.client = openai.OpenAI(
            api_key=API_TOKEN,
            base_url=BASE_URL,
            timeout=120.0
        )
        self.model = MODEL_NAME
        self.results = []

    def wait_for_server_ready(self, max_wait_time: int = 300) -> bool:
        """Wait for the inference server to be ready"""
        print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞ SmolDocling (–º–∞–∫—Å–∏–º—É–º {max_wait_time} —Å–µ–∫—É–Ω–¥)...")
        
        start_time = time.time()
        while time.time() - start_time < max_wait_time:
            try:
                # Send a simple test request
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": "Hello"}],
                    max_tokens=10,
                    temperature=0.5
                )
                
                if response.choices[0].message.content:
                    print("‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
                    return True
            except Exception as e:
                print(f"    –°–µ—Ä–≤–µ—Ä –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤... ({str(e)[:50]}...)")
                time.sleep(15)  # Wait 15 seconds before retrying
        
        print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ —Å—Ç–∞–ª –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏")
        return False

    def test_connection(self) -> bool:
        try:
            print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ SmolDocling...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç"}],
                max_tokens=10,
                temperature=0.5
            )
            print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ! –û—Ç–≤–µ—Ç: {response.choices[0].message.content.strip()}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            return False

    def optimize_image_for_processing(self, image_path: Path) -> Path:
        """Optimize image for processing by reducing size and quality"""
        print(f"      –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_path.name}")
        
        # Open image
        with Image.open(image_path) as img:
            # Convert to RGB if necessary
            if img.mode != 'RGB':
                img = img.convert('RGB')
            
            # Calculate new size (max 1200px on longest side)
            width, height = img.size
            max_size = 1200
            
            if width > height:
                if width > max_size:
                    new_width = max_size
                    new_height = int(height * (max_size / width))
                else:
                    new_width, new_height = width, height
            else:
                if height > max_size:
                    new_height = max_size
                    new_width = int(width * (max_size / height))
                else:
                    new_width, new_height = width, height
            
            # Resize if needed
            if new_width != width or new_height != height:
                img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                print(f"         –ò–∑–º–µ–Ω–µ–Ω —Ä–∞–∑–º–µ—Ä —Å {width}x{height} –Ω–∞ {new_width}x{new_height}")
            
            # Save optimized image
            optimized_path = image_path.parent / f"{image_path.stem}_optimized{image_path.suffix}"
            img.save(optimized_path, format='PNG', optimize=True, quality=85)
            
            # Check file size
            final_size = optimized_path.stat().st_size / (1024 * 1024)
            print(f"         –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {final_size:.2f} MB")
            
            return optimized_path

    def image_to_base64(self, image_path: Path) -> str:
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode('utf-8')

    def pdf_to_optimized_pages_images_base64(self, pdf_path: Path, output_dir: Path) -> List[str]:
        """Convert PDF to optimized images and return base64 strings"""
        if not PDF2IMAGE_AVAILABLE:
            raise ImportError("pdf2image not installed")
        
        try:
            # Convert all pages of PDF to PIL images with lower DPI for optimization
            print(f"         –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF —Å DPI=150 –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
            pil_images = convert_from_path(str(pdf_path), dpi=150)
            if not pil_images:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
            
            base64_images = []
            for i, img in enumerate(pil_images):
                # Save image to disk first
                image_filename = f"{pdf_path.stem}_page_{i+1}_raw.png"
                image_path = output_dir / image_filename
                img.save(image_path, format='PNG')
                
                # Optimize image
                optimized_path = self.optimize_image_for_processing(image_path)
                
                # Convert optimized image to base64
                base64_img = self.image_to_base64(optimized_path)
                base64_images.append(base64_img)
                
                print(f"         üñºÔ∏è  –°–æ–∑–¥–∞–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}: {optimized_path.name}")
                
            return base64_images
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            raise

    def create_docling_prompt(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è SmolDocling —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        return "Convert this page to docling."

    def process_single_page_docling(self, base64_image: str) -> Dict[str, Any]:
        """Process a single page/image with SmolDocling using correct prompt"""
        try:
            messages_content = [
                {"type": "text", "text": self.create_docling_prompt()},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
            ]
            
            print(f"      ‚û°Ô∏è  –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ SmolDocling —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º...")
            response_start_time = time.time()
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": messages_content}],
                max_tokens=4000,  # Reasonable limit for DocTags output
                temperature=0.0,  # Deterministic output
                response_format={"type": "text"}  # Allow text response for DocTags
            )
            response_time = time.time() - response_start_time
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            print(f"      ‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {response_time:.2f} —Å–µ–∫—É–Ω–¥")
            print(f"         –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response.choices[0].message.content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # Parse DocTags response
            doctags = response.choices[0].message.content.strip()
            
            return {
                "success": True,
                "doctags": doctags,
                "response_time": response_time,
                "tokens_used": tokens_used
            }
                
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    def process_file(self, file_info: Dict[str, Any], file_index: int) -> Optional[Dict[str, Any]]:
        file_path = Path(file_info["path"])
        file_id = file_info["id"]
        description = file_info["description"]
        
        file_output_dir = OUTPUT_DIR / file_id
        file_output_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n{'='*80}")
        print(f"[{file_index+1}/3] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file_path.name}")
        print(f"ID: {file_id}")
        print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {description}")
        print(f"–ü—É—Ç—å: {file_path}")
        print(f"{'='*80}")

        file_start_time = time.time()
        file_tokens_used = 0
        
        file_result = {
            "file_id": file_id,
            "file_name": file_path.name,
            "description": description,
            "path": str(file_path),
            "processing_time": 0.0,
            "tokens_used": 0,
            "pages": [],
            "status": "failed",
            "error": None
        }

        try:
            if not file_path.exists():
                raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            
            print(f"   üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF: {file_path.name}")
            print(f"      –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / 1024:.1f} KB")
            if not PDF2IMAGE_AVAILABLE:
                raise ImportError("pdf2image not installed")
            
            print(f"      –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü PDF...")
            base64_images = self.pdf_to_optimized_pages_images_base64(file_path, file_output_dir)
            print(f"      –í—Å–µ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {len(base64_images)}")
            
            # Process each page with correct SmolDocling prompt
            page_results = []
            for i, base64_image in enumerate(base64_images):
                page_num = i + 1
                print(f"      üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} –∏–∑ {len(base64_images)}")
                page_result = self.process_single_page_docling(base64_image)
                page_results.append(page_result)
                
                if page_result.get("success"):
                    file_tokens_used += page_result.get("tokens_used", 0)
                    
                    # Save DocTags to file
                    doctags_filename = f"{file_path.stem}_page_{page_num}_doctags.txt"
                    doctags_path = file_output_dir / doctags_filename
                    with open(doctags_path, "w", encoding="utf-8") as f:
                        f.write(page_result["doctags"])
                    print(f"      üíæ DocTags —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {doctags_path}")
                    
                    # Try to convert DocTags to JSON if possible
                    try:
                        # Some DocTags might be JSON-parseable
                        json_data = json.loads(page_result["doctags"])
                        json_filename = f"{file_path.stem}_page_{page_num}_parsed.json"
                        json_path = file_output_dir / json_filename
                        with open(json_path, "w", encoding="utf-8") as f:
                            json.dump(json_data, f, indent=2, ensure_ascii=False)
                        print(f"      üíæ –ü–∞—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {json_path}")
                    except json.JSONDecodeError:
                        print(f"      ‚ö†Ô∏è  DocTags –Ω–µ —è–≤–ª—è–µ—Ç—Å—è JSON, —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ —Ç–µ–∫—Å—Ç")
                else:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
            
            file_result["tokens_used"] = file_tokens_used
            
            # Save individual page results for reference
            page_results_path = file_output_dir / f"{file_path.stem}_page_results.json"
            with open(page_results_path, "w", encoding="utf-8") as f:
                json.dump(page_results, f, indent=2, ensure_ascii=False)
            print(f"      üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {page_results_path}")
            
            file_result["pages"] = page_results
            
            # Check if any page was processed successfully
            if any(r.get("success") for r in page_results):
                file_result["status"] = "success"
            else:
                file_result["status"] = "partial_success"
            
        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            file_result["error"] = str(e)
        
        file_result["processing_time"] = time.time() - file_start_time
        self.results.append(file_result)
        
        return file_result

    def generate_markdown_report(self):
        """Generate a markdown report of the results"""
        print(f"\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞...")
        report_path = OUTPUT_DIR / f"smoldocling_fixed_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é SmolDocling –Ω–∞ 3 PDF —Ñ–∞–π–ª–∞—Ö\n\n")
            f.write(f"**–î–∞—Ç–∞ –æ—Ç—á–µ—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"## –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n\n")
            f.write(f"- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç: 'Convert this page to docling.'\n")
            f.write(f"- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–º–∞–∫—Å. 1200px, –∫–∞—á–µ—Å—Ç–≤–æ 85%)\n")
            f.write(f"- ‚úÖ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏\n")
            f.write(f"- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ (4000)\n")
            f.write(f"- ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ DocTags —Ñ–æ—Ä–º–∞—Ç–∞\n\n")
            
            f.write(f"## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n")
            f.write(f"- –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: 3\n")
            successful_files = sum(1 for r in self.results if r['status'] in ['success', 'partial_success'])
            f.write(f"- –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_files}\n")
            f.write(f"- –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {sum(r['processing_time'] for r in self.results):.2f} —Å–µ–∫—É–Ω–¥\n")
            f.write(f"- –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {sum(r['tokens_used'] for r in self.results)}\n\n")

            f.write(f"## –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ —Ñ–∞–π–ª–∞–º\n\n")
            for file_result in self.results:
                f.write(f"### –§–∞–π–ª: `{file_result['file_name']}`\n")
                f.write(f"- ID: `{file_result['file_id']}`\n")
                f.write(f"- –û–ø–∏—Å–∞–Ω–∏–µ: {file_result['description']}\n")
                f.write(f"- –°—Ç–∞—Ç—É—Å: **{file_result['status'].upper()}**\n")
                f.write(f"- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {file_result['processing_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
                f.write(f"- –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {file_result['tokens_used']}\n")
                if file_result['error']:
                    f.write(f"- –û—à–∏–±–∫–∞: `{file_result['error']}`\n")
                f.write(f"\n")

                if file_result['pages']:
                    f.write(f"#### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º\n")
                    for i, page_result in enumerate(file_result['pages']):
                        f.write(f"**–°—Ç—Ä–∞–Ω–∏—Ü–∞ {i+1}:** ")
                        if page_result.get('success'):
                            f.write(f"‚úÖ –£—Å–ø–µ—à–Ω–æ ({page_result.get('response_time', 0):.2f} —Å–µ–∫, {page_result.get('tokens_used', 0)} —Ç–æ–∫–µ–Ω–æ–≤)\n")
                            
                            # Show DocTags preview
                            doctags = page_result.get('doctags', '')
                            if doctags:
                                preview = doctags[:200] + ('...' if len(doctags) > 200 else '')
                                f.write(f"```\n{preview}\n```\n")
                        else:
                            f.write(f"‚ùå –û—à–∏–±–∫–∞: {page_result.get('error', 'Unknown error')}\n")
                        f.write(f"\n")
                f.write(f"---\n\n")
        
        print(f"‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

    def run(self):
        print(f"\n{'='*80}")
        print(f"–ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï INTEGR–ê–¶–ò–ò SMOLDOCLING –ù–ê 3 PDF –§–ê–ô–õ–ê–•")
        print(f"{'='*80}")

        # Wait for server to be ready
        if not self.wait_for_server_ready():
            print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
            return

        if not self.test_connection():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å.")
            return

        print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(TEST_FILES)}")
        print(f"üéØ –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(TEST_FILES)} —Ñ–∞–π–ª–æ–≤")

        for i, file_info in enumerate(TEST_FILES):
            self.process_file(file_info, i)
        
        self.generate_markdown_report()

        print(f"\n{'='*80}")
        print(f"‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print(f"{'='*80}")

if __name__ == "__main__":
    tester = SmolDoclingFixedTester()
    tester.run()
