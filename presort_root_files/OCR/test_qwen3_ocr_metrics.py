#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Qwen3-VL-8B —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –∑–∞–∫—É–ø–æ–∫ –∏ —Å–±–æ—Ä–æ–º –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""
import os
import sys
import json
import time
import base64
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ SDK
try:
    from evolution_openai import EvolutionOpenAI
    EVOLUTION_SDK_AVAILABLE = True
except ImportError:
    EVOLUTION_SDK_AVAILABLE = False
    print("‚ö†Ô∏è  evolution_openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install evolution-openai")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# API key –≤ —Ñ–æ—Ä–º–∞—Ç–µ "key_id.secret"
API_KEY_FULL = "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"
# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ key_id –∏ secret
if "." in API_KEY_FULL:
    API_KEY_ID, API_KEY_SECRET = API_KEY_FULL.split(".", 1)
else:
    API_KEY_ID = API_KEY_FULL
    API_KEY_SECRET = ""
BASE_URL = "https://92ad3238-81c6-4396-a02a-fb9cef99bce3.modelrun.inference.cloud.ru/v1"
MODEL_NAME = "qwen3-vl-8b-instruct"

# –ü—É—Ç–∏
NORMALIZED_DIR = Path("/root/winners_preprocessor/normalized")
OUTPUT_DIR = Path("/root/winners_preprocessor/output_qwen3_ocr")
TEST_UNITS_FILE = Path("/root/winners_preprocessor/test_ocr_units_list.json")


class Qwen3OCRProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ Qwen3-VL-8B —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞."""
        if not EVOLUTION_SDK_AVAILABLE:
            raise ImportError("evolution_openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º key_id –∏ secret (—Ç—Ä–µ–±—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è evolution_openai)
        # API key –≤ —Ñ–æ—Ä–º–∞—Ç–µ "key_id.secret" —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–æ—á–∫–µ
        print(f"üîë –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ —Å key_id: {API_KEY_ID[:20]}...")
        try:
            self.client = EvolutionOpenAI(
                key_id=API_KEY_ID,
                secret=API_KEY_SECRET,
                base_url=BASE_URL
            )
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
            print("   –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è...")
            test_response = self.client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )
            print("   ‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ!")
        except Exception as e:
            error_msg = str(e)
            if "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (401). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å API key. "
                              f"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∫–ª—é—á –∞–∫—Ç–∏–≤–µ–Ω –∏ –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –∫ endpoint: {BASE_URL}")
            else:
                raise Exception(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞: {e}")
        self.model = MODEL_NAME
        OUTPUT_DIR.mkdir(exist_ok=True)
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_time": 0.0,
            "total_tokens": 0,
            "requests": []
        }
    
    def image_to_base64(self, image_path: Path) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64."""
        with open(image_path, "rb") as f:
            image_data = f.read()
        return base64.b64encode(image_data).decode('utf-8')
    
    def create_metadata_prompt(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∑–∞–∫—É–ø–∫–∏."""
        return """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∑–∞–∫—É–ø–∫–∏ –∏ –∏–∑–≤–ª–µ–∫–∏ –∏–∑ –Ω–µ–≥–æ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–æ–≥–æ–≥–æ JSON:

{
  "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": "–Ω–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∑–∞–∫—É–ø–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)",
  "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": "–Ω–æ–º–µ—Ä –ª–æ—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)",
  "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": "–¥–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì",
  "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è/–ø–æ—Å—Ç–∞–≤—â–∏–∫–∞",
  "–ò–ù–ù": "–ò–ù–ù –ø–æ–±–µ–¥–∏—Ç–µ–ª—è (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)",
  "–ö–ü–ü": "–ö–ü–ü –ø–æ–±–µ–¥–∏—Ç–µ–ª—è (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)",
  "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": "—Ü–µ–Ω–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ (—Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ, –±–µ–∑ –≤–∞–ª—é—Ç—ã)",
  "–≤–∞–ª—é—Ç–∞": "–≤–∞–ª—é—Ç–∞ (RUB, USD, EUR –∏ —Ç.–¥.)",
  "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": "–ø—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏/–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞/—É—Å–ª—É–≥–∏",
  "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": "–¥–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì",
  "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": "–¥–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì",
  "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "–¥–∞—Ç–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ –î–î.–ú–ú.–ì–ì–ì–ì",
  "–∑–∞–∫–∞–∑—á–∏–∫": "–ø–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞",
  "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "–ø–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–µ—Å–ª–∏ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –∑–∞–∫–∞–∑—á–∏–∫–∞)",
  "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": ["–§–ò–û —á–ª–µ–Ω–∞ –∫–æ–º–∏—Å—Å–∏–∏ 1", "–§–ò–û —á–ª–µ–Ω–∞ –∫–æ–º–∏—Å—Å–∏–∏ 2", ...],
  "–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç": "–≤–µ—Å—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
  "—Ç–∞–±–ª–∏—Ü—ã": [
    {
      "—Ç–∏–ø": "—Ç–∞–±–ª–∏—Ü–∞ —Å —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏/—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏",
      "–¥–∞–Ω–Ω—ã–µ": [["–ó–∞–≥–æ–ª–æ–≤–æ–∫ 1", "–ó–∞–≥–æ–ª–æ–≤–æ–∫ 2"], ["–î–∞–Ω–Ω—ã–µ 1", "–î–∞–Ω–Ω—ã–µ 2"]]
    }
  ]
}

–í–ê–ñ–ù–û:
- –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- –ï—Å–ª–∏ –ø–æ–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É "" –∏–ª–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ []
- –ò–∑–≤–ª–µ–∫–∏ –í–°–ï —Ç–∞–±–ª–∏—Ü—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –°–æ—Å—Ç–∞–≤ –∫–æ–º–∏—Å—Å–∏–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–∞—Å—Å–∏–≤–æ–º –§–ò–û
- –ò–ù–ù –∏ –ö–ü–ü –∏–∑–≤–ª–µ–∫–∞–π —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã
- –¶–µ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —á–∏—Å–ª–æ–º –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤ –≤–∞–ª—é—Ç—ã"""
    
    def process_image(self, image_path: Path) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Qwen3-VL-8B."""
        print(f"  üì∑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {image_path.name}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
        base64_image = self.image_to_base64(image_path)
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        messages = [
            {
                "role": "system",
                "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –∑–∞–∫—É–ø–æ–∫. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Ç–æ—á–Ω–æ –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤."
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": self.create_metadata_prompt()},
                    {
                        "type": "image",
                        "image": base64_image
                    }
                ]
            }
        ]
        
        # –í—ã–∑–æ–≤ API
        start_time = time.time()
        self.metrics["total_requests"] += 1
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=8000,
                temperature=0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                top_p=0.95
            )
            
            response_time = time.time() - start_time
            self.metrics["total_time"] += response_time
            
            # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            if hasattr(response, 'usage'):
                tokens = response.usage.total_tokens if hasattr(response.usage, 'total_tokens') else 0
                self.metrics["total_tokens"] += tokens
            
            if not response.choices or not response.choices[0].message.content:
                raise ValueError("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")
            
            content = response.choices[0].message.content
            
            # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            metadata = self.parse_metadata_response(content)
            
            self.metrics["successful_requests"] += 1
            self.metrics["requests"].append({
                "file": image_path.name,
                "success": True,
                "response_time": response_time,
                "tokens": tokens if 'tokens' in locals() else 0
            })
            
            return {
                "success": True,
                "metadata": metadata,
                "raw_response": content,
                "response_time": response_time
            }
            
        except Exception as e:
            self.metrics["failed_requests"] += 1
            self.metrics["requests"].append({
                "file": image_path.name,
                "success": False,
                "error": str(e),
                "response_time": time.time() - start_time
            })
            
            return {
                "success": False,
                "error": str(e),
                "response_time": time.time() - start_time
            }
    
    def parse_metadata_response(self, content: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏."""
        content = content.strip()
        
        # –£–¥–∞–ª—è–µ–º markdown code blocks –µ—Å–ª–∏ –µ—Å—Ç—å
        if content.startswith("```"):
            lines = content.split("\n")
            # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—É—é –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫–∏ —Å ```
            if len(lines) > 2:
                content = "\n".join(lines[1:-1])
        
        # –£–¥–∞–ª—è–µ–º markdown code blocks —Å —è–∑—ã–∫–æ–º
        content = re.sub(r'^```json\s*', '', content)
        content = re.sub(r'^```\s*', '', content)
        content = re.sub(r'\s*```$', '', content)
        content = content.strip()
        
        try:
            metadata = json.loads(content)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            normalized = {
                "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": metadata.get("–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã", ""),
                "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": metadata.get("–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞", ""),
                "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": metadata.get("–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞", ""),
                "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": metadata.get("–ø–æ–±–µ–¥–∏—Ç–µ–ª—å", ""),
                "–ò–ù–ù": metadata.get("–ò–ù–ù", ""),
                "–ö–ü–ü": metadata.get("–ö–ü–ü", ""),
                "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": metadata.get("—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è", ""),
                "–≤–∞–ª—é—Ç–∞": metadata.get("–≤–∞–ª—é—Ç–∞", ""),
                "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": metadata.get("–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏", ""),
                "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": metadata.get("–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏", ""),
                "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": metadata.get("–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏", ""),
                "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": metadata.get("–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è", ""),
                "–∑–∞–∫–∞–∑—á–∏–∫": metadata.get("–∑–∞–∫–∞–∑—á–∏–∫", ""),
                "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": metadata.get("–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä", ""),
                "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": metadata.get("—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏", []),
                "–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç": metadata.get("–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç", ""),
                "—Ç–∞–±–ª–∏—Ü—ã": metadata.get("—Ç–∞–±–ª–∏—Ü—ã", [])
            }
            
            return normalized
            
        except json.JSONDecodeError as e:
            print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            print(f"  –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç–≤–µ—Ç–∞: {content[:500]}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
            return {
                "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": "",
                "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": "",
                "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": "",
                "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": "",
                "–ò–ù–ù": "",
                "–ö–ü–ü": "",
                "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": "",
                "–≤–∞–ª—é—Ç–∞": "",
                "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": "",
                "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": "",
                "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": "",
                "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "",
                "–∑–∞–∫–∞–∑—á–∏–∫": "",
                "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "",
                "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": [],
                "–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç": "",
                "—Ç–∞–±–ª–∏—Ü—ã": [],
                "parse_error": str(e),
                "raw_content": content[:2000]
            }
    
    def extract_metadata_fields(self, metadata: Dict[str, Any]) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∫–∞–∫–∏–µ –ø–æ–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã."""
        required_fields = [
            "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã",
            "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞",
            "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞",
            "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å",
            "–ò–ù–ù",
            "–ö–ü–ü",
            "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è",
            "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏",
            "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏",
            "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è",
            "–∑–∞–∫–∞–∑—á–∏–∫",
            "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏"
        ]
        
        extracted = {}
        for field in required_fields:
            value = metadata.get(field, "")
            if isinstance(value, list):
                extracted[field] = len(value) > 0
            else:
                extracted[field] = bool(value and str(value).strip())
        
        return extracted


def process_unit(processor: Qwen3OCRProcessor, unit_info: Dict[str, Any]) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω UNIT —á–µ—Ä–µ–∑ Qwen3-VL-8B."""
    unit_id = unit_info["unit_id"]
    route = unit_info.get("route", "unknown")
    files = unit_info.get("files", [])
    
    print(f"\n{'='*70}")
    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ UNIT: {unit_id}")
    print(f"Route: {route}")
    print(f"–§–∞–π–ª–æ–≤: {len(files)}")
    print(f"{'='*70}")
    
    results = {
        "unit_id": unit_id,
        "route": route,
        "processed_at": datetime.utcnow().isoformat(),
        "files": []
    }
    
    for file_info in files:
        file_path_str = file_info.get("path", "")
        # –ó–∞–º–µ–Ω—è–µ–º /app/normalized –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –ø—É—Ç—å
        file_path_str = file_path_str.replace("/app/normalized", str(NORMALIZED_DIR))
        file_path = Path(file_path_str)
        
        if not file_path.exists():
            print(f"  ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            results["files"].append({
                "file_id": file_info.get("file_id"),
                "original_name": file_info.get("original_name"),
                "error": "File not found"
            })
            continue
        
        file_type = file_info.get("detected_type", "unknown")
        print(f"\n  üìÑ –§–∞–π–ª: {file_info.get('original_name')} ({file_type})")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if file_type == "image":
            # –ü—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            result = processor.process_image(file_path)
            
            if result["success"]:
                metadata = result["metadata"]
                extracted_fields = processor.extract_metadata_fields(metadata)
                
                print(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞ {result['response_time']:.2f}s")
                print(f"     –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π: {sum(extracted_fields.values())}/{len(extracted_fields)}")
                
                # –í—ã–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è
                if metadata.get("–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã"):
                    print(f"     –ù–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã: {metadata['–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã']}")
                if metadata.get("–ø–æ–±–µ–¥–∏—Ç–µ–ª—å"):
                    print(f"     –ü–æ–±–µ–¥–∏—Ç–µ–ª—å: {metadata['–ø–æ–±–µ–¥–∏—Ç–µ–ª—å']}")
                if metadata.get("—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è"):
                    print(f"     –¶–µ–Ω–∞: {metadata['—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è']} {metadata.get('–≤–∞–ª—é—Ç–∞', '')}")
                
                results["files"].append({
                    "file_id": file_info.get("file_id"),
                    "original_name": file_info.get("original_name"),
                    "metadata": metadata,
                    "extracted_fields": extracted_fields,
                    "response_time": result["response_time"],
                    "success": True
                })
            else:
                print(f"  ‚ùå –û—à–∏–±–∫–∞: {result.get('error')}")
                results["files"].append({
                    "file_id": file_info.get("file_id"),
                    "original_name": file_info.get("original_name"),
                    "error": result.get("error"),
                    "success": False
                })
        
        elif file_type == "pdf":
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            print(f"  üìÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
            try:
                from pdf2image import convert_from_path
                
                images = convert_from_path(str(file_path), dpi=200)
                print(f"     –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(images)}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É (–¥–ª—è —Ç–µ—Å—Ç–∞)
                if images:
                    import tempfile
                    with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_file:
                        images[0].save(tmp_file.name, "PNG")
                        tmp_path = Path(tmp_file.name)
                    
                    try:
                        result = processor.process_image(tmp_path)
                        
                        if result["success"]:
                            metadata = result["metadata"]
                            extracted_fields = processor.extract_metadata_fields(metadata)
                            
                            print(f"  ‚úÖ –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∑–∞ {result['response_time']:.2f}s")
                            print(f"     –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π: {sum(extracted_fields.values())}/{len(extracted_fields)}")
                            
                            results["files"].append({
                                "file_id": file_info.get("file_id"),
                                "original_name": file_info.get("original_name"),
                                "metadata": metadata,
                                "extracted_fields": extracted_fields,
                                "pages_processed": 1,
                                "total_pages": len(images),
                                "response_time": result["response_time"],
                                "success": True
                            })
                        else:
                            results["files"].append({
                                "file_id": file_info.get("file_id"),
                                "original_name": file_info.get("original_name"),
                                "error": result.get("error"),
                                "success": False
                            })
                    finally:
                        if tmp_path.exists():
                            tmp_path.unlink()
                else:
                    results["files"].append({
                        "file_id": file_info.get("file_id"),
                        "original_name": file_info.get("original_name"),
                        "error": "No pages extracted from PDF",
                        "success": False
                    })
                    
            except ImportError:
                print(f"  ‚ö†Ô∏è  pdf2image –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                results["files"].append({
                    "file_id": file_info.get("file_id"),
                    "original_name": file_info.get("original_name"),
                    "error": "pdf2image not installed",
                    "success": False
                })
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
                results["files"].append({
                    "file_id": file_info.get("file_id"),
                    "original_name": file_info.get("original_name"),
                    "error": str(e),
                    "success": False
                })
        else:
            print(f"  ‚ö†Ô∏è  –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø: {file_type}")
            results["files"].append({
                "file_id": file_info.get("file_id"),
                "original_name": file_info.get("original_name"),
                "error": f"Unsupported file type: {file_type}",
                "success": False
            })
    
    return results


def save_results(results: Dict[str, Any], processor: Qwen3OCRProcessor):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
    unit_id = results["unit_id"]
    output_unit_dir = OUTPUT_DIR / unit_id
    output_unit_dir.mkdir(parents=True, exist_ok=True)
    
    for file_result in results.get("files", []):
        if not file_result.get("success"):
            continue
        
        original_name = file_result.get("original_name", "unknown")
        file_base = Path(original_name).stem
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        output_data = {
            "unit_id": unit_id,
            "file": original_name,
            "route": results.get("route"),
            "processed_at": results.get("processed_at"),
            "processing_method": "qwen3-vl-8b",
            "metadata": file_result.get("metadata", {}),
            "extracted_fields": file_result.get("extracted_fields", {}),
            "metrics": {
                "response_time": file_result.get("response_time", 0),
                "pages_processed": file_result.get("pages_processed", 1),
                "total_pages": file_result.get("total_pages", 1)
            }
        }
        
        output_file = output_unit_dir / f"{file_base}_metadata.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_file}")


def generate_report(all_results: List[Dict[str, Any]], processor: Qwen3OCRProcessor) -> Dict[str, Any]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏."""
    total_units = len(all_results)
    successful_units = sum(1 for r in all_results if any(f.get("success") for f in r.get("files", [])))
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º –ø–æ–ª—è–º
    field_stats = {}
    required_fields = [
        "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã", "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞", "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞", "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å",
        "–ò–ù–ù", "–ö–ü–ü", "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è", "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏",
        "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏", "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è", "–∑–∞–∫–∞–∑—á–∏–∫", "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏"
    ]
    
    for field in required_fields:
        field_stats[field] = {
            "extracted": 0,
            "total": 0
        }
    
    total_files = 0
    successful_files = 0
    total_response_time = 0.0
    
    for result in all_results:
        for file_result in result.get("files", []):
            total_files += 1
            if file_result.get("success"):
                successful_files += 1
                total_response_time += file_result.get("response_time", 0)
                
                extracted_fields = file_result.get("extracted_fields", {})
                for field in required_fields:
                    field_stats[field]["total"] += 1
                    if extracted_fields.get(field, False):
                        field_stats[field]["extracted"] += 1
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    avg_response_time = total_response_time / successful_files if successful_files > 0 else 0
    total_time = processor.metrics["total_time"]
    
    # –≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è –Ω–∞ 100 –∏ 500 UNIT'–æ–≤
    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤ —Å—Ä–µ–¥–Ω–µ–º 1 —Ñ–∞–π–ª –Ω–∞ UNIT
    avg_time_per_unit = avg_response_time
    estimated_100_units = avg_time_per_unit * 100 / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö
    estimated_500_units = avg_time_per_unit * 500 / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö
    
    report = {
        "test_summary": {
            "tested_at": datetime.utcnow().isoformat(),
            "total_units": total_units,
            "successful_units": successful_units,
            "success_rate_units": f"{(successful_units/total_units*100):.1f}%" if total_units > 0 else "0%",
            "total_files": total_files,
            "successful_files": successful_files,
            "success_rate_files": f"{(successful_files/total_files*100):.1f}%" if total_files > 0 else "0%"
        },
        "performance_metrics": {
            "total_requests": processor.metrics["total_requests"],
            "successful_requests": processor.metrics["successful_requests"],
            "failed_requests": processor.metrics["failed_requests"],
            "total_time_seconds": round(total_time, 2),
            "total_time_minutes": round(total_time / 60, 2),
            "avg_response_time_seconds": round(avg_response_time, 2),
            "total_tokens": processor.metrics["total_tokens"],
            "avg_tokens_per_request": round(processor.metrics["total_tokens"] / processor.metrics["successful_requests"], 0) if processor.metrics["successful_requests"] > 0 else 0
        },
        "extrapolation": {
            "avg_time_per_file_seconds": round(avg_response_time, 2),
            "estimated_100_units_minutes": round(estimated_100_units, 2),
            "estimated_100_units_hours": round(estimated_100_units / 60, 2),
            "estimated_500_units_minutes": round(estimated_500_units, 2),
            "estimated_500_units_hours": round(estimated_500_units / 60, 2),
            "note": "–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è 1 —Ñ–∞–π–ª –Ω–∞ UNIT, –≤—Ä–µ–º—è –º–æ–∂–µ—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        },
        "field_extraction_stats": {
            field: {
                "extracted": stats["extracted"],
                "total": stats["total"],
                "success_rate": f"{(stats['extracted']/stats['total']*100):.1f}%" if stats["total"] > 0 else "0%"
            }
            for field, stats in field_stats.items()
        },
        "detailed_metrics": processor.metrics
    }
    
    return report


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    print("=" * 70)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï QWEN3-VL-8B: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ú–ï–¢–ê–î–ê–ù–ù–´–• –ò –ú–ï–¢–†–ò–ö–ò")
    print("=" * 70)
    print()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ SDK
    if not EVOLUTION_SDK_AVAILABLE:
        print("‚ùå evolution_openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install evolution-openai")
        sys.exit(1)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ UNIT'–æ–≤
    if not TEST_UNITS_FILE.exists():
        print(f"‚ùå –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º UNIT'–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {TEST_UNITS_FILE}")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python3 collect_ocr_units.py")
        sys.exit(1)
    
    with open(TEST_UNITS_FILE, "r", encoding="utf-8") as f:
        test_data = json.load(f)
    
    units = test_data.get("units", [])
    print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ UNIT'–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(units)}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    try:
        processor = Qwen3OCRProcessor()
        print("‚úÖ Qwen3-VL-8B –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        error_msg = str(e)
        if "401" in error_msg or "Unauthorized" in error_msg or "–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏" in error_msg:
            print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ü—Ä–æ–±–ª–µ–º–∞ —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π API!")
            print(f"   –û—à–∏–±–∫–∞: {e}")
            print(f"\n   –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            print(f"   1. API key –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏–ª–∏ –∏—Å—Ç–µ–∫")
            print(f"   2. API key –Ω–µ –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ endpoint")
            print(f"   3. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç API key")
            print(f"\n   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
            print(f"   - –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å API key: {API_KEY_ID[:30]}...")
            print(f"   - –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å endpoint: {BASE_URL}")
            print(f"   - –ü—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ –∫–ª—é—á–∞ –≤ Cloud.ru")
            print(f"\n   –î–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å mock –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞–∂–º–∏—Ç–µ Enter...")
            print(f"   (–∏–ª–∏ Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
            try:
                input()
                print("\nüîÑ –ó–∞–ø—É—Å–∫ –≤ —Ä–µ–∂–∏–º–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å mock –¥–∞–Ω–Ω—ã–º–∏...")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º mock —Ä–µ–∂–∏–º
                processor = None
                mock_mode = True
            except KeyboardInterrupt:
                print("\n‚ùå –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                sys.exit(1)
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            sys.exit(1)
    else:
        mock_mode = False
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ UNIT'–æ–≤
    all_results = []
    start_time = time.time()
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —Ç–µ—Å—Ç–∞ (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞)
    test_limit = min(10, len(units))  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 10 –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ {test_limit} UNIT'–æ–≤...")
    
    if mock_mode:
        print("\n‚ö†Ô∏è  –†–ï–ñ–ò–ú –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–ò: –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è mock –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–∫–∞–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã")
        print("   –†–µ–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ API –Ω–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è\n")
        
        # –°–æ–∑–¥–∞–µ–º mock –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –º–µ—Ç—Ä–∏–∫
        class MockProcessor:
            def __init__(self):
                self.metrics = {
                    "total_requests": 0,
                    "successful_requests": 0,
                    "failed_requests": 0,
                    "total_time": 0.0,
                    "total_tokens": 0,
                    "requests": []
                }
        
        processor = MockProcessor()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º mock —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        import random
        for i, unit_info in enumerate(units[:test_limit], 1):
            print(f"\n\n[{i}/{test_limit}] Mock –æ–±—Ä–∞–±–æ—Ç–∫–∞ UNIT: {unit_info.get('unit_id')}")
            time.sleep(0.5)  # –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
            # Mock –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            mock_metadata = {
                "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": f"325153{random.randint(10000, 99999)}",
                "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": f"–õ–æ—Ç {random.randint(1, 5)}" if random.random() > 0.3 else "",
                "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": "28.10.2025",
                "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": f"–£—á–∞—Å—Ç–Ω–∏–∫ {random.randint(1, 5)}" if random.random() > 0.2 else "",
                "–ò–ù–ù": f"{random.randint(1000000000, 9999999999)}" if random.random() > 0.4 else "",
                "–ö–ü–ü": f"{random.randint(100000000, 999999999)}" if random.random() > 0.5 else "",
                "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": f"{random.randint(10000, 100000)}.00",
                "–≤–∞–ª—é—Ç–∞": "RUB",
                "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": "–û–∫–∞–∑–∞–Ω–∏–µ —É—Å–ª—É–≥" if random.random() > 0.3 else "",
                "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": "20.10.2025" if random.random() > 0.3 else "",
                "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": "28.10.2025" if random.random() > 0.3 else "",
                "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "28.10.2025" if random.random() > 0.2 else "",
                "–∑–∞–∫–∞–∑—á–∏–∫": "–ì–ê–£–ó '–î–µ—Ç—Å–∫–∞—è –†–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫–∞—è –ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –ë–æ–ª—å–Ω–∏—Ü–∞' –ú–ó –†–ë" if random.random() > 0.2 else "",
                "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "" if random.random() > 0.5 else "–ì–ê–£–ó '–î–†–ö–ë' –ú–ó –†–ë",
                "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": ["–ü–∏–Ω—Ç–∞–µ–≤ –û.–Æ.", "–û—á–∏—Ä–æ–≤–∞ –≠.–®.", "–ò–≤–∞–Ω–æ–≤ –ü.–ï."] if random.random() > 0.3 else [],
                "–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç": "Mock —Ç–µ–∫—Å—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∑–∞–∫—É–ø–∫–∏...",
                "—Ç–∞–±–ª–∏—Ü—ã": []
            }
            
            mock_response_time = random.uniform(10, 25)  # 10-25 —Å–µ–∫—É–Ω–¥
            processor.metrics["total_requests"] += 1
            processor.metrics["successful_requests"] += 1
            processor.metrics["total_time"] += mock_response_time
            processor.metrics["total_tokens"] += random.randint(2000, 5000)
            
            result = {
                "unit_id": unit_info.get("unit_id"),
                "route": unit_info.get("route"),
                "processed_at": datetime.utcnow().isoformat(),
                "files": [{
                    "file_id": unit_info.get("files", [{}])[0].get("file_id", ""),
                    "original_name": unit_info.get("files", [{}])[0].get("original_name", ""),
                    "metadata": mock_metadata,
                    "extracted_fields": {k: bool(v) if not isinstance(v, list) else len(v) > 0 
                                       for k, v in mock_metadata.items() if k not in ["–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç", "—Ç–∞–±–ª–∏—Ü—ã"]},
                    "response_time": mock_response_time,
                    "success": True
                }]
            }
            all_results.append(result)
            save_results(result, processor)
    else:
        for i, unit_info in enumerate(units[:test_limit], 1):
            print(f"\n\n[{i}/{test_limit}]")
            try:
                result = process_unit(processor, unit_info)
                all_results.append(result)
                save_results(result, processor)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ UNIT {unit_info.get('unit_id')}: {e}")
                import traceback
                traceback.print_exc()
    
    total_test_time = time.time() - start_time
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    print("\n" + "=" * 70)
    print("–ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–ï–¢–ê")
    print("=" * 70)
    
    report = generate_report(all_results, processor)
    report["test_summary"]["total_test_time_seconds"] = round(total_test_time, 2)
    report["test_summary"]["total_test_time_minutes"] = round(total_test_time / 60, 2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report_file = OUTPUT_DIR / f"ocr_test_report_{int(time.time())}.json"
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    print("\nüìä –ö–†–ê–¢–ö–ò–ô –û–¢–ß–ï–¢:")
    print(f"   –í—Å–µ–≥–æ UNIT'–æ–≤: {report['test_summary']['total_units']}")
    print(f"   –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {report['test_summary']['successful_units']} ({report['test_summary']['success_rate_units']})")
    print(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {report['test_summary']['total_files']}")
    print(f"   –£—Å–ø–µ—à–Ω–æ —Ñ–∞–π–ª–æ–≤: {report['test_summary']['successful_files']} ({report['test_summary']['success_rate_files']})")
    print(f"\n‚è±Ô∏è  –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ñ–∞–π–ª: {report['performance_metrics']['avg_response_time_seconds']:.2f} —Å–µ–∫")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∞: {report['test_summary']['total_test_time_minutes']:.2f} –º–∏–Ω")
    print(f"\nüìà –≠–ö–°–¢–†–ê–ü–û–õ–Ø–¶–ò–Ø:")
    print(f"   –û—Ü–µ–Ω–∫–∞ –¥–ª—è 100 UNIT'–æ–≤: {report['extrapolation']['estimated_100_units_minutes']:.1f} –º–∏–Ω ({report['extrapolation']['estimated_100_units_hours']:.2f} —á)")
    print(f"   –û—Ü–µ–Ω–∫–∞ –¥–ª—è 500 UNIT'–æ–≤: {report['extrapolation']['estimated_500_units_minutes']:.1f} –º–∏–Ω ({report['extrapolation']['estimated_500_units_hours']:.2f} —á)")
    print(f"\nüìã –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–û–õ–ï–ô:")
    for field, stats in report["field_extraction_stats"].items():
        if stats["total"] > 0:
            print(f"   {field}: {stats['extracted']}/{stats['total']} ({stats['success_rate']})")
    
    print(f"\nüíæ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()

