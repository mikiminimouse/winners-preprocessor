#!/usr/bin/env python3
"""
–ü–†–ê–í–ò–õ–¨–ù–´–ô PDF –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –†–ï–ê–õ–¨–ù–´–ú –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
- pdfplumber –¥–ª—è PDF —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–ª–æ–µ–º
- pytesseract –¥–ª—è —Å–∫–∞–Ω–æ–≤
- OpenAI –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞
"""
import os
import sys
import json
import time
import openai
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

try:
    import pdfplumber
    from pdf2image import convert_from_path
    import pytesseract
    from PIL import Image
    PDF_TOOLS_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip3 install pdfplumber pdf2image pytesseract pillow")
    sys.exit(1)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
GRANITE_API_URL = os.getenv(
    "GRANITE_API_URL",
    "https://8cb66180-db3a-4963-8068-51f87e716259.modelrun.inference.cloud.ru/v1"
)
GRANITE_API_TOKEN = os.getenv(
    "GRANITE_API_TOKEN",
    "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"
)
GRANITE_MODEL = os.getenv("GRANITE_MODEL", "granite-docling")


class RealPDFProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –†–ï–ê–õ–¨–ù–û–ì–û –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
    
    def __init__(self, output_dir: str = "output_real_extraction"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        self.client = openai.OpenAI(
            api_key=GRANITE_API_TOKEN,
            base_url=GRANITE_API_URL
        )
        
        print(f"‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        print(f"   Output: {self.output_dir}")
    
    def extract_text_from_pdf(self, pdf_path: Path) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF (—Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π + OCR –¥–ª—è —Å–∫–∞–Ω–æ–≤)
        
        Returns:
            Dict —Å –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º, —Ç–∞–±–ª–∏—Ü–∞–º–∏, –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        print(f"\nüìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑: {pdf_path.name}")
        
        all_text = []
        all_tables = []
        pages_count = 0
        has_text_layer = False
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                pages_count = len(pdf.pages)
                print(f"   –°—Ç—Ä–∞–Ω–∏—Ü: {pages_count}")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    print(f"   –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{pages_count}...", end=" ")
                    
                    # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π
                    page_text = page.extract_text()
                    
                    if page_text and len(page_text.strip()) > 50:
                        # –ï—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π
                        all_text.append(f"\n\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} ---\n\n{page_text}")
                        has_text_layer = True
                        print("‚úÖ —Ç–µ–∫—Å—Ç", end="")
                    else:
                        # –°–∫–∞–Ω - –Ω—É–∂–µ–Ω OCR
                        print("üîç OCR", end="")
                        ocr_text = self._ocr_page(pdf_path, page_num)
                        if ocr_text:
                            all_text.append(f"\n\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} (OCR) ---\n\n{ocr_text}")
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
                    tables = page.extract_tables()
                    if tables:
                        print(f" + {len(tables)} —Ç–∞–±–ª.", end="")
                        for table_idx, table in enumerate(tables):
                            all_tables.append({
                                "page": page_num,
                                "table_number": table_idx + 1,
                                "data": table
                            })
                    
                    print()  # –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
            
            combined_text = "\n".join(all_text)
            
            print(f"\n‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ:")
            print(f"   –¢–µ–∫—Å—Ç–∞: {len(combined_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –¢–∞–±–ª–∏—Ü: {len(all_tables)}")
            print(f"   –ú–µ—Ç–æ–¥: {'–¢–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π' if has_text_layer else 'OCR'}")
            
            return {
                "success": True,
                "text": combined_text,
                "tables": all_tables,
                "pages": pages_count,
                "has_text_layer": has_text_layer,
                "method": "pdfplumber + pytesseract"
            }
            
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            return {
                "success": False,
                "error": str(e),
                "text": "",
                "tables": []
            }
    
    def _ocr_page(self, pdf_path: Path, page_num: int) -> str:
        """OCR –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            images = convert_from_path(
                str(pdf_path),
                dpi=200,
                first_page=page_num,
                last_page=page_num
            )
            
            if images:
                # OCR
                text = pytesseract.image_to_string(images[0], lang='rus+eng')
                return text
            
            return ""
        except Exception as e:
            print(f"\n      ‚ö†Ô∏è  OCR –æ—à–∏–±–∫–∞: {e}")
            return ""
    
    def extract_metadata(self, text: str) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Granite API
        """
        print(f"\nüîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞ ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)...")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç
        max_chars = 15000
        if len(text) > max_chars:
            text_for_extraction = text[:max_chars]
            print(f"   ‚ö†Ô∏è  –¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω: {len(text)} ‚Üí {max_chars}")
        else:
            text_for_extraction = text
        
        if len(text_for_extraction.strip()) < 100:
            print(f"   ‚ö†Ô∏è  –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
            return self._get_empty_metadata()
        
        # –ü—Ä–æ–º–ø—Ç
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–æ—Ç–æ–∫–æ–ª –∑–∞–∫—É–ø–∫–∏ –∏ –∏–∑–≤–ª–µ–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ.

–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:
{text_for_extraction}

–ò–∑–≤–ª–µ–∫–∏ —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–ª—è (–µ—Å–ª–∏ –ø–æ–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —É–∫–∞–∂–∏ null):

{{
  "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": "–Ω–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã/–∑–∞–∫—É–ø–∫–∏/—Ç–µ–Ω–¥–µ—Ä–∞",
  "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": "–Ω–æ–º–µ—Ä –ª–æ—Ç–∞",
  "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": "–¥–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –î–î.–ú–ú.–ì–ì–ì–ì",
  "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": "–ø–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è",
  "–ò–ù–ù": "–ò–ù–ù –ø–æ–±–µ–¥–∏—Ç–µ–ª—è",
  "–ö–ü–ü": "–ö–ü–ü –ø–æ–±–µ–¥–∏—Ç–µ–ª—è",
  "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": "—Ü–µ–Ω–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞ (—á–∏—Å–ª–æ)",
  "–≤–∞–ª—é—Ç–∞": "RUB/USD/EUR",
  "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": "–ø—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏",
  "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": "–î–î.–ú–ú.–ì–ì–ì–ì",
  "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": "–î–î.–ú–ú.–ì–ì–ì–ì",
  "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "–î–î.–ú–ú.–ì–ì–ì–ì",
  "–∑–∞–∫–∞–∑—á–∏–∫": "–ø–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞",
  "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "–ø–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞",
  "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": ["–§–ò–û —á–ª–µ–Ω–∞ 1", "–§–ò–û —á–ª–µ–Ω–∞ 2"],
  "—É—á–∞—Å—Ç–Ω–∏–∫–∏": [
    {{
      "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": "–Ω–∞–∑–≤–∞–Ω–∏–µ —É—á–∞—Å—Ç–Ω–∏–∫–∞",
      "—Å—Ç–∞—Ç—É—Å": "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å/–æ—Ç–∫–ª–æ–Ω–µ–Ω/–¥–æ–ø—É—â–µ–Ω",
      "—Å—É–º–º–∞": "—Å—É–º–º–∞",
      "–Ω–æ–º–µ—Ä_–∑–∞—è–≤–∫–∏": "–Ω–æ–º–µ—Ä"
    }}
  ]
}}

–í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ/–ø–æ—Å–ª–µ."""
        
        try:
            response = self.client.chat.completions.create(
                model=GRANITE_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=4000,
                temperature=0.0
            )
            
            raw_response = response.choices[0].message.content
            metadata = self._parse_json_response(raw_response)
            
            filled_fields = sum(1 for v in metadata.values() if v and v != "")
            print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π: {filled_fields}/16")
            
            return metadata
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return self._get_empty_metadata()
    
    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞"""
        import re
        
        response = response.strip()
        if response.startswith('```json'):
            response = response[7:]
        if response.startswith('```'):
            response = response[3:]
        if response.endswith('```'):
            response = response[:-3]
        response = response.strip()
        
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                pass
        
        return self._get_empty_metadata()
    
    def _get_empty_metadata(self) -> Dict[str, Any]:
        """–ü—É—Å—Ç–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        return {
            "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": None,
            "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": None,
            "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": None,
            "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": None,
            "–ò–ù–ù": None,
            "–ö–ü–ü": None,
            "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": None,
            "–≤–∞–ª—é—Ç–∞": None,
            "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": None,
            "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": None,
            "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": None,
            "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": None,
            "–∑–∞–∫–∞–∑—á–∏–∫": None,
            "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": None,
            "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": [],
            "—É—á–∞—Å—Ç–Ω–∏–∫–∏": []
        }
    
    def process_pdf(self, pdf_path: Path) -> Dict[str, Any]:
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF"""
        print(f"\n{'='*70}")
        print(f"üöÄ –û–ë–†–ê–ë–û–¢–ö–ê: {pdf_path.name}")
        print(f"{'='*70}")
        
        start_time = time.time()
        
        # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        extraction = self.extract_text_from_pdf(pdf_path)
        
        if not extraction["success"]:
            return {
                "success": False,
                "error": extraction.get("error"),
                "pdf_path": str(pdf_path)
            }
        
        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        metadata = self.extract_metadata(extraction["text"])
        metadata["–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç"] = extraction["text"]
        metadata["—Ç–∞–±–ª–∏—Ü—ã"] = extraction["tables"]
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ Markdown
        markdown = self._create_markdown(pdf_path, extraction["text"], extraction["tables"], metadata)
        
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self._save_results(pdf_path, markdown, metadata, extraction)
        
        processing_time = time.time() - start_time
        
        print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ –∑–∞ {processing_time:.2f}—Å")
        print(f"   –¢–µ–∫—Å—Ç–∞: {len(extraction['text'])} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –¢–∞–±–ª–∏—Ü: {len(extraction['tables'])}")
        
        return {
            "success": True,
            "pdf_path": str(pdf_path),
            "text_length": len(extraction["text"]),
            "tables_count": len(extraction["tables"]),
            "processing_time": processing_time
        }
    
    def _create_markdown(
        self,
        pdf_path: Path,
        text: str,
        tables: List[Dict],
        metadata: Dict[str, Any]
    ) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ Markdown"""
        md = f"# {pdf_path.name}\n\n"
        md += f"**–î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        md += "---\n\n"
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        md += "## üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n\n"
        md += f"- **–ù–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã:** {metadata.get('–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã') or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n"
        md += f"- **–ù–æ–º–µ—Ä –ª–æ—Ç–∞:** {metadata.get('–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞') or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n"
        md += f"- **–î–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞:** {metadata.get('–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞') or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n"
        md += f"- **–ü–æ–±–µ–¥–∏—Ç–µ–ª—å:** {metadata.get('–ø–æ–±–µ–¥–∏—Ç–µ–ª—å') or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n"
        md += f"- **–ò–ù–ù:** {metadata.get('–ò–ù–ù') or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n"
        md += f"- **–ö–ü–ü:** {metadata.get('–ö–ü–ü') or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n"
        md += f"- **–¶–µ–Ω–∞:** {metadata.get('—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è') or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'} {metadata.get('–≤–∞–ª—é—Ç–∞') or ''}\n"
        md += f"- **–ü—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏:** {metadata.get('–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏') or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n"
        md += f"- **–ó–∞–∫–∞–∑—á–∏–∫:** {metadata.get('–∑–∞–∫–∞–∑—á–∏–∫') or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n\n"
        
        # –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
        md += "## üìÑ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n\n"
        md += text + "\n\n"
        
        # –¢–∞–±–ª–∏—Ü—ã
        if tables:
            md += f"## üìä –¢–∞–±–ª–∏—Ü—ã ({len(tables)})\n\n"
            for table_info in tables:
                md += f"### –¢–∞–±–ª–∏—Ü–∞ {table_info['table_number']} (–°—Ç—Ä–∞–Ω–∏—Ü–∞ {table_info['page']})\n\n"
                table_data = table_info['data']
                if table_data:
                    # Markdown —Ç–∞–±–ª–∏—Ü–∞
                    for i, row in enumerate(table_data):
                        md += "| " + " | ".join(str(cell or "") for cell in row) + " |\n"
                        if i == 0:
                            md += "|" + "|".join([" --- "] * len(row)) + "|\n"
                md += "\n"
        
        return md
    
    def _save_results(
        self,
        pdf_path: Path,
        markdown: str,
        metadata: Dict[str, Any],
        extraction: Dict[str, Any]
    ):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        base_name = pdf_path.stem
        
        # Markdown
        md_file = self.output_dir / f"{base_name}.md"
        with open(md_file, "w", encoding="utf-8") as f:
            f.write(markdown)
        print(f"   üíæ {md_file.name}")
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ JSON
        metadata_file = self.output_dir / f"{base_name}_metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        print(f"   üíæ {metadata_file.name}")
        
        # –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        full_file = self.output_dir / f"{base_name}_full.json"
        with open(full_file, "w", encoding="utf-8") as f:
            json.dump({
                "source": pdf_path.name,
                "processed_at": datetime.now().isoformat(),
                "extraction_info": {
                    "pages": extraction["pages"],
                    "has_text_layer": extraction["has_text_layer"],
                    "method": extraction["method"],
                    "text_length": len(extraction["text"]),
                    "tables_count": len(extraction["tables"])
                },
                "metadata": metadata
            }, f, indent=2, ensure_ascii=False)
        print(f"   üíæ {full_file.name}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="–†–ï–ê–õ–¨–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞")
    parser.add_argument("pdf_path", help="–ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
    parser.add_argument("--output", "-o", default="output_real_extraction", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    parser.add_argument("--limit", "-l", type=int, help="–õ–∏–º–∏—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    args = parser.parse_args()
    
    pdf_path = Path(args.pdf_path)
    processor = RealPDFProcessor(output_dir=args.output)
    
    if pdf_path.is_file():
        # –û–¥–∏–Ω —Ñ–∞–π–ª
        result = processor.process_pdf(pdf_path)
        sys.exit(0 if result["success"] else 1)
    
    elif pdf_path.is_dir():
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        pdf_files = list(pdf_path.glob("*.pdf"))
        
        if not pdf_files:
            print(f"‚ùå PDF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {pdf_path}")
            sys.exit(1)
        
        if args.limit:
            pdf_files = pdf_files[:args.limit]
        
        print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ PDF —Ñ–∞–π–ª–æ–≤: {len(pdf_files)}")
        
        success_count = 0
        for i, pdf_file in enumerate(pdf_files, 1):
            print(f"\n[{i}/{len(pdf_files)}]")
            result = processor.process_pdf(pdf_file)
            if result["success"]:
                success_count += 1
        
        print(f"\n{'='*70}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count}/{len(pdf_files)}")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {processor.output_dir}")
        print(f"{'='*70}")
    
    else:
        print(f"‚ùå –ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
        sys.exit(1)


if __name__ == "__main__":
    main()

