#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Qwen3-VL-8B –∫–∞–∫ –≤–Ω–µ—à–Ω–µ–≥–æ ML inference –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è AST JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ Docling.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen3-VL-8B –¥–ª—è:
- OCR (Image ‚Üí text)
- Layout detection / segmentation
- Table extraction
- –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ AST JSON (text, tables, layout, metadata)
"""
import os
import sys
import json
import time
import base64
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ SDK
try:
    from evolution_openai import EvolutionOpenAI
    EVOLUTION_SDK_AVAILABLE = True
except ImportError:
    EVOLUTION_SDK_AVAILABLE = False
    print("‚ö†Ô∏è  evolution_openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install evolution-openai")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_KEY = "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"
BASE_URL = "https://92ad3238-81c6-4396-a02a-fb9cef99bce3.modelrun.inference.cloud.ru/v1"
MODEL_NAME = "qwen3-vl-8b-instruct"

# –ü—É—Ç–∏
NORMALIZED_DIR = Path("/root/winners_preprocessor/normalized")
OUTPUT_DIR = Path("/root/winners_preprocessor/output_qwen3")
TEST_UNITS_FILE = Path("/root/winners_preprocessor/test_units_list.json")


class Qwen3VisionProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ Qwen3-VL-8B."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞."""
        if not EVOLUTION_SDK_AVAILABLE:
            raise ImportError("evolution_openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        self.client = EvolutionOpenAI(
            api_key=API_KEY,
            base_url=BASE_URL
        )
        self.model = MODEL_NAME
        OUTPUT_DIR.mkdir(exist_ok=True)
    
    def image_to_base64(self, image_path: Path) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64."""
        with open(image_path, "rb") as f:
            image_data = f.read()
        return base64.b64encode(image_data).decode('utf-8')
    
    def create_ast_prompt(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è AST JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ Docling."""
        return """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ –∏–∑ –Ω–µ–≥–æ –≤—Å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –≤–∫–ª—é—á–∞—è —Ç–µ–∫—Å—Ç, —Ç–∞–±–ª–∏—Ü—ã, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä–æ–≥–æ–≥–æ JSON —Å–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π:

{
  "text": "–ø–æ–ª–Ω—ã–π –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
  "tables": [
    {
      "type": "table",
      "rows": [
        ["–ó–∞–≥–æ–ª–æ–≤–æ–∫ 1", "–ó–∞–≥–æ–ª–æ–≤–æ–∫ 2"],
        ["–î–∞–Ω–Ω—ã–µ 1", "–î–∞–Ω–Ω—ã–µ 2"]
      ],
      "bbox": [x1, y1, x2, y2]
    }
  ],
  "layout": {
    "pages": [
      {
        "page_num": 1,
        "blocks": [
          {
            "type": "text" | "title" | "paragraph" | "list",
            "text": "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –±–ª–æ–∫–∞",
            "bbox": [x1, y1, x2, y2]
          }
        ]
      }
    ],
    "sections": [
      {
        "title": "–ù–∞–∑–≤–∞–Ω–∏–µ —Å–µ–∫—Ü–∏–∏",
        "level": 1,
        "content": "—Ç–µ–∫—Å—Ç —Å–µ–∫—Ü–∏–∏"
      }
    ]
  },
  "metadata": {
    "title": "–∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
    "author": "–∞–≤—Ç–æ—Ä (–µ—Å–ª–∏ –µ—Å—Ç—å)",
    "date": "–¥–∞—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)",
    "pages_count": 1
  }
}

–í–ê–ñ–ù–û:
- –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ []
- –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ [x1, y1, x2, y2]
- –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, —Å–ø–∏—Å–∫–∏)
- –ò–∑–≤–ª–µ–∫–∏ –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã —Å –∏—Ö –¥–∞–Ω–Ω—ã–º–∏"""
    
    def process_image(self, image_path: Path) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Qwen3-VL-8B."""
        print(f"  üì∑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_path.name}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
        base64_image = self.image_to_base64(image_path)
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        messages = [
            {
                "role": "system",
                "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": self.create_ast_prompt()},
                    {
                        "type": "image",
                        "image": base64_image
                    }
                ]
            }
        ]
        
        # –í—ã–∑–æ–≤ API
        start_time = time.time()
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=8000,
                temperature=0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                top_p=0.95
            )
            
            response_time = time.time() - start_time
            
            if not response.choices or not response.choices[0].message.content:
                raise ValueError("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")
            
            content = response.choices[0].message.content
            
            # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            ast_json = self.parse_ast_response(content)
            
            return {
                "success": True,
                "ast": ast_json,
                "raw_response": content,
                "response_time": response_time
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": time.time() - start_time
            }
    
    def parse_ast_response(self, content: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç AST JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏."""
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –≤ –æ—Ç–≤–µ—Ç–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç –≤ markdown –∏–ª–∏ —Ç–µ–∫—Å—Ç)
        content = content.strip()
        
        # –£–¥–∞–ª—è–µ–º markdown code blocks –µ—Å–ª–∏ –µ—Å—Ç—å
        if content.startswith("```"):
            lines = content.split("\n")
            # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—É—é –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫–∏ —Å ```
            content = "\n".join(lines[1:-1])
        
        # –£–¥–∞–ª—è–µ–º markdown code blocks —Å —è–∑—ã–∫–æ–º
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        
        content = content.strip()
        
        try:
            ast = json.loads(content)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if not isinstance(ast, dict):
                raise ValueError("AST –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            normalized_ast = {
                "text": ast.get("text", ""),
                "tables": ast.get("tables", []),
                "layout": ast.get("layout", {
                    "pages": [],
                    "sections": [],
                    "blocks": []
                }),
                "metadata": ast.get("metadata", {})
            }
            
            return normalized_ast
            
        except json.JSONDecodeError as e:
            print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            print(f"  –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç–≤–µ—Ç–∞: {content[:500]}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–∏ –æ—à–∏–±–∫–µ
            return {
                "text": "",
                "tables": [],
                "layout": {"pages": [], "sections": [], "blocks": []},
                "metadata": {},
                "parse_error": str(e),
                "raw_content": content[:1000]
            }


def process_unit(processor: Qwen3VisionProcessor, unit_info: Dict[str, Any]) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω UNIT —á–µ—Ä–µ–∑ Qwen3-VL-8B."""
    unit_id = unit_info["unit_id"]
    route = unit_info.get("route", "unknown")
    files = unit_info.get("files", [])
    
    print(f"\n{'='*70}")
    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ UNIT: {unit_id}")
    print(f"Route: {route}")
    print(f"–§–∞–π–ª–æ–≤: {len(files)}")
    print(f"{'='*70}")
    
    results = {
        "unit_id": unit_id,
        "route": route,
        "processed_at": datetime.utcnow().isoformat(),
        "files": []
    }
    
    for file_info in files:
        file_path_str = file_info.get("path", "")
        # –ó–∞–º–µ–Ω—è–µ–º /app/normalized –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –ø—É—Ç—å
        file_path_str = file_path_str.replace("/app/normalized", str(NORMALIZED_DIR))
        file_path = Path(file_path_str)
        
        if not file_path.exists():
            print(f"  ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            results["files"].append({
                "file_id": file_info.get("file_id"),
                "original_name": file_info.get("original_name"),
                "error": "File not found"
            })
            continue
        
        file_type = file_info.get("detected_type", "unknown")
        print(f"\n  üìÑ –§–∞–π–ª: {file_info.get('original_name')} ({file_type})")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if file_type == "image":
            # –ü—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            result = processor.process_image(file_path)
            
            if result["success"]:
                ast = result["ast"]
                print(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞ {result['response_time']:.2f}s")
                print(f"     –¢–µ–∫—Å—Ç: {len(ast.get('text', ''))} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"     –¢–∞–±–ª–∏—Ü: {len(ast.get('tables', []))}")
                
                results["files"].append({
                    "file_id": file_info.get("file_id"),
                    "original_name": file_info.get("original_name"),
                    "ast": ast,
                    "response_time": result["response_time"],
                    "success": True
                })
            else:
                print(f"  ‚ùå –û—à–∏–±–∫–∞: {result.get('error')}")
                results["files"].append({
                    "file_id": file_info.get("file_id"),
                    "original_name": file_info.get("original_name"),
                    "error": result.get("error"),
                    "success": False
                })
        
        elif file_type == "pdf":
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            print(f"  üìÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
            try:
                from pdf2image import convert_from_path
                
                images = convert_from_path(str(file_path), dpi=200)
                print(f"     –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(images)}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                page_results = []
                combined_ast = {
                    "text": "",
                    "tables": [],
                    "layout": {"pages": [], "sections": [], "blocks": []},
                    "metadata": {"pages_count": len(images)}
                }
                
                for page_num, image in enumerate(images, 1):
                    print(f"     –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}/{len(images)}...")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    import tempfile
                    with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp_file:
                        image.save(tmp_file.name, "PNG")
                        tmp_path = Path(tmp_file.name)
                    
                    try:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                        result = processor.process_image(tmp_path)
                        
                        if result["success"]:
                            page_ast = result["ast"]
                            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                            combined_ast["text"] += f"\n\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} ---\n\n{page_ast.get('text', '')}"
                            combined_ast["tables"].extend(page_ast.get("tables", []))
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ layout
                            page_layout = {
                                "page_num": page_num,
                                "blocks": page_ast.get("layout", {}).get("blocks", [])
                            }
                            combined_ast["layout"]["pages"].append(page_layout)
                            
                            page_results.append({
                                "page": page_num,
                                "success": True,
                                "response_time": result["response_time"]
                            })
                        else:
                            page_results.append({
                                "page": page_num,
                                "success": False,
                                "error": result.get("error")
                            })
                    finally:
                        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                        if tmp_path.exists():
                            tmp_path.unlink()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                results["files"].append({
                    "file_id": file_info.get("file_id"),
                    "original_name": file_info.get("original_name"),
                    "ast": combined_ast,
                    "pages_processed": len([r for r in page_results if r.get("success")]),
                    "total_pages": len(images),
                    "page_results": page_results,
                    "success": any(r.get("success") for r in page_results)
                })
                
                print(f"  ‚úÖ PDF –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len([r for r in page_results if r.get('success')])}/{len(images)} —Å—Ç—Ä–∞–Ω–∏—Ü")
                
            except ImportError:
                print(f"  ‚ö†Ô∏è  pdf2image –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdf2image")
                print(f"     –¢–∞–∫–∂–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è poppler-utils: sudo apt-get install poppler-utils")
                results["files"].append({
                    "file_id": file_info.get("file_id"),
                    "original_name": file_info.get("original_name"),
                    "error": "pdf2image not installed",
                    "success": False
                })
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF: {e}")
                results["files"].append({
                    "file_id": file_info.get("file_id"),
                    "original_name": file_info.get("original_name"),
                    "error": str(e),
                    "success": False
                })
        
        elif file_type == "docx":
            # –î–ª—è DOCX –Ω—É–∂–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—á–µ—Ä–µ–∑ LibreOffice –∏–ª–∏ python-docx + reportlab)
            print(f"  ‚ö†Ô∏è  –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è DOCX –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫")
            print(f"     –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å DOCX –≤ PDF, –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PDF –æ–±—Ä–∞–±–æ—Ç–∫—É")
            results["files"].append({
                "file_id": file_info.get("file_id"),
                "original_name": file_info.get("original_name"),
                "error": "DOCX to images conversion not implemented. Convert to PDF first.",
                "success": False
            })
        
        else:
            print(f"  ‚ö†Ô∏è  –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_type}")
            results["files"].append({
                "file_id": file_info.get("file_id"),
                "original_name": file_info.get("original_name"),
                "error": f"Unsupported file type: {file_type}",
                "success": False
            })
    
    return results


def save_results(results: Dict[str, Any]):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ Docling."""
    unit_id = results["unit_id"]
    output_unit_dir = OUTPUT_DIR / unit_id
    output_unit_dir.mkdir(parents=True, exist_ok=True)
    
    for file_result in results.get("files", []):
        if not file_result.get("success"):
            continue
        
        original_name = file_result.get("original_name", "unknown")
        file_base = Path(original_name).stem
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º AST JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ Docling
        ast = file_result.get("ast", {})
        output_data = {
            "unit_id": unit_id,
            "file": original_name,
            "route": results.get("route"),
            "detected_type": "image",
            "needs_ocr": False,
            "status": "processed",
            "processing_method": "qwen3-vl-8b",
            "text": ast.get("text", ""),
            "tables": ast.get("tables", []),
            "metadata": ast.get("metadata", {}),
            "layout": ast.get("layout", {
                "pages": [],
                "sections": [],
                "blocks": []
            }),
            "metrics": {
                "unit_id": unit_id,
                "file_name": original_name,
                "route": results.get("route"),
                "processing_times": {
                    "qwen3_vision": file_result.get("response_time", 0)
                },
                "file_stats": {
                    "text_length": len(ast.get("text", "")),
                    "tables_extracted": len(ast.get("tables", [])),
                    "pages_count": len(ast.get("layout", {}).get("pages", []))
                },
                "status": "completed",
                "created_at": results.get("processed_at")
            }
        }
        
        output_file = output_unit_dir / f"{file_base}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_file}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    print("=" * 70)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï QWEN3-VL-8B –î–õ–Ø –§–û–†–ú–ò–†–û–í–ê–ù–ò–Ø AST JSON")
    print("=" * 70)
    print()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ SDK
    if not EVOLUTION_SDK_AVAILABLE:
        print("‚ùå evolution_openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install evolution-openai")
        sys.exit(1)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ UNIT'–æ–≤
    if not TEST_UNITS_FILE.exists():
        print(f"‚ùå –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º UNIT'–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {TEST_UNITS_FILE}")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python3 collect_test_units.py")
        sys.exit(1)
    
    with open(TEST_UNITS_FILE, "r", encoding="utf-8") as f:
        test_data = json.load(f)
    
    units = test_data.get("units", [])
    print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ UNIT'–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(units)}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    try:
        processor = Qwen3VisionProcessor()
        print("‚úÖ Qwen3-VL-8B –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        sys.exit(1)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ UNIT'–æ–≤
    all_results = []
    for i, unit_info in enumerate(units, 1):
        print(f"\n\n[{i}/{len(units)}]")
        try:
            result = process_unit(processor, unit_info)
            all_results.append(result)
            save_results(result)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ UNIT {unit_info.get('unit_id')}: {e}")
            import traceback
            traceback.print_exc()
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 70)
    print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 70)
    
    total = len(all_results)
    successful = sum(1 for r in all_results if any(f.get("success") for f in r.get("files", [])))
    
    print(f"–í—Å–µ–≥–æ UNIT'–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total}")
    print(f"–£—Å–ø–µ—à–Ω–æ: {successful}")
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_DIR}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π –æ—Ç—á–µ—Ç
    report_file = OUTPUT_DIR / f"test_report_{int(time.time())}.json"
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump({
            "tested_at": datetime.utcnow().isoformat(),
            "total_units": total,
            "successful_units": successful,
            "results": all_results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()

