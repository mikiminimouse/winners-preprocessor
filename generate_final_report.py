#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
"""
import json
import sys
import requests
from datetime import datetime
from typing import Dict, Any, List

def get_metrics(session_id: str = None) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑ API."""
    url = "http://localhost:8080/metrics/processing"
    if session_id:
        url += f"?session_id={session_id}"
    
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Error: {response.status_code}")
        sys.exit(1)
    
    return response.json()


def format_duration(started_at: str, completed_at: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    try:
        start = datetime.fromisoformat(started_at.replace('Z', '+00:00'))
        end = datetime.fromisoformat(completed_at.replace('Z', '+00:00'))
        duration = end - start
        seconds = duration.total_seconds()
        
        if seconds < 60:
            return f"~{int(seconds)} —Å–µ–∫—É–Ω–¥"
        elif seconds < 3600:
            return f"~{int(seconds / 60)} –º–∏–Ω—É—Ç"
        else:
            hours = int(seconds / 3600)
            minutes = int((seconds % 3600) / 60)
            return f"~{hours}—á {minutes}–º"
    except:
        return "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"


def generate_report(metrics: Dict[str, Any]) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown."""
    lines = []
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    lines.append("# –û–¢–ß–ï–¢ –û–ë –û–ë–†–ê–ë–û–¢–ö–ï –î–û–ö–£–ú–ï–ù–¢–û–í")
    lines.append("")
    lines.append(f"**Session ID:** {metrics['session_id']}  ")
    lines.append(f"**–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞:** {metrics['started_at']}  ")
    lines.append(f"**–í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è:** {metrics.get('completed_at', 'N/A')}  ")
    
    duration = format_duration(metrics['started_at'], metrics.get('completed_at', metrics['started_at']))
    lines.append(f"**–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** {duration}")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    s = metrics.get('summary', {})
    lines.append("## –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    lines.append("")
    lines.append("| –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å | –ó–Ω–∞—á–µ–Ω–∏–µ |")
    lines.append("|------------|----------|")
    lines.append(f"| **–í—Å–µ–≥–æ –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤** | {s.get('total_input_files', 0)} |")
    lines.append(f"| **–°–æ–∑–¥–∞–Ω–æ unit'–æ–≤** | {s.get('total_units', 0)} |")
    
    total_files = s.get('total_input_files', 1)
    total_units = s.get('total_units', 0)
    success_rate = (total_units / total_files * 100) if total_files > 0 else 0
    lines.append(f"| **–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏** | {success_rate:.1f}% |")
    lines.append(f"| **–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞—Ä—Ö–∏–≤–æ–≤** | {s.get('total_archives', 0)} |")
    lines.append(f"| **–ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∏–∑ –∞—Ä—Ö–∏–≤–æ–≤** | {s.get('total_extracted', 0)} |")
    lines.append(f"| **–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫** | {s.get('total_errors', 0)} |")
    
    total_errors = s.get('total_errors', 0)
    error_rate = (total_errors / total_files * 100) if total_files > 0 else 0
    lines.append(f"| **–ü—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫** | {error_rate:.1f}% |")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
    lines.append("## –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –†–ê–°–®–ò–†–ï–ù–ò–Ø–ú")
    lines.append("")
    lines.append("| –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ |")
    lines.append("|------------|------------|")
    by_ext = s.get('by_extension', {})
    for ext, count in sorted(by_ext.items(), key=lambda x: -x[1]):
        percent = (count / total_files * 100) if total_files > 0 else 0
        lines.append(f"| `{ext}` | {count} —Ñ–∞–π–ª–∞ ({percent:.1f}%) |")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º
    lines.append("## –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –û–ü–†–ï–î–ï–õ–ï–ù–ù–´–ú –¢–ò–ü–ê–ú")
    lines.append("")
    lines.append("| –¢–∏–ø —Ñ–∞–π–ª–∞ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ |")
    lines.append("|-----------|------------|")
    by_type = s.get('by_detected_type', {})
    for ftype, count in sorted(by_type.items(), key=lambda x: -x[1]):
        lines.append(f"| `{ftype}` | {count} |")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ PDF
    pdf_stats = s.get('pdf_statistics', {})
    if pdf_stats and pdf_stats.get('total_pdf', 0) > 0:
        lines.append("## –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û PDF")
        lines.append("")
        total_pdf = pdf_stats.get('total_pdf', 0)
        with_text = pdf_stats.get('pdf_with_text_layer', 0)
        needs_ocr = pdf_stats.get('pdf_requires_ocr', 0)
        
        lines.append("| –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –ü—Ä–æ—Ü–µ–Ω—Ç |")
        lines.append("|------------|------------|---------|")
        lines.append(f"| **–í—Å–µ–≥–æ PDF —Ñ–∞–π–ª–æ–≤** | {total_pdf} | 100% |")
        text_percent = (with_text / total_pdf * 100) if total_pdf > 0 else 0
        lines.append(f"| –° —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–ª–æ–µ–º (–Ω–µ —Ç—Ä–µ–±—É—é—Ç OCR) | {with_text} | {text_percent:.1f}% |")
        ocr_percent = (needs_ocr / total_pdf * 100) if total_pdf > 0 else 0
        lines.append(f"| –¢—Ä–µ–±—É—é—Ç OCR | {needs_ocr} | {ocr_percent:.1f}% |")
        lines.append("")
        lines.append("---")
        lines.append("")
    
    # –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏–≤–æ–≤
    lines.append("## –ê–ù–ê–õ–ò–ó –ê–†–•–ò–í–û–í")
    lines.append("")
    archives = metrics.get('archives_extracted', [])
    total_archives = len(archives)
    successful = sum(1 for a in archives if a.get('success', False))
    failed = total_archives - successful
    
    lines.append(f"**–í—Å–µ–≥–æ –∞—Ä—Ö–∏–≤–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ:** {total_archives}  ")
    lines.append(f"**–£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–æ:** {successful}  ")
    lines.append(f"**–ù–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–æ:** {failed}")
    lines.append("")
    
    if archives:
        lines.append("### –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∞—Ä—Ö–∏–≤–∞–º:")
        lines.append("")
        
        for i, a in enumerate(archives, 1):
            lines.append(f"{i}. **{a['original_file']}**")
            lines.append(f"   - Archive ID: `{a['archive_id']}`")
            lines.append(f"   - –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {a.get('extracted_count', 0)}")
            lines.append(f"   - –£—Å–ø–µ—à–Ω–æ: {'‚úÖ' if a.get('success', False) else '‚ùå'}")
            
            if a.get('success', False):
                # –§–∞–π–ª—ã –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
                files_by_ext = a.get('files_by_extension', {})
                if files_by_ext:
                    lines.append(f"   - **–§–∞–π–ª—ã –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º:**")
                    for ext, count in sorted(files_by_ext.items(), key=lambda x: -x[1]):
                        lines.append(f"     - `{ext}`: {count} —Ñ–∞–π–ª–æ–≤")
                
                # –§–∞–π–ª—ã –ø–æ —Ç–∏–ø–∞–º
                files_by_type = a.get('files_by_type', {})
                if files_by_type:
                    lines.append(f"   - **–§–∞–π–ª—ã –ø–æ —Ç–∏–ø–∞–º:**")
                    for ftype, count in sorted(files_by_type.items(), key=lambda x: -x[1]):
                        lines.append(f"     - `{ftype}`: {count} —Ñ–∞–π–ª–æ–≤")
                
                # Pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏
                pipeline_info = a.get('pipeline_info', {})
                if pipeline_info:
                    lines.append(f"   - **Pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏:**")
                    for ftype, info in sorted(pipeline_info.items()):
                        count = info.get('count', 0)
                        route = info.get('route', 'unknown')
                        needs_ocr = info.get('needs_ocr', 0)
                        requires_conv = info.get('requires_conversion', 0)
                        
                        lines.append(f"     - `{ftype}` ({count} —Ñ–∞–π–ª–æ–≤):")
                        lines.append(f"       - Route: `{route}`")
                        if needs_ocr > 0:
                            lines.append(f"       - –¢—Ä–µ–±—É—é—Ç OCR: {needs_ocr}")
                        if requires_conv > 0:
                            lines.append(f"       - –¢—Ä–µ–±—É—é—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {requires_conv}")
                
                # –î–µ—Ç–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ (–ø–µ—Ä–≤—ã–µ 10)
                extracted_details = a.get('extracted_files_details', [])
                if extracted_details:
                    lines.append(f"   - **–î–µ—Ç–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –∞—Ä—Ö–∏–≤–µ:**")
                    for idx, file_detail in enumerate(extracted_details[:10], 1):
                        name = file_detail.get('original_name', 'unknown')
                        ftype = file_detail.get('detected_type', 'unknown')
                        needs_ocr = file_detail.get('needs_ocr', False)
                        size = file_detail.get('size', 0)
                        size_mb = size / (1024 * 1024)
                        ocr_status = "—Ç—Ä–µ–±—É–µ—Ç OCR" if needs_ocr else "—Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π"
                        lines.append(f"     - `{name}` | {ftype} | {size_mb:.2f} MB | {ocr_status}")
                    if len(extracted_details) > 10:
                        lines.append(f"     - ... –∏ –µ—â–µ {len(extracted_details) - 10} —Ñ–∞–π–ª–æ–≤")
            else:
                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ—à–∏–±–∫–µ
                errors = metrics.get('errors', [])
                archive_errors = [e for e in errors if a['archive_id'] in e.get('details', '')]
                if archive_errors:
                    error = archive_errors[0]
                    error_msg = error.get('error', 'Unknown error')[:200]
                    lines.append(f"   - **–û—à–∏–±–∫–∞:** {error_msg}")
            
            lines.append("")
    
    lines.append("---")
    lines.append("")
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    lines.append("## –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö")
    lines.append("")
    errors = metrics.get('errors', [])
    lines.append(f"**–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫:** {len(errors)}  ")
    
    if errors:
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —ç—Ç–∞–ø–∞–º
        by_stage = {}
        for e in errors:
            stage = e.get('stage', 'unknown')
            if stage not in by_stage:
                by_stage[stage] = []
            by_stage[stage].append(e)
        
        if by_stage:
            lines.append(f"**–û—à–∏–±–∫–∏ –ø–æ —ç—Ç–∞–ø–∞–º:**")
            for stage, errs in sorted(by_stage.items()):
                lines.append(f"- `{stage}`: {len(errs)} –æ—à–∏–±–æ–∫")
            lines.append("")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—à–∏–±–æ–∫
        lines.append("### –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—à–∏–±–æ–∫:")
        lines.append("")
        
        # RAR –∞—Ä—Ö–∏–≤—ã
        rar_errors = [e for e in errors if 'rar' in e.get('error', '').lower() or 'unsupported method' in e.get('error', '').lower()]
        if rar_errors:
            lines.append(f"1. **RAR –∞—Ä—Ö–∏–≤—ã —Å –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º –º–µ—Ç–æ–¥–æ–º —Å–∂–∞—Ç–∏—è ({len(rar_errors)} —Ñ–∞–π–ª–æ–≤):**")
            for e in rar_errors[:5]:
                filename = e['file'].split('/')[-1] if '/' in e['file'] else e['file']
                lines.append(f"   - `{filename}`")
            lines.append("")
            lines.append("   **–ü—Ä–∏—á–∏–Ω–∞:** 7z –Ω–µ –º–æ–∂–µ—Ç —Ä–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å —ç—Ç–∏ RAR –∞—Ä—Ö–∏–≤—ã –∏–∑-–∑–∞ \"Unsupported Method\". ")
            lines.append("   –í–µ—Ä–æ—è—Ç–Ω–æ, —ç—Ç–æ RAR5 –∞—Ä—Ö–∏–≤—ã –∏–ª–∏ –∞—Ä—Ö–∏–≤—ã —Å –º–µ—Ç–æ–¥–æ–º —Å–∂–∞—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è p7zip.")
            lines.append("")
            lines.append("   **–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `unrar` –¥–ª—è RAR –∞—Ä—Ö–∏–≤–æ–≤, —Å fallback –Ω–∞ 7z.")
            lines.append("")
        
        # HTML —Ñ–∞–π–ª—ã
        html_errors = [e for e in errors if 'html' in e.get('error', '').lower() or e.get('stage') == 'extraction' and 'html' in e.get('file', '').lower()]
        if html_errors:
            lines.append(f"2. **HTML —Ñ–∞–π–ª—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .doc ({len(html_errors)} —Ñ–∞–π–ª–æ–≤):**")
            for e in html_errors[:5]:
                filename = e['file'].split('/')[-1] if '/' in e['file'] else e['file']
                lines.append(f"   - `{filename}`")
            lines.append("")
            lines.append("   **–ü—Ä–∏—á–∏–Ω–∞:** –§–∞–π–ª—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–∞–∫ HTML, –Ω–æ —Å–∏—Å—Ç–µ–º–∞ –ø—ã—Ç–∞–ª–∞—Å—å –∏—Ö —Ä–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –∫–∞–∫ –∞—Ä—Ö–∏–≤—ã.")
            lines.append("")
            lines.append("   **–†–µ—à–µ–Ω–∏–µ:** HTML —Ñ–∞–π–ª—ã —Ç–µ–ø–µ—Ä—å –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–∞–∫ –∞—Ä—Ö–∏–≤—ã.")
            lines.append("")
    
    lines.append("---")
    lines.append("")
    
    # –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    lines.append("## –í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    lines.append("")
    
    # –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
    lines.append("### ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ:")
    input_files = metrics.get('input_files', [])
    pdf_count = sum(1 for f in input_files if f.get('detected_type') == 'pdf')
    docx_count = sum(1 for f in input_files if f.get('detected_type') == 'docx')
    doc_count = sum(1 for f in input_files if f.get('detected_type') == 'doc' and not f.get('is_fake_doc', False))
    
    if pdf_count > 0:
        lines.append(f"- {pdf_count} PDF —Ñ–∞–π–ª–æ–≤ (–≤—Å–µ —É—Å–ø–µ—à–Ω–æ)")
    if docx_count > 0:
        lines.append(f"- {docx_count} DOCX —Ñ–∞–π–ª–æ–≤ (–≤—Å–µ —É—Å–ø–µ—à–Ω–æ)")
    if doc_count > 0:
        lines.append(f"- {doc_count} DOC —Ñ–∞–π–ª–æ–≤ (—É—Å–ø–µ—à–Ω–æ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ DOCX)")
    
    lines.append("")
    
    # –ü—Ä–æ–±–ª–µ–º—ã
    if total_errors > 0:
        lines.append("### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã:")
        lines.append("")
        
        if failed > 0:
            lines.append(f"1. **–ê—Ä—Ö–∏–≤—ã:**")
            lines.append(f"   - {failed} –∞—Ä—Ö–∏–≤–æ–≤ –Ω–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–æ")
            if rar_errors:
                lines.append("   - RAR –∞—Ä—Ö–∏–≤—ã —Ç–µ–ø–µ—Ä—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ unrar —Å fallback –Ω–∞ 7z")
            if html_errors:
                lines.append("   - HTML —Ñ–∞–π–ª—ã –±–æ–ª—å—à–µ –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–∞–∫ –∞—Ä—Ö–∏–≤—ã")
            lines.append("")
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –≤ MongoDB
    lines.append("### üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ MongoDB:")
    lines.append(f"- –ö–æ–ª–ª–µ–∫—Ü–∏—è: `docling_metadata.processing_metrics`")
    lines.append(f"- Session ID: `{metrics['session_id']}`")
    lines.append("- –í—Å–µ –¥–µ—Ç–∞–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ API: `/metrics/processing` –∏ `/metrics/summary`")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
    lines.append("## –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò")
    lines.append("")
    lines.append("1. ‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω `unrar` –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ RAR –∞—Ä—Ö–∏–≤–æ–≤")
    lines.append("2. ‚úÖ –£–ª—É—á—à–µ–Ω–∞ –ª–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è HTML —Ñ–∞–π–ª–æ–≤ - –æ–Ω–∏ –Ω–µ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞—é—Ç—Å—è")
    lines.append("3. ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω fallback –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è RAR –∞—Ä—Ö–∏–≤–æ–≤ (unrar ‚Üí 7z)")
    lines.append("4. ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –ø–æ PDF (OCR vs text layer)")
    lines.append("5. ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–∞—Ö –≤ –∞—Ä—Ö–∏–≤–∞—Ö")
    lines.append("")
    
    return "\n".join(lines)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    session_id = sys.argv[1] if len(sys.argv) > 1 else None
    
    print("–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")
    metrics = get_metrics(session_id)
    
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞...")
    report = generate_report(metrics)
    
    output_file = "FINAL_REPORT.md"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_file}")


if __name__ == "__main__":
    main()

