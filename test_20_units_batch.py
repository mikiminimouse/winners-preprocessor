#!/usr/bin/env python3
"""
–ú–∞—Å—Å–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ 20 UNIT'–æ–≤ —á–µ—Ä–µ–∑ Qwen3-VL-8B –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
"""
import os
import sys
import json
import time
import base64
import re
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from PIL import Image
import io

try:
    from openai import OpenAI
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False
    print("‚ö†Ô∏è  openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")

try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False
    print("‚ö†Ô∏è  pdf2image –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdf2image")
    print("   –¢–∞–∫–∂–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è: sudo apt-get install poppler-utils")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_KEY = "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"
BASE_URL = "https://92ad3238-81c6-4396-a02a-fb9cef99bce3.modelrun.inference.cloud.ru/v1"
MODEL_NAME = "qwen3-vl-8b-instruct"

# –ü—É—Ç–∏
NORMALIZED_DIR = Path("/root/winners_preprocessor/normalized")
OUTPUT_DIR = Path("/root/winners_preprocessor/output_qwen3_batch")
TEST_UNITS_FILE = Path("/root/winners_preprocessor/test_ocr_units_fixed.json")

class Qwen3BatchOCRProcessor:
    def __init__(self):
        if not OPENAI_SDK_AVAILABLE:
            raise ImportError("openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        self.client = OpenAI(api_key=API_KEY, base_url=BASE_URL, timeout=120.0)
        self.model = MODEL_NAME
        OUTPUT_DIR.mkdir(exist_ok=True)
        self.metrics = {
            "total_units": 0,
            "successful_units": 0,
            "total_files": 0,
            "successful_files": 0,
            "total_processing_time": 0.0,
            "total_tokens_used": 0,
            "unit_results": []
        }

    def test_connection(self) -> bool:
        try:
            print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç"}],
                max_tokens=10,
                temperature=0.5
            )
            print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ! –û—Ç–≤–µ—Ç: {response.choices[0].message.content.strip()}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            return False

    def image_to_base64(self, image_path: Path) -> str:
        with open(image_path, "rb") as f:
            return base64.b64encode(f.read()).decode('utf-8')

    def pdf_to_first_page_image_base64(self, pdf_path: Path) -> str:
        if not PDF2IMAGE_AVAILABLE:
            raise ImportError("pdf2image not installed")
        
        try:
            # Convert only first page of PDF to PIL image
            pil_images = convert_from_path(str(pdf_path), dpi=200, first_page=1, last_page=1)
            if not pil_images:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
            
            img = pil_images[0]
            # Convert PIL image to bytes and then to base64
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG') # Save as PNG for better quality
            return base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {e}")
            raise

    def create_prompt(self, file_type: str) -> str:
        if file_type == "pdf":
            return """
            –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É PDF-–¥–æ–∫—É–º–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–º –∑–∞–∫—É–ø–∫–∏.
            –ò–∑–≤–ª–µ–∫–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ.
            –ï—Å–ª–∏ –ø–æ–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —É–∫–∞–∂–∏—Ç–µ null.
            
            –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:
            - "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "32515314610-01")
            - "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–õ–æ—Ç 1", "1", "null" –µ—Å–ª–∏ –Ω–µ—Ç)
            - "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è)
            - "–ò–ù–ù": string (–ò–ù–ù –ø–æ–±–µ–¥–∏—Ç–µ–ª—è, —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
            - "–ö–ü–ü": string (–ö–ü–ü –ø–æ–±–µ–¥–∏—Ç–µ–ª—è, —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
            - "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": string (—Å—É–º–º–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, "10 025.00")
            - "–≤–∞–ª—é—Ç–∞": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "RUB", "—Ä—É–±.")
            - "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": string (–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞ –∑–∞–∫—É–ø–∫–∏)
            - "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–∑–∞–∫–∞–∑—á–∏–∫": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞)
            - "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞)
            - "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": array of strings (—Å–ø–∏—Å–æ–∫ –§–ò–û —á–ª–µ–Ω–æ–≤ –∫–æ–º–∏—Å—Å–∏–∏)
            
            –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ, –∏–∑–≤–ª–µ–∫–∏—Ç–µ –ø–æ–ª–Ω—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã.
            –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ:
            {
                "text": "–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "tables": [
                    {
                        "type": "table",
                        "rows": [["Header1", "Header2"], ["Value1", "Value2"]],
                        "bbox": [x1, y1, x2, y2]
                    }
                ],
                "layout": {
                    "pages": [
                        {
                            "page_num": 1,
                            "blocks": [
                                {"type": "title", "text": "...", "bbox": [...]},
                                {"type": "paragraph", "text": "...", "bbox": [...]},
                                {"type": "table", "bbox": [...]}
                            ]
                        }
                    ]
                },
                "metadata": {
                    "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": "...",
                    "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": "...",
                    "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": "...",
                    "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": "...",
                    "–ò–ù–ù": "...",
                    "–ö–ü–ü": "...",
                    "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": "...",
                    "–≤–∞–ª—é—Ç–∞": "...",
                    "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": "...",
                    "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": "...",
                    "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": "...",
                    "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "...",
                    "–∑–∞–∫–∞–∑—á–∏–∫": "...",
                    "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "...",
                    "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": ["...", "..."]
                }
            }
            """
        elif file_type == "image":
            return """
            –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —è–≤–ª—è–µ—Ç—Å—è —Å–∫–∞–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –∑–∞–∫—É–ø–∫–∏).
            –ò–∑–≤–ª–µ–∫–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ.
            –ï—Å–ª–∏ –ø–æ–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —É–∫–∞–∂–∏—Ç–µ null.
            
            –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:
            - "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "32515314610-01")
            - "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–õ–æ—Ç 1", "1", "null" –µ—Å–ª–∏ –Ω–µ—Ç)
            - "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è)
            - "–ò–ù–ù": string (–ò–ù–ù –ø–æ–±–µ–¥–∏—Ç–µ–ª—è, —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
            - "–ö–ü–ü": string (–ö–ü–ü –ø–æ–±–µ–¥–∏—Ç–µ–ª—è, —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
            - "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": string (—Å—É–º–º–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, "10 025.00")
            - "–≤–∞–ª—é—Ç–∞": string (–Ω–∞–ø—Ä–∏–º–µ—Ä, "RUB", "—Ä—É–±.")
            - "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": string (–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞ –∑–∞–∫—É–ø–∫–∏)
            - "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": string (–≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY)
            - "–∑–∞–∫–∞–∑—á–∏–∫": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∑–∞–∫–∞–∑—á–∏–∫–∞)
            - "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": string (–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–∞)
            - "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": array of strings (—Å–ø–∏—Å–æ–∫ –§–ò–û —á–ª–µ–Ω–æ–≤ –∫–æ–º–∏—Å—Å–∏–∏)
            
            –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ, –∏–∑–≤–ª–µ–∫–∏—Ç–µ –ø–æ–ª–Ω—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã.
            –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ:
            {
                "text": "–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "tables": [
                    {
                        "type": "table",
                        "rows": [["Header1", "Header2"], ["Value1", "Value2"]],
                        "bbox": [x1, y1, x2, y2]
                    }
                ],
                "layout": {
                    "pages": [
                        {
                            "page_num": 1,
                            "blocks": [
                                {"type": "title", "text": "...", "bbox": [...]},
                                {"type": "paragraph", "text": "...", "bbox": [...]},
                                {"type": "table", "bbox": [...]}
                            ]
                        }
                    ]
                },
                "metadata": {
                    "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": "...",
                    "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": "...",
                    "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": "...",
                    "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": "...",
                    "–ò–ù–ù": "...",
                    "–ö–ü–ü": "...",
                    "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": "...",
                    "–≤–∞–ª—é—Ç–∞": "...",
                    "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": "...",
                    "–¥–∞—Ç–∞_–Ω–∞—á–∞–ª–∞_–ø–æ–¥–∞—á–∏": "...",
                    "–¥–∞—Ç–∞_–æ–∫–æ–Ω—á–∞–Ω–∏—è_–ø–æ–¥–∞—á–∏": "...",
                    "–¥–∞—Ç–∞_–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è": "...",
                    "–∑–∞–∫–∞–∑—á–∏–∫": "...",
                    "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": "...",
                    "—Å–æ—Å—Ç–∞–≤_–∫–æ–º–∏—Å—Å–∏–∏": ["...", "..."]
                }
            }
            """
        else:
            return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Ä–∞—Å–ø–æ–∑–Ω–∞–π—Ç–µ —Ä–∞–∑–º–µ—Ç–∫—É —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏—Ç–µ —Ç–µ–∫—Å—Ç, —Ç–∞–±–ª–∏—Ü—ã, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Docling JSON."

    def process_unit(self, unit_info: Dict[str, Any], unit_index: int, total_units: int) -> Optional[Dict[str, Any]]:
        unit_id = unit_info["unit_id"]
        unit_dir = Path(unit_info["unit_dir"])
        files_in_unit = unit_info["files"]
        
        unit_output_dir = OUTPUT_DIR / unit_id
        unit_output_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n{'='*80}")
        print(f"[{unit_index+1}/{total_units}] –û–±—Ä–∞–±–æ—Ç–∫–∞ UNIT: {unit_id}")
        print(f"{'='*80}")

        unit_start_time = time.time()
        unit_total_tokens = 0
        unit_successful_files = 0
        
        unit_result = {
            "unit_id": unit_id,
            "route": unit_info.get("route", "unknown"),
            "files_processed": [],
            "total_unit_time": 0.0,
            "total_unit_tokens": 0,
            "status": "failed",
            "error": None
        }

        for file_index, file_info in enumerate(files_in_unit):
            file_path = Path(file_info["path"])
            original_name = file_info["original_name"]
            detected_type = file_info["detected_type"]

            print(f"   üìÑ [{file_index+1}/{len(files_in_unit)}] –§–∞–π–ª: {original_name} ({detected_type})")

            file_start_time = time.time()
            file_tokens_used = 0
            
            try:
                messages_content = []
                prompt_text = self.create_prompt(detected_type)

                if detected_type == "image":
                    print(f"\n      üì∑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {original_name}")
                    print(f"         –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / (1024*1024):.1f} MB")
                    base64_image = self.image_to_base64(file_path)
                    print(f"         –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ base64...")
                    print(f"         Base64 –¥–ª–∏–Ω–∞: {len(base64_image)} —Å–∏–º–≤–æ–ª–æ–≤")
                    messages_content.append({"type": "text", "text": prompt_text})
                    messages_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}})
                elif detected_type == "pdf":
                    print(f"\n      üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF: {original_name}")
                    print(f"         –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / 1024:.1f} KB")
                    if not PDF2IMAGE_AVAILABLE:
                        raise ImportError("pdf2image not installed")
                    
                    print(f"         –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
                    base64_image = self.pdf_to_first_page_image_base64(file_path)
                    temp_image_path = unit_output_dir / f"{file_path.stem}.png"
                    with open(temp_image_path, "wb") as f:
                        f.write(base64.b64decode(base64_image))
                    print(f"         üì∑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {temp_image_path.name}")
                    print(f"         –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {temp_image_path.stat().st_size / (1024*1024):.1f} MB")
                    print(f"         Base64 –¥–ª–∏–Ω–∞: {len(base64_image)} —Å–∏–º–≤–æ–ª–æ–≤")
                    messages_content.append({"type": "text", "text": prompt_text})
                    messages_content.append({"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}})
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ (docx, html_text) –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º vision API
                    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ –æ–±—ã—á–Ω—ã–π LLM
                    print(f"      ‚ùå –¢–∏–ø —Ñ–∞–π–ª–∞ '{detected_type}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è Vision API. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                    continue

                print(f"      ‚û°Ô∏è  –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Qwen3-VL-8B...")
                response_start_time = time.time()
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": messages_content}],
                    max_tokens=5000,
                    temperature=0.0, # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                    response_format={"type": "json_object"} # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º JSON
                )
                response_time = time.time() - response_start_time
                file_tokens_used = response.usage.total_tokens if response.usage else 0
                self.metrics["total_tokens_used"] += file_tokens_used

                print(f"      ‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {response_time:.2f} —Å–µ–∫—É–Ω–¥")
                print(f"         –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response.choices[0].message.content)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞
                try:
                    llm_output = json.loads(response.choices[0].message.content)
                    print(f"      üì¶ –ü–∞—Ä—Å–∏–Ω–≥ JSON...")
                except json.JSONDecodeError as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
                    llm_output = {"error": f"JSON Decode Error: {e}", "raw_response": response.choices[0].message.content}

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                output_filename = f"{file_path.stem}_qwen3_result.json"
                output_path = unit_output_dir / output_filename
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(llm_output, f, indent=2, ensure_ascii=False)
                print(f"      üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")

                unit_successful_files += 1
                unit_result["files_processed"].append({
                    "file_name": original_name,
                    "detected_type": detected_type,
                    "status": "success",
                    "response_time": response_time,
                    "tokens_used": file_tokens_used,
                    "output_path": str(output_path),
                    "llm_output_preview": llm_output.get("text", "")[:200] + "..." if isinstance(llm_output.get("text"), str) else str(llm_output.get("text", ""))[:200] + "..."
                })

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                unit_result["files_processed"].append({
                    "file_name": original_name,
                    "detected_type": detected_type,
                    "status": "failed",
                    "error": str(e)
                })
        
        unit_result["total_unit_time"] = time.time() - unit_start_time
        unit_result["total_unit_tokens"] = unit_total_tokens
        if unit_successful_files > 0:
            unit_result["status"] = "success"
            self.metrics["successful_units"] += 1
            self.metrics["successful_files"] += unit_successful_files
        
        self.metrics["total_units"] += 1
        self.metrics["total_files"] += len(files_in_unit)
        self.metrics["total_processing_time"] += unit_result["total_unit_time"]
        self.metrics["unit_results"].append(unit_result)
        
        return unit_result

    def generate_summary_report(self):
        print(f"\n{'='*80}")
        print(f"–ì–ï–ù–ï–†–ê–¶–ò–Ø –ú–ï–¢–†–ò–ö")
        print(f"{'='*80}")

        total_units = self.metrics["total_units"]
        successful_units = self.metrics["successful_units"]
        total_files = self.metrics["total_files"]
        successful_files = self.metrics["successful_files"]
        total_time = self.metrics["total_processing_time"]
        total_tokens = self.metrics["total_tokens_used"]

        avg_time_per_file = total_time / successful_files if successful_files > 0 else 0
        avg_tokens_per_file = total_tokens / successful_files if successful_files > 0 else 0

        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ: {successful_units}/{total_units} ({successful_units/total_units*100:.1f}%)")
        print(f"‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —É—Å–ø–µ—à–Ω—ã–π —Ñ–∞–π–ª: {avg_time_per_file:.2f} —Å–µ–∫")
        print(f"üî¢ –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {total_tokens}")

        # –≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è
        estimated_100_units_time = avg_time_per_file * 100
        estimated_500_units_time = avg_time_per_file * 500

        print(f"üìà –û—Ü–µ–Ω–∫–∞ –¥–ª—è 100 UNIT'–æ–≤: {estimated_100_units_time / 60:.1f} –º–∏–Ω ({estimated_100_units_time / 3600:.2f} —á)")
        print(f"üìà –û—Ü–µ–Ω–∫–∞ –¥–ª—è 500 UNIT'–æ–≤: {estimated_500_units_time / 60:.1f} –º–∏–Ω ({estimated_500_units_time / 3600:.2f} —á)")

        metrics_summary = {
            "timestamp": datetime.now().isoformat(),
            "total_units_attempted": total_units,
            "successful_units": successful_units,
            "total_files_attempted": total_files,
            "successful_files": successful_files,
            "total_processing_time_seconds": total_time,
            "avg_time_per_successful_file_seconds": avg_time_per_file,
            "total_tokens_used": total_tokens,
            "avg_tokens_per_successful_file": avg_tokens_per_file,
            "extrapolation": {
                "estimated_100_units_minutes": estimated_100_units_time / 60,
                "estimated_100_units_hours": estimated_100_units_time / 3600,
                "estimated_500_units_minutes": estimated_500_units_time / 60,
                "estimated_500_units_hours": estimated_500_units_time / 3600
            }
        }
        metrics_output_path = OUTPUT_DIR / f"metrics_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(metrics_output_path, "w", encoding="utf-8") as f:
            json.dump(metrics_summary, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_output_path}")

        return metrics_summary

    def generate_comparison_report(self):
        print(f"\nüìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è...")
        report_path = OUTPUT_DIR / f"comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"# –û—Ç—á–µ—Ç –æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ UNIT'–æ–≤ —á–µ—Ä–µ–∑ Qwen3-VL-8B\n\n")
            f.write(f"**–î–∞—Ç–∞ –æ—Ç—á–µ—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n")
            f.write(f"- –í—Å–µ–≥–æ UNIT'–æ–≤ –≤ —Ç–µ—Å—Ç–µ: {self.metrics['total_units']}\n")
            f.write(f"- –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ UNIT'–æ–≤: {self.metrics['successful_units']}\n")
            f.write(f"- –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.metrics['total_processing_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
            f.write(f"- –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {self.metrics['total_tokens_used']}\n\n")

            f.write(f"## –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ UNIT'–∞–º\n\n")
            for unit_result in self.metrics["unit_results"]:
                f.write(f"### UNIT ID: `{unit_result['unit_id']}`\n")
                f.write(f"- –°—Ç–∞—Ç—É—Å: **{unit_result['status'].upper()}**\n")
                f.write(f"- –ú–∞—Ä—à—Ä—É—Ç: `{unit_result['route']}`\n")
                f.write(f"- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ UNIT'–∞: {unit_result['total_unit_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
                if unit_result['error']:
                    f.write(f"- –û—à–∏–±–∫–∞: `{unit_result['error']}`\n")
                f.write(f"\n")

                for file_proc_result in unit_result["files_processed"]:
                    f.write(f"#### –§–∞–π–ª: `{file_proc_result['file_name']}`\n")
                    f.write(f"- –¢–∏–ø: `{file_proc_result['detected_type']}`\n")
                    f.write(f"- –°—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏: **{file_proc_result['status'].upper()}**\n")
                    if file_proc_result['status'] == 'success':
                        f.write(f"- –í—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞ –∫ API: {file_proc_result['response_time']:.2f} —Å–µ–∫—É–Ω–¥\n")
                        f.write(f"- –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {file_proc_result['tokens_used']}\n")
                        f.write(f"- –ü—É—Ç—å –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É JSON: `{file_proc_result['output_path']}`\n")
                        
                        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–∫–∞–Ω–∞ –∏ MD –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        original_file_path = NORMALIZED_DIR / unit_result['unit_id'] / "files" / file_proc_result['file_name']
                        if original_file_path.exists():
                            f.write(f"\n##### –ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç:\n")
                            f.write(f"```\n")
                            f.write(f"–ü—É—Ç—å: {original_file_path}\n")
                            f.write(f"–¢–∏–ø: {file_proc_result['detected_type']}\n")
                            f.write(f"–†–∞–∑–º–µ—Ä: {original_file_path.stat().st_size / 1024:.1f} KB\n")
                            f.write(f"```\n")
                        else:
                            f.write(f"\n##### –ò—Å—Ö–æ–¥–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ `{original_file_path}`\n")

                        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LLM output –≤ Markdown
                        try:
                            with open(file_proc_result['output_path'], 'r', encoding='utf-8') as json_f:
                                llm_output = json.load(json_f)
                            
                            f.write(f"\n##### –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ Qwen3-VL-8B (Docling AST -> Markdown):\n")
                            f.write(f"```markdown\n")
                            f.write(f"# {llm_output.get('metadata', {}).get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')}\n\n")
                            f.write(f"**–î–∞—Ç–∞:** {llm_output.get('metadata', {}).get('date', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}\n\n")
                            f.write(f"## –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n")
                            f.write(f"{llm_output.get('text', '–¢–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω')}\n\n")
                            
                            if llm_output.get('tables'):
                                f.write(f"## –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã\n")
                                for table in llm_output['tables']:
                                    f.write(f"```\n")
                                    # –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã –≤ Markdown
                                    if 'rows' in table and table['rows']:
                                        header = table['rows'][0]
                                        body = table['rows'][1:]
                                        f.write("| " + " | ".join(header) + " |\n")
                                        f.write("|" + "---|".join(["---"] * len(header)) + "|\n")
                                        for row in body:
                                            f.write("| " + " | ".join(row) + " |\n")
                                    f.write("```\n\n")
                            
                            f.write(f"## –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n")
                            for key, value in llm_output.get('metadata', {}).items():
                                if key not in ['title', 'date']: # –£–∂–µ –≤—ã–≤–µ–¥–µ–Ω—ã
                                    f.write(f"- **{key.replace('_', ' ').capitalize()}:** {value}\n")
                            f.write(f"```\n")

                        except Exception as e:
                            f.write(f"\n##### –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏/—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ JSON: {e}\n")
                    else:
                        f.write(f"- –û—à–∏–±–∫–∞: `{file_proc_result['error']}`\n")
                    f.write(f"\n---\n\n")
        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

    def run(self):
        print(f"\n{'='*80}")
        print(f"–ú–ê–°–°–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê 20 UNIT'–û–í –ß–ï–†–ï–ó QWEN3-VL-8B")
        print(f"{'='*80}")

        if not self.test_connection():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å.")
            return

        try:
            with open(TEST_UNITS_FILE, "r", encoding="utf-8") as f:
                test_units_data = json.load(f)
                self.test_units = test_units_data["units"]
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º UNIT'–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {TEST_UNITS_FILE}")
            print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ collect_ocr_units.py –¥–ª—è —Å–±–æ—Ä–∞ UNIT'–æ–≤.")
            return

        print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ UNIT'–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(self.test_units)}")
        print(f"üéØ –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.test_units)} UNIT'–æ–≤")

        for i, unit_info in enumerate(self.test_units):
            self.process_unit(unit_info, i, len(self.test_units))
        
        self.generate_summary_report()
        self.generate_comparison_report()

        print(f"\n{'='*80}")
        print(f"‚úÖ –ú–ê–°–°–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
        print(f"{'='*80}")

if __name__ == "__main__":
    processor = Qwen3BatchOCRProcessor()
    processor.run()
