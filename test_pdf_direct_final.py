#!/usr/bin/env python3
"""
–§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢: –û–ë–†–ê–ë–û–¢–ö–ê PDF –ù–ê–ü–†–Ø–ú–£–Æ –ß–ï–†–ï–ó SMOLDOCLING
"""
import os
import sys
import json
import time
import base64
from pathlib import Path
from datetime import datetime

try:
    import openai
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False
    print("‚ö†Ô∏è  openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è SmolDocling
API_TOKEN = "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"
BASE_URL = "https://d63e30af-085a-49f0-9724-8162da967af2.modelrun.inference.cloud.ru/v1"
MODEL_NAME = "model-run-4qigw-disease"

# –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
TEST_FILE = "/root/winners_preprocessor/normalized/UNIT_43a02eedd2bbca86/files/! –ü—Ä–æ—Ç–æ–∫–æ–ª –≠–ú-17.pdf"
OUTPUT_DIR = Path("/root/winners_preprocessor/output_pdf_direct_final")

class DirectPDFProcessor:
    def __init__(self):
        if not OPENAI_SDK_AVAILABLE:
            raise ImportError("openai SDK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

        self.client = openai.OpenAI(
            api_key=API_TOKEN,
            base_url=BASE_URL,
            timeout=600.0  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        )
        self.model = MODEL_NAME
        OUTPUT_DIR.mkdir(exist_ok=True)

    def wait_for_server_ready(self, max_wait_time: int = 60) -> bool:
        """Wait for the inference server to be ready"""
        print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞ SmolDocling (–º–∞–∫—Å–∏–º—É–º {max_wait_time} —Å–µ–∫—É–Ω–¥)...")

        start_time = time.time()
        while time.time() - start_time < max_wait_time:
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": "Hello"}],
                    max_tokens=10,
                    temperature=0.5
                )
                if response.choices[0].message.content:
                    print("‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
                    return True
            except Exception as e:
                print(f"    –°–µ—Ä–≤–µ—Ä –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤... ({str(e)[:50]}...)")
                time.sleep(5)

        print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ —Å—Ç–∞–ª –¥–æ—Å—Ç—É–ø–µ–Ω –≤ —Ç–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏")
        return False

    def encode_pdf_to_base64(self, pdf_path: Path) -> str:
        """–ö–æ–¥–∏—Ä–æ–≤–∞—Ç—å PDF —Ñ–∞–π–ª –≤ base64"""
        print(f"üìÑ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ PDF –≤ base64: {pdf_path.name}")

        with open(pdf_path, "rb") as pdf_file:
            pdf_data = pdf_file.read()

        base64_pdf = base64.b64encode(pdf_data).decode('utf-8')
        print(f"‚úÖ PDF –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω: {len(base64_pdf)} —Å–∏–º–≤–æ–ª–æ–≤ base64 ({len(pdf_data)} –±–∞–π—Ç)")

        return base64_pdf

    def test_different_pdf_formats(self, base64_pdf: str, pdf_path: Path):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–ø—Ä–∞–≤–∫–∏ PDF"""
        print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ê–ó–ù–´–• –§–û–†–ú–ê–¢–û–í PDF...")

        test_cases = [
            {
                'name': 'PDF as text message',
                'messages': [{"role": "user", "content": f"Process this PDF document: data:application/pdf;base64,{base64_pdf}"}],
                'description': 'PDF –∫–∞–∫ —Ç–µ–∫—Å—Ç –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏'
            },
            {
                'name': 'PDF as document attachment',
                'messages': [
                    {"role": "user", "content": [
                        {"type": "text", "text": "Extract all text and tables from this PDF document."},
                        {"type": "file", "file": {"file_data": f"data:application/pdf;base64,{base64_pdf}", "filename": pdf_path.name}}
                    ]}
                ],
                'description': 'PDF –∫–∞–∫ file attachment'
            },
            {
                'name': 'PDF instruction only',
                'messages': [{"role": "user", "content": f"Here is a PDF document encoded in base64. Please extract all text, tables, and structured information: {base64_pdf[:1000]}..."}],
                'description': '–¢–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å base64'
            }
        ]

        results = {}

        for i, test_case in enumerate(test_cases):
            print(f"\n--- –¢–ï–°–¢ {i+1}: {test_case['name']} ---")
            print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {test_case['description']}")

            try:
                start_time = time.time()
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=test_case['messages'],
                    max_tokens=8000,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    temperature=0.0
                )

                processing_time = time.time() - start_time
                tokens_used = response.usage.total_tokens if response.usage else 0
                content = response.choices[0].message.content

                print("‚úÖ –£–°–ü–ï–•:")
                print(f"   –í—Ä–µ–º—è: {processing_time:.2f} —Å–µ–∫")
                print(f"   –¢–æ–∫–µ–Ω–æ–≤: {tokens_used}")
                print(f"   –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")

                results[test_case['name']] = {
                    'success': True,
                    'content': content,
                    'tokens_used': tokens_used,
                    'processing_time': processing_time,
                    'method': test_case['description']
                }

                # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–µ–≤—å—é
                preview = content[:300] + "..." if len(content) > 300 else content
                print(f"   –ü—Ä–µ–≤—å—é: {preview}")

            except Exception as e:
                error_msg = str(e)
                print(f"‚ùå –û–®–ò–ë–ö–ê: {error_msg[:100]}...")

                results[test_case['name']] = {
                    'success': False,
                    'error': error_msg,
                    'method': test_case['description']
                }

        return results

    def create_comprehensive_markdown(self, results: dict, pdf_path: Path) -> str:
        """–°–æ–∑–¥–∞—Ç—å –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π Markdown –æ—Ç—á–µ—Ç"""
        print("üìù –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ Markdown –æ—Ç—á–µ—Ç–∞...")

        markdown_parts = []

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        header = f"# –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó PDF –î–û–ö–£–ú–ï–ù–¢–ê: {pdf_path.name}\n\n"
        header += f"**–ú–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏:** –ü—Ä—è–º–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ PDF –≤ SmolDocling\n"
        header += f"**–î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        header += f"**–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞:** {pdf_path.stat().st_size} –±–∞–π—Ç\n\n"
        markdown_parts.append(header)

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–º—É –º–µ—Ç–æ–¥—É
        total_successful = 0
        total_text_length = 0

        for method_name, result in results.items():
            markdown_parts.append(f"## –ú–ï–¢–û–î: {method_name}\n")

            if result['success']:
                total_successful += 1
                content = result['content']
                total_text_length += len(content)

                markdown_parts.append(f"**‚úÖ –£–°–ü–ï–•**\n")
                markdown_parts.append(f"- **–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {result['processing_time']:.2f} —Å–µ–∫\n")
                markdown_parts.append(f"- **–¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ:** {result['tokens_used']}\n")
                markdown_parts.append(f"- **–î–ª–∏–Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:** {len(content)} —Å–∏–º–≤–æ–ª–æ–≤\n\n")

                # –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç
                content_analysis = self.analyze_extracted_content(content)
                markdown_parts.append(f"### –ê–ù–ê–õ–ò–ó –°–û–î–ï–†–ñ–ò–ú–û–ì–û\n")
                markdown_parts.append(f"- **–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞:** {content_analysis['document_type']}\n")
                markdown_parts.append(f"- **–°–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª:** {'–î–∞' if content_analysis['has_protocol'] else '–ù–µ—Ç'}\n")
                markdown_parts.append(f"- **–°–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–∫—É–ø–∫–∏:** {'–î–∞' if content_analysis['has_procurement'] else '–ù–µ—Ç'}\n")
                markdown_parts.append(f"- **–°–æ–¥–µ—Ä–∂–∏—Ç —Ç–∞–±–ª–∏—Ü—ã:** {'–î–∞' if content_analysis['has_tables'] else '–ù–µ—Ç'}\n")
                markdown_parts.append(f"- **–£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:** {content_analysis['confidence']:.2f}\n\n")

                # –ü–æ–∫–∞–∑–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                markdown_parts.append(f"### –ò–ó–í–õ–ï–ß–ï–ù–ù–´–ô –¢–ï–ö–°–¢\n\n")
                markdown_parts.append(f"```\n{content}\n```\n\n")

            else:
                markdown_parts.append(f"**‚ùå –ù–ï–£–î–ê–ß–ê**\n")
                markdown_parts.append(f"- **–û—à–∏–±–∫–∞:** {result['error'][:200]}...\n\n")

        # –°–≤–æ–¥–∫–∞
        markdown_parts.append(f"## –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n\n")
        markdown_parts.append(f"- **–í—Å–µ–≥–æ –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ:** {len(results)}\n")
        markdown_parts.append(f"- **–£—Å–ø–µ—à–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤:** {total_successful}\n")
        markdown_parts.append(f"- **–û–±—â–∏–π –æ–±—ä–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:** {total_text_length} —Å–∏–º–≤–æ–ª–æ–≤\n")

        if total_successful > 0:
            avg_length = total_text_length / total_successful
            markdown_parts.append(f"- **–°—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –º–µ—Ç–æ–¥:** {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤\n")

        # –í—ã–≤–æ–¥—ã
        markdown_parts.append(f"## –í–´–í–û–î–´\n\n")

        if total_successful == 0:
            markdown_parts.append("‚ùå **SmolDocling –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å PDF —Ñ–∞–π–ª –Ω–∞–ø—Ä—è–º—É—é.**\n\n")
            markdown_parts.append("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n")
            markdown_parts.append("- SmolDocling –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n")
            markdown_parts.append("- PDF —Ñ–∞–π–ª—ã —Ç—Ä–µ–±—É—é—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n")
            markdown_parts.append("- –°–µ—Ä–≤–µ—Ä –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ —Ç–∏–ø –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n")
        else:
            markdown_parts.append(f"‚úÖ **SmolDocling —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å PDF –≤ {total_successful} –∏–∑ {len(results)} –º–µ—Ç–æ–¥–æ–≤.**\n\n")
            markdown_parts.append("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n")
            markdown_parts.append("- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n")
            markdown_parts.append("- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ thumbnail\n")
            markdown_parts.append("- –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤\n")

        return '\n'.join(markdown_parts)

    def analyze_extracted_content(self, content: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"""
        text_lower = content.lower()

        analysis = {
            'has_protocol': '–ø—Ä–æ—Ç–æ–∫–æ–ª' in text_lower,
            'has_procurement': any(word in text_lower for word in ['–∑–∞–∫—É–ø–∫', '—Ç–µ–Ω–¥–µ—Ä', '–∫–æ–Ω–∫—É—Ä—Å', '–ø—Ä–µ–¥–ª–æ–∂–µ–Ω']),
            'has_commission': any(word in text_lower for word in ['–∫–æ–º–∏—Å—Å', '–∑–∞—Å–µ–¥–∞–Ω–∏', '—á–ª–µ–Ω']),
            'has_tables': '|' in content or '\t' in content or 'table' in text_lower,
            'has_numbers': any(char.isdigit() for char in content),
            'document_type': 'protocol' if '–ø—Ä–æ—Ç–æ–∫–æ–ª' in text_lower else 'procurement' if '–∑–∞–∫—É–ø–∫' in text_lower else 'unknown',
            'confidence': 0.0
        }

        # –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence = 0.0
        if analysis['has_protocol']: confidence += 0.4
        if analysis['has_procurement']: confidence += 0.3
        if analysis['has_commission']: confidence += 0.2
        if analysis['has_numbers']: confidence += 0.1
        if len(content) > 100: confidence += 0.2

        analysis['confidence'] = min(confidence, 1.0)

        return analysis

    def run(self):
        print("üöÄ –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢: –û–ë–†–ê–ë–û–¢–ö–ê PDF –ù–ê–ü–†–Ø–ú–£–Æ")
        print(f"–§–∞–π–ª: {TEST_FILE}")

        if not self.wait_for_server_ready():
            print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
            return

        print("‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")

        pdf_path = Path(TEST_FILE)
        if not pdf_path.exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
            return

        # –ö–æ–¥–∏—Ä–æ–≤–∞—Ç—å PDF
        base64_pdf = self.encode_pdf_to_base64(pdf_path)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        results = self.test_different_pdf_formats(base64_pdf, pdf_path)

        # –°–æ–∑–¥–∞—Ç—å –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –æ—Ç—á–µ—Ç
        comprehensive_markdown = self.create_comprehensive_markdown(results, pdf_path)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        markdown_file = OUTPUT_DIR / f"{pdf_path.stem}_direct_pdf_analysis.md"
        with open(markdown_file, "w", encoding="utf-8") as f:
            f.write(comprehensive_markdown)

        json_file = OUTPUT_DIR / f"{pdf_path.stem}_direct_pdf_results.json"
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump({
                'pdf_file': str(pdf_path),
                'processing_timestamp': datetime.now().isoformat(),
                'base64_length': len(base64_pdf),
                'original_size_bytes': pdf_path.stat().st_size,
                'test_results': results
            }, f, indent=2, ensure_ascii=False)

        print("\nüéâ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_DIR}")
        print(f"üìÑ Markdown –æ—Ç—á–µ—Ç: {markdown_file}")
        print(f"üìä JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {json_file}")

        # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–µ–≤—å—é –æ—Ç—á–µ—Ç–∞
        preview_length = 1500
        preview = comprehensive_markdown[:preview_length] + ("..." if len(comprehensive_markdown) > preview_length else "")
        print("\nüìã –ü–†–ï–í–¨–Æ –û–¢–ß–ï–¢–ê:")
        print("-" * 70)
        print(preview)
        print("-" * 70)

if __name__ == "__main__":
    processor = DirectPDFProcessor()
    processor.run()
