#!/usr/bin/env python3
"""
–ß–ï–°–¢–ù–´–ô —Ç–µ—Å—Ç –Ω–∞ –†–ï–ê–õ–¨–ù–´–• 10 PDF —Ñ–∞–π–ª–∞—Ö
–ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: pdfplumber + Granite –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
"""
import os
import sys
import json
import time
import openai
import pdfplumber
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# Granite –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
GRANITE_API = "https://8cb66180-db3a-4963-8068-51f87e716259.modelrun.inference.cloud.ru/v1"
GRANITE_TOKEN = "ZDRhOWQ3OTAtZjU4MC00MzA2LThhNTgtMDU1NGFlMjE4OWRl.85a830f9340966e0ad1fd1642884c7c8"

granite = openai.OpenAI(api_key=GRANITE_TOKEN, base_url=GRANITE_API)


def find_test_pdfs(limit=10) -> List[Path]:
    """–ù–∞–π—Ç–∏ —Ç–µ—Å—Ç–æ–≤—ã–µ PDF"""
    base = Path("/root/winners_preprocessor/pilot_winers223/data/pending/direct/pdf")
    pdfs = []
    for unit_dir in sorted(base.iterdir())[:50]:  # –ü–µ—Ä–≤—ã–µ 50 unit'–æ–≤
        if unit_dir.is_dir() and unit_dir.name.startswith("UNIT_"):
            files_dir = unit_dir / "files"
            if files_dir.exists():
                pdfs.extend(list(files_dir.glob("*.pdf")))
                if len(pdfs) >= limit:
                    break
    return pdfs[:limit]


def extract_text_from_pdf(pdf_path: Path) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ pdfplumber + OCR –¥–ª—è —Å–∫–∞–Ω–æ–≤"""
    all_text = []
    all_tables = []
    is_scan = False
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            pages = len(pdf.pages)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É - —Å–∫–∞–Ω –∏–ª–∏ —Ç–µ–∫—Å—Ç
            first_text = pdf.pages[0].extract_text() if pages > 0 else ""
            is_scan = not first_text or len(first_text.strip()) < 50
            
            if is_scan:
                # –≠–¢–û –°–ö–ê–ù - –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR
                print(f"üîç OCR...", end=" ", flush=True)
                from pdf2image import convert_from_path
                import pytesseract
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è OCR
                max_pages_for_ocr = min(5, pages)
                images = convert_from_path(
                    str(pdf_path),
                    dpi=200,
                    first_page=1,
                    last_page=max_pages_for_ocr
                )
                
                for page_num, image in enumerate(images, 1):
                    ocr_text = pytesseract.image_to_string(image, lang='rus+eng')
                    if ocr_text.strip():
                        all_text.append(f"\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} (OCR) ---\n{ocr_text}")
                
                # –¢–∞–±–ª–∏—Ü—ã –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ pdfplumber
                for page_num, page in enumerate(pdf.pages[:max_pages_for_ocr], 1):
                    tables = page.extract_tables()
                    if tables:
                        for table in tables:
                            all_tables.append({"page": page_num, "data": table})
            else:
                # –ï—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π
                for page_num, page in enumerate(pdf.pages[:10], 1):
                    text = page.extract_text()
                    if text:
                        all_text.append(f"\n--- –°–¢–†–ê–ù–ò–¶–ê {page_num} ---\n{text}")
                    
                    tables = page.extract_tables()
                    if tables:
                        for table in tables:
                            all_tables.append({"page": page_num, "data": table})
            
            combined_text = "\n".join(all_text)
            
            return {
                "success": True,
                "text": combined_text,
                "tables": all_tables,
                "pages": pages,
                "is_scan": is_scan
            }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "text": "",
            "tables": []
        }


def extract_metadata(text: str, debug=False) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ regex –ø–∞—Ä—Å–∏–Ω–≥"""
    import re
    
    if len(text.strip()) < 100:
        return get_empty_metadata()
    
    metadata = {}
    
    # –ù–æ–º–µ—Ä –ø—Ä–æ—Ü–µ–¥—É—Ä—ã (–ï–ò–°)
    match = re.search(r'(?:‚Ññ\s*|–Ω–æ–º–µ—Ä\s+)(\d{11})', text, re.IGNORECASE)
    metadata['–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã'] = match.group(1) if match else None
    
    # –ù–æ–º–µ—Ä –ª–æ—Ç–∞
    match = re.search(r'[–õ–ª]–æ—Ç[–∞]?\s*‚Ññ?\s*(\d+)', text)
    metadata['–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞'] = match.group(1) if match else None
    
    # –î–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
    match = re.search(r'¬´?(\d{1,2})¬ª?\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})', text, re.IGNORECASE)
    if match:
        months = {'—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03', '–∞–ø—Ä–µ–ª—è': '04', '–º–∞—è': '05', '–∏—é–Ω—è': '06',
                  '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08', '—Å–µ–Ω—Ç—è–±—Ä—è': '09', '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'}
        day, month, year = match.groups()
        metadata['–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞'] = f"{day.zfill(2)}.{months[month.lower()]}.{year}"
    else:
        match = re.search(r'(\d{2}\.\d{2}\.\d{4})', text)
        metadata['–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞'] = match.group(1) if match else None
    
    # –ü–æ–±–µ–¥–∏—Ç–µ–ª—å
    match = re.search(r'[–ü–ø]–æ–±–µ–¥–∏—Ç–µ–ª[—å—è]:\s*(.+?)(?:,\s*–ò–ù–ù|\n|$)', text)
    if not match:
        match = re.search(r'[–ó–∑]–∞–∫–ª—é—á–∏—Ç—å.*?—Å\s+(–û–û–û|–ê–û|–ó–ê–û|–û–ê–û|–ò–ü|–§–ì–£–ü)\s+[¬´"]?([^¬ª"\n,]+)', text)
        if match:
            metadata['–ø–æ–±–µ–¥–∏—Ç–µ–ª—å'] = f"{match.group(1)} {match.group(2).strip()}"
        else:
            metadata['–ø–æ–±–µ–¥–∏—Ç–µ–ª—å'] = None
    else:
        metadata['–ø–æ–±–µ–¥–∏—Ç–µ–ª—å'] = match.group(1).strip()
    
    # –ò–ù–ù
    match = re.search(r'–ò–ù–ù[:\s]*(\d{10,12})', text)
    metadata['–ò–ù–ù'] = match.group(1) if match else None
    
    # –ö–ü–ü
    match = re.search(r'–ö–ü–ü[:\s]*(\d{9})', text)
    metadata['–ö–ü–ü'] = match.group(1) if match else None
    
    # –¶–µ–Ω–∞
    match = re.search(r'[–¶—Ü]–µ–Ω–∞.*?(\d[\d\s]+\d)\s*(?:—Ä—É–±–ª|RUB)', text)
    if match:
        metadata['—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è'] = match.group(1).replace(' ', '')
    else:
        metadata['—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è'] = None
    
    # –í–∞–ª—é—Ç–∞
    metadata['–≤–∞–ª—é—Ç–∞'] = 'RUB' if '—Ä—É–±–ª' in text.lower() else None
    
    # –ü—Ä–µ–¥–º–µ—Ç –∑–∞–∫—É–ø–∫–∏
    match = re.search(r'[–ü–ø]—Ä–µ–¥–º–µ—Ç\s+(?:–∑–∞–∫—É–ø–∫–∏|–¥–æ–≥–æ–≤–æ—Ä–∞)[:\s]*(.+?)(?:\n|–¶–µ–Ω–∞|–°—Ä–æ–∫|$)', text, re.DOTALL)
    if match:
        subject = match.group(1).strip()[:200]
        metadata['–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏'] = re.sub(r'\s+', ' ', subject)
    else:
        metadata['–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏'] = None
    
    # –ó–∞–∫–∞–∑—á–∏–∫
    match = re.search(r'[–ó–∑]–∞–∫–∞–∑—á–∏–∫[:\s]*(.+?)(?:\n|–ò–ù–ù|–ê–¥—Ä–µ—Å|$)', text)
    if match:
        metadata['–∑–∞–∫–∞–∑—á–∏–∫'] = match.group(1).strip()[:150]
    else:
        metadata['–∑–∞–∫–∞–∑—á–∏–∫'] = None
    
    # –û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä
    match = re.search(r'[–û–æ]—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä[:\s]*(.+?)(?:\n|–ò–ù–ù|–ê–¥—Ä–µ—Å|$)', text)
    if match:
        metadata['–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä'] = match.group(1).strip()[:150]
    else:
        metadata['–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä'] = None
    
    if debug:
        import json
        print(f"   [DEBUG] –ò–∑–≤–ª–µ—á–µ–Ω–æ regex: {json.dumps(metadata, ensure_ascii=False, indent=2)}")
    
    return metadata


def parse_json(text: str) -> Dict:
    import re
    text = text.strip()
    
    # –£–¥–∞–ª—è–µ–º markdown
    if '```' in text:
        parts = text.split('```')
        for part in parts:
            if '{' in part:
                text = part
                break
    if text.startswith('json'):
        text = text[4:]
    
    # –ë–µ—Ä–µ–º –ü–ï–†–í–´–ô JSON (Granite –ø–æ–≤—Ç–æ—Ä—è–µ—Ç)
    lines = text.split('\n')
    json_lines = []
    brace_count = 0
    started = False
    
    for line in lines:
        if '{' in line and not started:
            started = True
        if started:
            json_lines.append(line)
            brace_count += line.count('{')
            brace_count -= line.count('}')
            if brace_count == 0 and '}' in line:
                break
    
    if json_lines:
        try:
            json_str = '\n'.join(json_lines)
            return json.loads(json_str)
        except Exception as e:
            pass
    
    # –§–æ–ª–±—ç–∫ - regex
    match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except:
            pass
    
    return get_empty_metadata()


def get_empty_metadata() -> Dict:
    return {
        "–Ω–æ–º–µ—Ä_–ø—Ä–æ—Ü–µ–¥—É—Ä—ã": None,
        "–Ω–æ–º–µ—Ä_–ª–æ—Ç–∞": None,
        "–¥–∞—Ç–∞_–ø—Ä–æ—Ç–æ–∫–æ–ª–∞": None,
        "–ø–æ–±–µ–¥–∏—Ç–µ–ª—å": None,
        "–ò–ù–ù": None,
        "–ö–ü–ü": None,
        "—Ü–µ–Ω–∞_–ø–æ–±–µ–¥–∏—Ç–µ–ª—è": None,
        "–≤–∞–ª—é—Ç–∞": None,
        "–ø—Ä–µ–¥–º–µ—Ç_–∑–∞–∫—É–ø–∫–∏": None,
        "–∑–∞–∫–∞–∑—á–∏–∫": None,
        "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä": None
    }


def create_markdown(filename: str, text: str, tables: List, metadata: Dict) -> str:
    """–°–æ–∑–¥–∞–Ω–∏–µ Markdown"""
    md = f"# {filename}\n\n"
    md += f"**–î–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    md += "---\n\n"
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    md += "## üìä –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n\n"
    for key, value in metadata.items():
        if key not in ['–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç', '—Ç–∞–±–ª–∏—Ü—ã']:
            display_key = key.replace('_', ' ').title()
            md += f"- **{display_key}:** {value or '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}\n"
    md += "\n"
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    md += "## üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è\n\n"
    md += f"- **–¢–µ–∫—Å—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–æ:** {len(text):,} —Å–∏–º–≤–æ–ª–æ–≤\n"
    md += f"- **–¢–∞–±–ª–∏—Ü –Ω–∞–π–¥–µ–Ω–æ:** {len(tables)}\n\n"
    
    # –¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 3000 —Å–∏–º–≤–æ–ª–æ–≤)
    md += "## üìÑ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n\n"
    if len(text) > 3000:
        md += text[:3000] + "\n\n_(—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏, –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ JSON)_\n\n"
    else:
        md += text + "\n\n"
    
    # –¢–∞–±–ª–∏—Ü—ã (–ø–µ—Ä–≤—ã–µ 2)
    if tables:
        md += f"## üìä –¢–∞–±–ª–∏—Ü—ã\n\n"
        for i, tbl in enumerate(tables[:2], 1):
            md += f"### –¢–∞–±–ª–∏—Ü–∞ {i} (–°—Ç—Ä–∞–Ω–∏—Ü–∞ {tbl['page']})\n\n"
            data = tbl['data']
            if data and len(data) > 0:
                for r_idx, row in enumerate(data[:10]):
                    md += "| " + " | ".join(str(c or "").replace("|", "\\|") for c in row) + " |\n"
                    if r_idx == 0:
                        md += "|" + "|".join([" --- "] * len(row)) + "|\n"
            md += "\n"
        if len(tables) > 2:
            md += f"_(–ü–æ–∫–∞–∑–∞–Ω—ã 2 –∏–∑ {len(tables)} —Ç–∞–±–ª–∏—Ü)_\n\n"
    
    return md


def process_one_pdf(pdf_path: Path, output_dir: Path, index: int, total: int) -> Dict:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ PDF"""
    print(f"\n[{index}/{total}] {pdf_path.name}")
    start = time.time()
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    print(f"   üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞...", end=" ", flush=True)
    result = extract_text_from_pdf(pdf_path)
    
    if not result["success"]:
        print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
        return {"success": False, "file": pdf_path.name}
    
    text = result["text"]
    tables = result["tables"]
    print(f"‚úÖ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤, {len(tables)} —Ç–∞–±–ª–∏—Ü")
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    print(f"   üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...", end=" ", flush=True)
    metadata = extract_metadata(text)
    filled = sum(1 for v in metadata.values() if v)
    print(f"‚úÖ {filled} –ø–æ–ª–µ–π")
    
    # Markdown
    md = create_markdown(pdf_path.name, text, tables, metadata)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    base = pdf_path.stem
    md_file = output_dir / f"{base}.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md)
    
    meta_file = output_dir / f"{base}_metadata.json"
    metadata["–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç"] = text
    metadata["—Ç–∞–±–ª–∏—Ü—ã"] = tables
    with open(meta_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    elapsed = time.time() - start
    print(f"   ‚è±Ô∏è  {elapsed:.1f}—Å")
    
    return {
        "success": True,
        "file": pdf_path.name,
        "text_length": len(text),
        "tables_count": len(tables),
        "metadata_fields": filled,
        "time": elapsed
    }


def main():
    print("="*70)
    print("–†–ï–ê–õ–¨–ù–´–ô –¢–ï–°–¢: 10 PDF —Ñ–∞–π–ª–æ–≤")
    print("="*70)
    print()
    
    # –ü–æ–∏—Å–∫ PDF
    print("–ü–æ–∏—Å–∫ PDF —Ñ–∞–π–ª–æ–≤...")
    pdfs = find_test_pdfs(10)
    
    if not pdfs:
        print("‚ùå PDF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        sys.exit(1)
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(pdfs)} —Ñ–∞–π–ª–æ–≤\n")
    
    # Output
    output = Path("output_REAL_TEST_10")
    output.mkdir(exist_ok=True)
    print(f"üìÅ Output: {output}\n")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    print("–û–±—Ä–∞–±–æ—Ç–∫–∞:")
    results = []
    
    for i, pdf in enumerate(pdfs, 1):
        result = process_one_pdf(pdf, output, i, len(pdfs))
        results.append(result)
    
    # –ò—Ç–æ–≥–∏
    print("\n" + "="*70)
    print("–ò–¢–û–ì–ò")
    print("="*70)
    
    success = [r for r in results if r.get("success")]
    failed = [r for r in results if not r.get("success")]
    
    print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ: {len(success)}/{len(pdfs)}")
    
    if failed:
        print(f"‚ùå –û—à–∏–±–∫–∏: {len(failed)}")
        for f in failed[:5]:
            print(f"   - {f['file']}")
    
    if success:
        total_text = sum(r["text_length"] for r in success)
        total_tables = sum(r["tables_count"] for r in success)
        total_meta = sum(r["metadata_fields"] for r in success)
        total_time = sum(r["time"] for r in success)
        
        print(f"\nüìä –ò–∑–≤–ª–µ—á–µ–Ω–æ:")
        print(f"   –¢–µ–∫—Å—Ç–∞: {total_text:,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –¢–∞–±–ª–∏—Ü: {total_tables}")
        print(f"   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {total_meta} –ø–æ–ª–µ–π (—Å—Ä–µ–¥–Ω–µ–µ: {total_meta/len(success):.1f})")
        print(f"   –í—Ä–µ–º—è: {total_time:.1f}—Å (—Å—Ä–µ–¥–Ω–µ–µ: {total_time/len(success):.1f}—Å)")
        
        # –ü—Ä–∏–º–µ—Ä—ã
        print(f"\nüìÑ –ü—Ä–∏–º–µ—Ä—ã –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:")
        for r in success[:3]:
            meta_file = output / f"{Path(r['file']).stem}_metadata.json"
            if meta_file.exists():
                with open(meta_file, 'r') as f:
                    meta = json.load(f)
                print(f"\n   {r['file']}:")
                for k, v in list(meta.items())[:5]:
                    if k not in ['–ø–æ–ª–Ω—ã–π_—Ç–µ–∫—Å—Ç', '—Ç–∞–±–ª–∏—Ü—ã'] and v:
                        print(f"      {k}: {str(v)[:50]}")
    
    print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output}")
    print("="*70)


if __name__ == "__main__":
    main()

