#!/usr/bin/env python3
"""
ÐŸÐ¾Ð»Ð½Ð¾Ñ†ÐµÐ½Ð½Ñ‹Ð¹ Ñ‚ÐµÑÑ‚ Cycle 1 Ñ batch processing, checkpoints, Ð¸ Ð¿Ð¾Ð»Ð½Ð¾Ð¹ validation.
Ð‘ÐÐ“ #4: error handling Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½ Ð·Ð´ÐµÑÑŒ Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² classify_unit
"""
import sys
import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from collections import defaultdict
from datetime import datetime
import argparse

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿ÑƒÑ‚ÑŒ Ðº docprep
sys.path.insert(0, str(Path(__file__).parent))

from docprep.engine.classifier import Classifier
from docprep.utils.disk_utils import check_disk_space, estimate_unit_size


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('test_cycle1_full_run.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class Cycle1TestRunner:
    """Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ñ€Ð°Ð½Ð½ÐµÑ€ Ð´Ð»Ñ Cycle 1 Ñ batch processing Ð¸ checkpoints."""

    def __init__(
        self,
        input_dir: Path,
        protocol_date: str,
        batch_size: int = 50,
        checkpoint_file: str = "checkpoint.json",
        dry_run: bool = False,
        copy_mode: bool = True,
        limit: Optional[int] = None,
    ):
        self.input_dir = Path(input_dir)
        self.protocol_date = protocol_date
        self.batch_size = batch_size
        self.checkpoint_file = Path(checkpoint_file)
        self.dry_run = dry_run
        self.copy_mode = copy_mode
        self.limit = limit
        self.classifier = Classifier()

        self.stats = {
            'start_time': None,
            'end_time': None,
            'total': 0,
            'processed': 0,
            'successful': 0,
            'failed': 0,
            'by_category': defaultdict(int),
            'by_destination': defaultdict(list),
            'by_file_type': defaultdict(int),
            'errors': [],
            'batches': [],
            'disk_space_before': None,
            'disk_space_after': None,
        }

        self.processed_units = set()

    def load_checkpoint(self) -> bool:
        """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ checkpoint Ð´Ð»Ñ resume."""
        if not self.checkpoint_file.exists():
            logger.info("No checkpoint found, starting fresh")
            return False

        try:
            with open(self.checkpoint_file, 'r') as f:
                checkpoint = json.load(f)

            self.processed_units = set(checkpoint.get('processed_units', []))
            # Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ stats Ð¸Ð· checkpoint
            saved_stats = checkpoint.get('stats', {})
            for key in ['processed', 'successful', 'failed']:
                if key in saved_stats:
                    self.stats[key] = saved_stats[key]
            if 'by_category' in saved_stats:
                self.stats['by_category'] = defaultdict(int, saved_stats['by_category'])
            if 'by_destination' in saved_stats:
                self.stats['by_destination'] = defaultdict(list, saved_stats['by_destination'])
            if 'by_file_type' in saved_stats:
                self.stats['by_file_type'] = defaultdict(int, saved_stats['by_file_type'])
            if 'errors' in saved_stats:
                self.stats['errors'] = saved_stats['errors']
            if 'batches' in saved_stats:
                self.stats['batches'] = saved_stats['batches']

            logger.info(f"Loaded checkpoint: {len(self.processed_units)} units already processed")
            return True

        except Exception as e:
            logger.error(f"Failed to load checkpoint: {e}")
            return False

    def save_checkpoint(self):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ checkpoint Ð¿Ð¾ÑÐ»Ðµ batch."""
        try:
            checkpoint = {
                'timestamp': datetime.now().isoformat(),
                'processed_units': list(self.processed_units),
                'stats': {
                    'processed': self.stats['processed'],
                    'successful': self.stats['successful'],
                    'failed': self.stats['failed'],
                    'by_category': dict(self.stats['by_category']),
                    'by_destination': {k: v for k, v in self.stats['by_destination'].items()},
                    'by_file_type': dict(self.stats['by_file_type']),
                    'errors': self.stats['errors'],
                    'batches': self.stats['batches'],
                },
            }

            with open(self.checkpoint_file, 'w') as f:
                json.dump(checkpoint, f, indent=2)

            logger.info(f"Checkpoint saved: {len(self.processed_units)} units processed")

        except Exception as e:
            logger.error(f"Failed to save checkpoint: {e}")

    def check_prerequisites(self) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ prerequisites Ð¿ÐµÑ€ÐµÐ´ Ð·Ð°Ð¿ÑƒÑÐºÐ¾Ð¼."""
        logger.info("Checking prerequisites...")

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Input
        if not self.input_dir.exists():
            logger.error(f"Input directory not found: {self.input_dir}")
            return False

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¼ÐµÑÑ‚Ð°
        required_gb = 50.0 if not self.dry_run else 1.0
        has_space, msg = check_disk_space(self.input_dir.parent, required_gb)
        if not has_space:
            logger.error(msg)
            return False

        self.stats['disk_space_before'] = msg

        logger.info("Prerequisites check PASSED")
        return True

    def get_units_to_process(self) -> List[Path]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº UNITs Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸."""
        all_units = sorted([u for u in self.input_dir.iterdir() if u.is_dir() and u.name.startswith('UNIT_')])

        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ limit ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½
        if self.limit:
            all_units = all_units[:self.limit]

        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ ÑƒÐ¶Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ
        units_to_process = [u for u in all_units if u.name not in self.processed_units]

        logger.info(f"Total units: {len(all_units)}, To process: {len(units_to_process)}")

        return units_to_process

    def process_batch(self, units_batch: List[Path]) -> Dict[str, Any]:
        """ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð¾Ð´Ð¸Ð½ batch UNITs."""
        batch_start = time.time()
        batch_stats = {
            'size': len(units_batch),
            'successful': 0,
            'failed': 0,
            'errors': [],
        }

        logger.info(f"Processing batch of {len(units_batch)} units...")

        for i, unit_path in enumerate(units_batch, 1):
            unit_name = unit_path.name

            # Ð‘ÐÐ“ #4: ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð½Ð° ÑƒÑ€Ð¾Ð²Ð½Ðµ Ð²Ñ‹Ð·Ð¾Ð²Ð° classify_unit
            try:
                # ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 10 UNITs
                if i % 10 == 0:
                    logger.info(f"  [{i}/{len(units_batch)}] Processing...")

                # ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐµÐ¼ UNIT
                result = self.classifier.classify_unit(
                    unit_path=unit_path,
                    cycle=1,
                    protocol_date=self.protocol_date,
                    dry_run=self.dry_run,
                    copy_mode=self.copy_mode,
                )

                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð½Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ¸
                if 'error' in result:
                    # Ð•ÑÑ‚ÑŒ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð² Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ðµ
                    batch_stats['failed'] += 1
                    error_info = {
                        'unit': unit_name,
                        'error': result.get('error', 'Unknown error'),
                        'error_type': result.get('error_type', 'Unknown'),
                    }
                    batch_stats['errors'].append(error_info)
                    self.stats['errors'].append(error_info)
                    logger.error(f"Failed to process {unit_name}: {result['error']}")
                    # ÐÐµ Ð¿Ð¾Ð¼ÐµÑ‡Ð°ÐµÐ¼ ÐºÐ°Ðº processed, Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ð¾Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ½Ð¾Ð²Ð°
                    continue

                # Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½
                batch_stats['successful'] += 1
                self.stats['successful'] += 1

                # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
                category = result['unit_category']
                self.stats['by_category'][category] += 1

                # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ destination (Ð°Ð½Ð°Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾ test_cycle1_200_with_verification.py)
                target = result['target_directory']
                if 'Merge' in target and 'Direct' in target:
                    destination = "Merge/Direct"
                elif 'Processing' in target:
                    if 'Convert' in target:
                        destination = "Processing_1/Convert"
                    elif 'Extract' in target:
                        destination = "Processing_1/Extract"
                    elif 'Normalize' in target:
                        destination = "Processing_1/Normalize"
                    else:
                        destination = "Processing_1/Other"
                elif 'Exception' in target:
                    if 'Empty' in target:
                        destination = "Exceptions_1/Empty"
                    elif 'Special' in target:
                        destination = "Exceptions_1/Special"
                    elif 'Ambiguous' in target:
                        destination = "Exceptions_1/Ambiguous"
                    else:
                        destination = "Exceptions_1/Other"
                else:
                    destination = "Unknown"

                self.stats['by_destination'][destination].append(unit_name)

                # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ñ‚Ð¸Ð¿Ñ‹ Ñ„Ð°Ð¹Ð»Ð¾Ð²
                for fc in result.get('file_classifications', []):
                    file_type = fc.get('classification', {}).get('detected_type', 'unknown')
                    self.stats['by_file_type'][file_type] += 1

                # ÐžÑ‚Ð¼ÐµÑ‡Ð°ÐµÐ¼ ÐºÐ°Ðº Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ð¹
                self.processed_units.add(unit_name)

            except FileNotFoundError as e:
                # Ð¤Ð°Ð¹Ð» Ð¸Ð»Ð¸ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹
                batch_stats['failed'] += 1
                error_info = {
                    'unit': unit_name,
                    'error': f"File not found: {e}",
                    'error_type': 'FileNotFoundError',
                }
                batch_stats['errors'].append(error_info)
                self.stats['errors'].append(error_info)
                logger.error(f"FileNotFoundError for {unit_name}: {e}")

            except PermissionError as e:
                # ÐÐµÑ‚ Ð¿Ñ€Ð°Ð² Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð°
                batch_stats['failed'] += 1
                error_info = {
                    'unit': unit_name,
                    'error': f"Permission denied: {e}",
                    'error_type': 'PermissionError',
                }
                batch_stats['errors'].append(error_info)
                self.stats['errors'].append(error_info)
                logger.error(f"PermissionError for {unit_name}: {e}")

            except (OSError, IOError) as e:
                # IO Ð¾ÑˆÐ¸Ð±ÐºÐ¸
                batch_stats['failed'] += 1
                error_info = {
                    'unit': unit_name,
                    'error': f"IO error: {e}",
                    'error_type': type(e).__name__,
                }
                batch_stats['errors'].append(error_info)
                self.stats['errors'].append(error_info)
                logger.error(f"IO error for {unit_name}: {e}")

            except Exception as e:
                # ÐÐµÐ¿Ñ€ÐµÐ´Ð²Ð¸Ð´ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸
                batch_stats['failed'] += 1
                error_info = {
                    'unit': unit_name,
                    'error': str(e),
                    'error_type': type(e).__name__,
                }
                batch_stats['errors'].append(error_info)
                self.stats['errors'].append(error_info)
                logger.exception(f"Unexpected error processing {unit_name}")

        batch_end = time.time()
        batch_stats['duration_sec'] = batch_end - batch_start
        batch_stats['units_per_sec'] = len(units_batch) / batch_stats['duration_sec'] if batch_stats['duration_sec'] > 0 else 0

        self.stats['batches'].append(batch_stats)
        self.stats['processed'] += len(units_batch)
        self.stats['failed'] += batch_stats['failed']

        logger.info(f"Batch completed: {batch_stats['successful']}/{len(units_batch)} successful, "
                   f"{batch_stats['failed']} failed, {batch_stats['duration_sec']:.1f}s")

        return batch_stats

    def validate_batch(self, units_batch: List[Path]) -> bool:
        """Ð’Ð°Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ batch."""
        logger.info("Validating batch...")

        validation_errors = []

        for unit_path in units_batch:
            unit_name = unit_path.name

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° 1: UNIT Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½
            if unit_name not in self.processed_units:
                validation_errors.append(f"{unit_name}: not marked as processed")
                continue

            # Ð’ dry_run Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð´Ð°Ð»ÑŒÐ½ÐµÐ¹ÑˆÐ°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð½Ðµ Ð¸Ð¼ÐµÐµÑ‚ ÑÐ¼Ñ‹ÑÐ»Ð°
            if self.dry_run:
                continue

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° 2: ÐÐ°Ð¹Ñ‚Ð¸ UNIT Ð² Ñ†ÐµÐ»ÐµÐ²Ñ‹Ñ… Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑÑ…
            # TODO: Ð±Ð¾Ð»ÐµÐµ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° (Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ñ‡Ñ‚Ð¾ Ñ„Ð°Ð¹Ð»Ñ‹ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑÐºÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹)

        if validation_errors:
            logger.warning(f"Validation found {len(validation_errors)} issues")
            for error in validation_errors[:10]:  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 10
                logger.warning(f"  - {error}")
            return False

        logger.info("Validation PASSED")
        return True

    def run(self):
        """Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð·Ð°Ð¿ÑƒÑÐºÐ°."""
        logger.info("=" * 80)
        logger.info("CYCLE 1 FULL RUN TEST")
        logger.info("=" * 80)
        logger.info(f"Input directory: {self.input_dir}")
        logger.info(f"Protocol date: {self.protocol_date}")
        logger.info(f"Batch size: {self.batch_size}")
        logger.info(f"Dry run: {self.dry_run}")
        logger.info(f"Copy mode: {self.copy_mode}")
        if self.limit:
            logger.info(f"Limit: {self.limit} UNITs")
        logger.info("=" * 80)

        self.stats['start_time'] = datetime.now().isoformat()

        # Prerequisites check
        if not self.check_prerequisites():
            logger.error("Prerequisites check failed, aborting")
            return False

        # Load checkpoint if exists
        self.load_checkpoint()

        # Get units to process
        units_to_process = self.get_units_to_process()
        self.stats['total'] = len(units_to_process) + len(self.processed_units)

        if not units_to_process:
            logger.info("No units to process")
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ Ð½ÐµÑ‡ÐµÐ³Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ
            self.stats['end_time'] = datetime.now().isoformat()
            self.generate_report()
            return True

        # Process in batches
        batch_num = 0
        for i in range(0, len(units_to_process), self.batch_size):
            batch_num += 1
            units_batch = units_to_process[i:i + self.batch_size]

            logger.info(f"\n{'=' * 80}")
            logger.info(f"BATCH {batch_num}: Processing units {i+1} to {i+len(units_batch)}")
            logger.info(f"{'=' * 80}")

            # Process batch
            batch_stats = self.process_batch(units_batch)

            # Validate batch
            self.validate_batch(units_batch)

            # Save checkpoint
            self.save_checkpoint()

            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¼ÐµÑÑ‚Ð° Ð¿Ð¾ÑÐ»Ðµ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ batch
            if not self.dry_run:
                has_space, msg = check_disk_space(self.input_dir.parent, 10.0)
                if not has_space:
                    logger.warning(msg)
                    logger.warning("Low disk space, stopping")
                    break

        self.stats['end_time'] = datetime.now().isoformat()

        # Final disk space check
        _, msg = check_disk_space(self.input_dir.parent, 0.0)
        self.stats['disk_space_after'] = msg

        # Generate report
        self.generate_report()

        logger.info("\n" + "=" * 80)
        logger.info("CYCLE 1 FULL RUN COMPLETED")
        logger.info("=" * 80)

        return True

    def generate_report(self):
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡ÐµÑ‚."""
        report_file = Path("/tmp/cycle1_full_run_FINAL_REPORT.md")

        logger.info(f"Generating report: {report_file}")

        # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
        start = datetime.fromisoformat(self.stats['start_time'])
        end = datetime.fromisoformat(self.stats['end_time'])
        duration = end - start

        # Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð°Ð¼
        merge_count = len(self.stats['by_destination'].get('Merge/Direct', []))
        processing_convert = len(self.stats['by_destination'].get('Processing_1/Convert', []))
        processing_extract = len(self.stats['by_destination'].get('Processing_1/Extract', []))
        processing_normalize = len(self.stats['by_destination'].get('Processing_1/Normalize', []))
        exceptions_empty = len(self.stats['by_destination'].get('Exceptions_1/Empty', []))
        exceptions_special = len(self.stats['by_destination'].get('Exceptions_1/Special', []))
        exceptions_ambiguous = len(self.stats['by_destination'].get('Exceptions_1/Ambiguous', []))

        processing_total = processing_convert + processing_extract + processing_normalize
        exceptions_total = exceptions_empty + exceptions_special + exceptions_ambiguous

        with open(report_file, 'w') as f:
            f.write("# CYCLE 1 FULL RUN FINAL REPORT\n\n")
            f.write(f"**Generated:** {datetime.now().isoformat()}\n\n")
            f.write("---\n\n")

            f.write("## Executive Summary\n\n")
            f.write(f"- **Date:** {self.stats['start_time']}\n")
            f.write(f"- **Duration:** {duration}\n")
            f.write(f"- **UNITs Total:** {self.stats['total']}\n")
            if self.stats['total'] > 0:
                f.write(f"- **UNITs Processed:** {self.stats['processed']}/{self.stats['total']} "
                       f"({self.stats['processed']/self.stats['total']*100:.1f}%)\n")
            else:
                f.write(f"- **UNITs Processed:** {self.stats['processed']}/{self.stats['total']}\n")
            f.write(f"- **Successful:** {self.stats['successful']}\n")
            f.write(f"- **Failed:** {self.stats['failed']}\n")
            f.write(f"- **Mode:** {'DRY RUN' if self.dry_run else 'REAL RUN'}\n")
            f.write(f"- **Copy Mode:** {self.copy_mode}\n")
            f.write(f"- **Status:** {'âœ… SUCCESS' if self.stats['failed'] == 0 else 'âš ï¸ PARTIAL SUCCESS'}\n\n")

            f.write("---\n\n")

            f.write("## Ð˜ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ðµ Ð±Ð°Ð³Ð¸ Ð² ÑÑ‚Ð¾Ð¼ Ð·Ð°Ð¿ÑƒÑÐºÐµ\n\n")
            f.write("1. âœ… **Ð‘ÐÐ“ #1**: Empty UNITs path - Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ `Exceptions_1/Empty`\n")
            f.write("2. âœ… **Ð‘ÐÐ“ #2**: Race condition Ð¿Ñ€Ð¸ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ð¸ target_dir - Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð¿ÑƒÑÑ‚Ð¾Ñ‚Ñƒ\n")
            f.write("3. âœ… **Ð‘ÐÐ“ #3**: Error handling Ð² move/copy Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑÑ… - Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ try-except Ð¾Ð±ÐµÑ€Ñ‚ÐºÐ¸\n")
            f.write("4. âœ… **Ð‘ÐÐ“ #4**: Error handling Ð² classify_unit - Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð¾ Ð² test script\n")
            f.write("5. âœ… **Ð‘ÐÐ“ #5**: fsync() Ð² save_manifest - Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð° Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð½Ð° Ð´Ð¸ÑÐº\n")
            f.write("6. âœ… **Ð‘ÐÐ“ #6**: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¼ÐµÑÑ‚Ð° - Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ disk_utils.py\n\n")

            f.write("---\n\n")

            if self.stats['processed'] > 0:
                f.write("## Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑÐ¼\n\n")
                for category, count in sorted(self.stats['by_category'].items(), key=lambda x: -x[1]):
                    percentage = (count / self.stats['processed']) * 100
                    f.write(f"- **{category}**: {count} ({percentage:.1f}%)\n")
                f.write("\n")

                f.write("## Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ð¼Ð°Ñ€ÑˆÑ€ÑƒÑ‚Ð°Ð¼\n\n")
                f.write(f"### ðŸŸ¢ Merge_0/Direct (Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹ Ðº Docling)\n")
                f.write(f"- **Ð’ÑÐµÐ³Ð¾:** {merge_count} UNITs ({(merge_count/self.stats['processed']*100):.1f}%)\n\n")

                f.write(f"### ðŸ”µ Processing_1 (Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸)\n")
                f.write(f"- **Ð’ÑÐµÐ³Ð¾:** {processing_total} UNITs ({(processing_total/self.stats['processed']*100):.1f}%)\n")
                f.write(f"  - Convert: {processing_convert}\n")
                f.write(f"  - Extract: {processing_extract}\n")
                f.write(f"  - Normalize: {processing_normalize}\n\n")

                f.write(f"### ðŸ”´ Exceptions_1 (Ð¸ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ)\n")
                f.write(f"- **Ð’ÑÐµÐ³Ð¾:** {exceptions_total} UNITs ({(exceptions_total/self.stats['processed']*100):.1f}%)\n")
                f.write(f"  - Empty: {exceptions_empty}\n")
                f.write(f"  - Special: {exceptions_special}\n")
                f.write(f"  - Ambiguous: {exceptions_ambiguous}\n\n")

                if self.stats['by_file_type']:
                    f.write("## Ð¢Ð¸Ð¿Ñ‹ Ñ„Ð°Ð¹Ð»Ð¾Ð²\n\n")
                    top_types = sorted(self.stats['by_file_type'].items(), key=lambda x: -x[1])[:15]
                    for file_type, count in top_types:
                        f.write(f"- **{file_type}**: {count}\n")
                    f.write("\n")

            f.write("## ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ\n\n")
            f.write(f"- **ÐžÐ±Ñ‰ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ:** {duration}\n")
            if self.stats['batches']:
                avg_batch_time = sum(b['duration_sec'] for b in self.stats['batches']) / len(self.stats['batches'])
                avg_units_per_sec = sum(b['units_per_sec'] for b in self.stats['batches']) / len(self.stats['batches'])
                f.write(f"- **Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ batch:** {avg_batch_time:.1f} sec\n")
                f.write(f"- **Ð¡Ñ€ÐµÐ´Ð½ÑÑ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ:** {avg_units_per_sec:.2f} units/sec\n")
            if self.stats.get('disk_space_before'):
                f.write(f"- **Disk space before:** {self.stats['disk_space_before']}\n")
            if self.stats.get('disk_space_after'):
                f.write(f"- **Disk space after:** {self.stats['disk_space_after']}\n")
            f.write("\n")

            if self.stats['errors']:
                f.write("## ÐžÑˆÐ¸Ð±ÐºÐ¸\n\n")
                f.write(f"**Ð’ÑÐµÐ³Ð¾ Ð¾ÑˆÐ¸Ð±Ð¾Ðº:** {len(self.stats['errors'])}\n\n")
                for i, error in enumerate(self.stats['errors'][:20], 1):  # ÐŸÐµÑ€Ð²Ñ‹Ðµ 20
                    f.write(f"{i}. **{error['unit']}**: {error.get('error_type', 'Unknown')} - {error.get('error', 'No details')}\n")
                if len(self.stats['errors']) > 20:
                    f.write(f"\n... Ð¸ ÐµÑ‰Ðµ {len(self.stats['errors']) - 20} Ð¾ÑˆÐ¸Ð±Ð¾Ðº\n")
                f.write("\n")

            f.write("---\n\n")
            f.write("**End of Report**\n")

        logger.info(f"Report saved: {report_file}")


def main():
    parser = argparse.ArgumentParser(description='Cycle 1 Full Run Test')
    parser.add_argument('--input-dir', type=str,
                       default='/root/winners_preprocessor/final_preprocessing/Data/2025-03-18/Input',
                       help='Input directory with UNITs')
    parser.add_argument('--protocol-date', type=str, default='2025-03-18',
                       help='Protocol date')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='Batch size for processing')
    parser.add_argument('--limit', type=int, default=None,
                       help='Limit number of UNITs to process (for testing)')
    parser.add_argument('--dry-run', action='store_true',
                       help='Dry run mode (no actual moving)')
    parser.add_argument('--copy-mode', action='store_true', default=False,
                       help='Use copy mode instead of move')
    parser.add_argument('--resume', action='store_true',
                       help='Resume from checkpoint')
    parser.add_argument('--checkpoint-file', type=str, default='checkpoint.json',
                       help='Checkpoint file path')

    args = parser.parse_args()

    runner = Cycle1TestRunner(
        input_dir=Path(args.input_dir),
        protocol_date=args.protocol_date,
        batch_size=args.batch_size,
        checkpoint_file=args.checkpoint_file,
        dry_run=args.dry_run,
        copy_mode=args.copy_mode,
        limit=args.limit,
    )

    success = runner.run()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
