#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç Cycle 1 –Ω–∞ 200 UNITs —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
"""
import sys
import os
from pathlib import Path
import json
from collections import defaultdict
import shutil

from docprep.engine.classifier import Classifier

def check_unit_files(unit_path: Path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ñ–∞–π–ª—ã –≤ UNIT"""
    files = []
    if not unit_path.exists():
        return files

    for item in unit_path.iterdir():
        if item.is_file():
            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ñ–∞–π–ª—ã
            if item.name not in ['manifest.json', 'audit.log.jsonl', 'unit.meta.json',
                                 'docprep.contract.json', 'raw_url_map.json']:
                files.append({
                    'name': item.name,
                    'size': item.stat().st_size,
                    'path': str(item)
                })
    return files

def test_cycle1_200units_with_verification():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞ 200 UNITs —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ñ–∞–π–ª–æ–≤"""

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    classifier = Classifier()

    # –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ UNITs
    with open("/tmp/test_200_units_2025_03_18.txt", "r") as f:
        units = [line.strip() for line in f if line.strip()]

    input_dir = Path("/root/winners_preprocessor/final_preprocessing/Data/2025-03-18/Input")
    protocol_date = "2025-03-18"

    results = []
    stats = {
        'total': len(units),
        'processed': 0,
        'errors': 0,
        'files_moved': 0,
        'files_remained': 0,
        'by_category': defaultdict(int),
        'by_destination': defaultdict(list),
        'by_file_type': defaultdict(int),
        'exceptions_details': defaultdict(list),  # –î–µ—Ç–∞–ª–∏ Exceptions
    }

    print("=" * 80)
    print("–î–ï–¢–ê–õ–¨–ù–´–ô –¢–ï–°–¢ CYCLE 1: 200 UNITs —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è")
    print("=" * 80)
    print(f"\n–î–∞—Ç–∞: {protocol_date}")
    print(f"–í—Å–µ–≥–æ UNITs: {len(units)}")
    print(f"–†–µ–∂–∏–º: dry_run=True (–ë–ï–ó —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è)")
    print("\n–û–±—Ä–∞–±–æ—Ç–∫–∞...")

    for i, unit_name in enumerate(units, 1):
        unit_path = input_dir / unit_name

        if not unit_path.exists():
            stats['errors'] += 1
            continue

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 20 UNITs
        if i % 20 == 0:
            print(f"  [{i}/{len(units)}] –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ...")

        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã –î–û
        files_before = check_unit_files(unit_path)

        try:
            # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º UNIT (dry_run=True - –ë–ï–ó –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è)
            result = classifier.classify_unit(
                unit_path=unit_path,
                cycle=1,
                protocol_date=protocol_date,
                dry_run=True,  # –í–ê–ñ–ù–û: —Å–Ω–∞—á–∞–ª–∞ —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç
            )

            stats['processed'] += 1

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            category = result['unit_category']
            stats['by_category'][category] += 1

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º destination
            target = result['target_directory']
            if 'Merge' in target and 'Direct' in target:
                destination = "Merge/Direct"
                destination_category = "merge_direct"
            elif 'Processing' in target:
                if 'Convert' in target:
                    destination = "Processing_1/Convert"
                    destination_category = "processing_convert"
                elif 'Extract' in target:
                    destination = "Processing_1/Extract"
                    destination_category = "processing_extract"
                elif 'Normalize' in target:
                    destination = "Processing_1/Normalize"
                    destination_category = "processing_normalize"
                else:
                    destination = "Processing_1/Other"
                    destination_category = "processing_other"
            elif 'Exception' in target:
                if 'Empty' in target:
                    destination = "Exceptions_1/Empty"
                    destination_category = "exceptions_empty"
                elif 'Special' in target:
                    destination = "Exceptions_1/Special"
                    destination_category = "exceptions_special"
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ Special
                    stats['exceptions_details']['special'].append({
                        'unit': unit_name,
                        'files': [f['name'] for f in files_before]
                    })
                elif 'Ambiguous' in target:
                    destination = "Exceptions_1/Ambiguous"
                    destination_category = "exceptions_ambiguous"
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ Ambiguous
                    stats['exceptions_details']['ambiguous'].append({
                        'unit': unit_name,
                        'files': [f['name'] for f in files_before]
                    })
                else:
                    destination = "Exceptions_1/Other"
                    destination_category = "exceptions_other"
            else:
                destination = "Unknown"
                destination_category = "unknown"

            stats['by_destination'][destination].append(unit_name)

            # –°–æ–±–∏—Ä–∞–µ–º —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤
            for fc in result.get('file_classifications', []):
                file_type = fc['classification'].get('detected_type', 'unknown')
                stats['by_file_type'][file_type] += 1

            # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã –ü–û–°–õ–ï (–≤ dry_run –æ–Ω–∏ –Ω–µ –¥–æ–ª–∂–Ω—ã –∏–∑–º–µ–Ω–∏—Ç—å—Å—è)
            files_after = check_unit_files(unit_path)

            # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤
            files_count = len(files_before)
            if files_count > 0:
                # –í dry_run —Ñ–∞–π–ª—ã –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞—Ç—å—Å—è
                if len(files_after) == len(files_before):
                    stats['files_remained'] += files_count
                else:
                    # –≠—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ –≤ dry_run —Ä–µ–∂–∏–º–µ
                    print(f"\n  ‚ö†Ô∏è  {unit_name}: —Ñ–∞–π–ª–æ–≤ –î–û={len(files_before)}, –ü–û–°–õ–ï={len(files_after)}")

            results.append({
                'unit_name': unit_name,
                'category': category,
                'is_mixed': result['is_mixed'],
                'files_count_before': len(files_before),
                'files_count_after': len(files_after),
                'files_before': [f['name'] for f in files_before],
                'destination': destination,
                'destination_category': destination_category,
                'target_directory': str(result['target_directory']),
            })

        except Exception as e:
            stats['errors'] += 1
            print(f"\n  ‚ùå {unit_name}: {e}")
            results.append({
                'unit_name': unit_name,
                'error': str(e),
            })

    # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print("\n" + "=" * 80)
    print("–î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)

    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}/{stats['total']}")
    if stats['errors'] > 0:
        print(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}")

    print("\n" + "‚îÄ" * 80)
    print("üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú")
    print("‚îÄ" * 80)
    for category, count in sorted(stats['by_category'].items(), key=lambda x: -x[1]):
        percentage = (count / stats['processed']) * 100 if stats['processed'] > 0 else 0
        bar = "‚ñà" * int(percentage / 2)
        print(f"  {category:12} : {count:4} ({percentage:5.1f}%) {bar}")

    print("\n" + "‚îÄ" * 80)
    print("üìç –î–ï–¢–ê–õ–¨–ù–û–ï –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ú–ê–†–®–†–£–¢–ê–ú")
    print("‚îÄ" * 80)

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º
    merge_count = len(stats['by_destination'].get('Merge/Direct', []))
    processing_convert = len(stats['by_destination'].get('Processing_1/Convert', []))
    processing_extract = len(stats['by_destination'].get('Processing_1/Extract', []))
    processing_normalize = len(stats['by_destination'].get('Processing_1/Normalize', []))
    exceptions_empty = len(stats['by_destination'].get('Exceptions_1/Empty', []))
    exceptions_special = len(stats['by_destination'].get('Exceptions_1/Special', []))
    exceptions_ambiguous = len(stats['by_destination'].get('Exceptions_1/Ambiguous', []))

    processing_total = processing_convert + processing_extract + processing_normalize
    exceptions_total = exceptions_empty + exceptions_special + exceptions_ambiguous

    print(f"\n  üü¢ Merge_0/Direct (–≥–æ—Ç–æ–≤—ã –∫ Docling):")
    print(f"     {merge_count} UNITs ({(merge_count/stats['processed']*100):.1f}%)")

    print(f"\n  üîµ Processing_1 (—Ç—Ä–µ–±—É—é—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏):")
    print(f"     –í—Å–µ–≥–æ: {processing_total} UNITs ({(processing_total/stats['processed']*100):.1f}%)")
    print(f"       ‚Ä¢ Convert:   {processing_convert}")
    print(f"       ‚Ä¢ Extract:   {processing_extract}")
    print(f"       ‚Ä¢ Normalize: {processing_normalize}")

    print(f"\n  üî¥ Exceptions_1 (–∏—Å–∫–ª—é—á–µ–Ω–∏—è):")
    print(f"     –í—Å–µ–≥–æ: {exceptions_total} UNITs ({(exceptions_total/stats['processed']*100):.1f}%)")
    print(f"       ‚Ä¢ Empty:     {exceptions_empty}")
    print(f"       ‚Ä¢ Special:   {exceptions_special}")
    print(f"       ‚Ä¢ Ambiguous: {exceptions_ambiguous}")

    # –î–µ—Ç–∞–ª–∏ Exceptions (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if exceptions_special > 0:
        print(f"\n  üîé –î–µ—Ç–∞–ª–∏ Special Exceptions:")
        for item in stats['exceptions_details']['special'][:5]:  # –ø–µ—Ä–≤—ã–µ 5
            print(f"     - {item['unit']}: {', '.join(item['files'][:3])}")

    if exceptions_ambiguous > 0:
        print(f"\n  üîé –î–µ—Ç–∞–ª–∏ Ambiguous Exceptions:")
        for item in stats['exceptions_details']['ambiguous'][:5]:  # –ø–µ—Ä–≤—ã–µ 5
            print(f"     - {item['unit']}: {', '.join(item['files'][:3])}")

    print("\n" + "‚îÄ" * 80)
    print("üìÑ –¢–û–ü-15 –¢–ò–ü–û–í –§–ê–ô–õ–û–í")
    print("‚îÄ" * 80)
    top_types = sorted(stats['by_file_type'].items(), key=lambda x: -x[1])[:15]
    for file_type, count in top_types:
        print(f"  {file_type:25} : {count:4}")

    print("\n" + "‚îÄ" * 80)
    print("üì¶ –ü–†–û–í–ï–†–ö–ê –ü–ï–†–ï–ú–ï–©–ï–ù–ò–Ø –§–ê–ô–õ–û–í")
    print("‚îÄ" * 80)
    print(f"  –§–∞–π–ª–æ–≤ –æ—Å—Ç–∞–ª–æ—Å—å –≤ Input (dry_run): {stats['files_remained']}")
    print(f"  ‚ö†Ô∏è  dry_run=True —Ä–µ–∂–∏–º - —Ñ–∞–π–ª—ã –ù–ï –ø–µ—Ä–µ–º–µ—â–∞—é—Ç—Å—è")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_file = "/tmp/cycle1_200units_2025_03_18_results.json"
    with open(output_file, "w") as f:
        json.dump({
            'stats': {
                'total': stats['total'],
                'processed': stats['processed'],
                'errors': stats['errors'],
                'files_remained': stats['files_remained'],
                'by_category': dict(stats['by_category']),
                'by_destination': {k: len(v) for k, v in stats['by_destination'].items()},
                'by_file_type': dict(stats['by_file_type']),
                'exceptions_details': {
                    'special': stats['exceptions_details']['special'],
                    'ambiguous': stats['exceptions_details']['ambiguous'],
                },
            },
            'results': results
        }, f, indent=2, ensure_ascii=False)

    print(f"\nüíæ –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")

    # –°–æ–∑–¥–∞—ë–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
    summary_file = "/tmp/cycle1_200units_2025_03_18_summary.txt"
    with open(summary_file, "w") as f:
        f.write("=" * 80 + "\n")
        f.write("–î–ï–¢–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê: CYCLE 1 TEST (200 UNITs –∏–∑ 2025-03-18)\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}/{stats['total']}\n")
        f.write(f"–û—à–∏–±–æ–∫: {stats['errors']}\n\n")
        f.write("–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï:\n")
        f.write(f"  ‚úÖ Merge/Direct:          {merge_count:4} ({(merge_count/stats['processed']*100):.1f}%)\n")
        f.write(f"  üîÑ Processing_1/Convert:  {processing_convert:4}\n")
        f.write(f"  üì¶ Processing_1/Extract:  {processing_extract:4}\n")
        f.write(f"  üîß Processing_1/Normalize:{processing_normalize:4}\n")
        f.write(f"  ‚ö†Ô∏è  Exceptions_1/Empty:    {exceptions_empty:4}\n")
        f.write(f"  ‚ö†Ô∏è  Exceptions_1/Special:  {exceptions_special:4}\n")
        f.write(f"  ‚ö†Ô∏è  Exceptions_1/Ambiguous:{exceptions_ambiguous:4}\n")
        f.write(f"\n  –í–°–ï–ì–û Exceptions: {exceptions_total:4} ({(exceptions_total/stats['processed']*100):.1f}%)\n")

    print(f"üìÑ –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞: {summary_file}")
    print("\n" + "=" * 80)

    return results, stats


if __name__ == "__main__":
    test_cycle1_200units_with_verification()
