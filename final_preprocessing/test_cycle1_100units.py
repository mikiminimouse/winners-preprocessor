#!/usr/bin/env python3
"""
–¢–µ—Å—Ç Cycle 1 –Ω–∞ 100 —Å–ª—É—á–∞–π–Ω—ã—Ö UNITs
"""
import sys
from pathlib import Path
import json
from collections import defaultdict

from docprep.engine.classifier import Classifier

def test_cycle1_100units():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞ 100 UNITs"""

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    classifier = Classifier()

    # –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ UNITs –¥–ª—è —Ç–µ—Å—Ç–∞
    with open("/tmp/test_100_units.txt", "r") as f:
        units = [line.strip() for line in f if line.strip()]

    input_dir = Path("/root/winners_preprocessor/final_preprocessing/Data/2025-03-04/Input")
    protocol_date = "2025-03-04"

    results = []
    stats = {
        'total': len(units),
        'processed': 0,
        'errors': 0,
        'by_category': defaultdict(int),
        'by_destination': defaultdict(list),
        'by_file_type': defaultdict(int),
    }

    print("=" * 80)
    print("–¢–ï–°–¢ CYCLE 1: 100 —Å–ª—É—á–∞–π–Ω—ã—Ö UNITs")
    print("=" * 80)
    print(f"\n–í—Å–µ–≥–æ UNITs: {len(units)}")
    print(f"Protocol Date: {protocol_date}")
    print("\n–û–±—Ä–∞–±–æ—Ç–∫–∞...")

    for i, unit_name in enumerate(units, 1):
        unit_path = input_dir / unit_name

        if not unit_path.exists():
            stats['errors'] += 1
            continue

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 UNITs
        if i % 10 == 0:
            print(f"  [{i}/{len(units)}] –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ...")

        try:
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º UNIT (dry_run=True)
            result = classifier.classify_unit(
                unit_path=unit_path,
                cycle=1,
                protocol_date=protocol_date,
                dry_run=True,
            )

            stats['processed'] += 1

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            category = result['unit_category']
            stats['by_category'][category] += 1

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º destination
            target = result['target_directory']
            if 'Merge' in target and 'Direct' in target:
                destination = "Merge/Direct"
            elif 'Processing' in target:
                if 'Convert' in target:
                    destination = "Processing_1/Convert"
                elif 'Extract' in target:
                    destination = "Processing_1/Extract"
                elif 'Normalize' in target:
                    destination = "Processing_1/Normalize"
                else:
                    destination = "Processing_1/Other"
            elif 'Exception' in target:
                if 'Empty' in target:
                    destination = "Exceptions_1/Empty"
                elif 'Special' in target:
                    destination = "Exceptions_1/Special"
                elif 'Ambiguous' in target:
                    destination = "Exceptions_1/Ambiguous"
                else:
                    destination = "Exceptions_1/Other"
            else:
                destination = "Unknown"

            stats['by_destination'][destination].append(unit_name)

            # –°–æ–±–∏—Ä–∞–µ–º —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤
            for fc in result.get('file_classifications', []):
                file_type = fc['classification'].get('detected_type', 'unknown')
                stats['by_file_type'][file_type] += 1

            results.append({
                'unit_name': unit_name,
                'category': category,
                'is_mixed': result['is_mixed'],
                'file_count': len(result.get('file_classifications', [])),
                'destination': destination,
                'target_directory': str(result['target_directory']),
            })

        except Exception as e:
            stats['errors'] += 1
            results.append({
                'unit_name': unit_name,
                'error': str(e),
            })

    # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print("\n" + "=" * 80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 80)

    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}/{stats['total']}")
    if stats['errors'] > 0:
        print(f"‚ùå –û—à–∏–±–æ–∫: {stats['errors']}")

    print("\n" + "‚îÄ" * 80)
    print("üìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú")
    print("‚îÄ" * 80)
    for category, count in sorted(stats['by_category'].items(), key=lambda x: -x[1]):
        percentage = (count / stats['processed']) * 100 if stats['processed'] > 0 else 0
        bar = "‚ñà" * int(percentage / 2)
        print(f"  {category:12} : {count:3} ({percentage:5.1f}%) {bar}")

    print("\n" + "‚îÄ" * 80)
    print("üìç –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ú–ê–†–®–†–£–¢–ê–ú (Web UI Cycle 1)")
    print("‚îÄ" * 80)

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º
    merge_count = len(stats['by_destination'].get('Merge/Direct', []))
    processing_convert = len(stats['by_destination'].get('Processing_1/Convert', []))
    processing_extract = len(stats['by_destination'].get('Processing_1/Extract', []))
    processing_normalize = len(stats['by_destination'].get('Processing_1/Normalize', []))
    exceptions_total = sum(len(v) for k, v in stats['by_destination'].items() if 'Exception' in k)

    processing_total = processing_convert + processing_extract + processing_normalize

    print(f"\n  üü¢ Merge_0/Direct (–≥–æ—Ç–æ–≤—ã –∫ Docling):")
    print(f"     {merge_count} UNITs ({(merge_count/stats['processed']*100):.1f}%)")

    print(f"\n  üîµ Processing_1 (—Ç—Ä–µ–±—É—é—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏):")
    print(f"     –í—Å–µ–≥–æ: {processing_total} UNITs ({(processing_total/stats['processed']*100):.1f}%)")
    print(f"       ‚Ä¢ Convert:   {processing_convert}")
    print(f"       ‚Ä¢ Extract:   {processing_extract}")
    print(f"       ‚Ä¢ Normalize: {processing_normalize}")

    print(f"\n  üî¥ Exceptions_1 (–∏—Å–∫–ª—é—á–µ–Ω–∏—è):")
    print(f"     {exceptions_total} UNITs ({(exceptions_total/stats['processed']*100):.1f}%)")
    for dest, units_list in sorted(stats['by_destination'].items()):
        if 'Exception' in dest:
            print(f"       ‚Ä¢ {dest.split('/')[-1]}: {len(units_list)}")

    print("\n" + "‚îÄ" * 80)
    print("üìÑ –¢–û–ü-10 –¢–ò–ü–û–í –§–ê–ô–õ–û–í")
    print("‚îÄ" * 80)
    top_types = sorted(stats['by_file_type'].items(), key=lambda x: -x[1])[:10]
    for file_type, count in top_types:
        print(f"  {file_type:20} : {count:3}")

    # –î–µ—Ç–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–ø–µ—Ä–≤—ã–µ 5)
    print("\n" + "‚îÄ" * 80)
    print("üìã –ü–†–ò–ú–ï–†–´ UNITS –ü–û –ù–ê–ü–†–ê–í–õ–ï–ù–ò–Ø–ú")
    print("‚îÄ" * 80)

    for dest in ['Merge/Direct', 'Processing_1/Convert', 'Processing_1/Extract', 'Exceptions_1/Empty']:
        units_list = stats['by_destination'].get(dest, [])
        if units_list:
            print(f"\n  {dest}: (–ø–æ–∫–∞–∑–∞–Ω–æ {min(5, len(units_list))} –∏–∑ {len(units_list)})")
            for unit in units_list[:5]:
                print(f"    - {unit}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_file = "/tmp/cycle1_100units_results.json"
    with open(output_file, "w") as f:
        json.dump({
            'stats': {
                'total': stats['total'],
                'processed': stats['processed'],
                'errors': stats['errors'],
                'by_category': dict(stats['by_category']),
                'by_destination': {k: len(v) for k, v in stats['by_destination'].items()},
                'by_file_type': dict(stats['by_file_type']),
            },
            'results': results
        }, f, indent=2, ensure_ascii=False)

    print(f"\nüíæ –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")

    # –°–æ–∑–¥–∞—ë–º –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á—ë—Ç
    summary_file = "/tmp/cycle1_100units_summary.txt"
    with open(summary_file, "w") as f:
        f.write("=" * 80 + "\n")
        f.write("–ö–†–ê–¢–ö–ê–Ø –°–í–û–î–ö–ê: CYCLE 1 TEST (100 UNITs)\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['processed']}/{stats['total']}\n")
        f.write(f"–û—à–∏–±–æ–∫: {stats['errors']}\n\n")
        f.write("–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï:\n")
        f.write(f"  ‚úÖ Merge/Direct:         {merge_count} ({(merge_count/stats['processed']*100):.1f}%)\n")
        f.write(f"  üîÑ Processing_1/Convert: {processing_convert}\n")
        f.write(f"  üì¶ Processing_1/Extract: {processing_extract}\n")
        f.write(f"  üîß Processing_1/Normalize: {processing_normalize}\n")
        f.write(f"  ‚ö†Ô∏è  Exceptions_1:         {exceptions_total} ({(exceptions_total/stats['processed']*100):.1f}%)\n")

    print(f"üìÑ –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞: {summary_file}")
    print("\n" + "=" * 80)

    return results, stats


if __name__ == "__main__":
    test_cycle1_100units()
