#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–∏–∫–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ WITHOUT merge2docling.

–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π —Ü–∏–∫–ª –æ—Ç–¥–µ–ª—å–Ω–æ:
- Cycle 1: Input -> Processing_1 -> Merge_1
- Cycle 2: Merge_1 -> Processing_2 -> Merge_2
- Cycle 3: Merge_2 -> Processing_3 -> Merge_3
- –ò —Ç–∞–∫ –¥–∞–ª–µ–µ, –ø–æ–∫–∞ –µ—Å—Ç—å —á—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å

–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ –≤ Processing –±–æ–ª—å—à–µ –Ω–µ—Ç units –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ü–∏–∫–ª–∞.
–ù–ï –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π merge2docling!
"""
import sys
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from docprep.core.config import (
    get_cycle_paths,
    get_processing_paths,
    get_data_paths,
    init_directory_structure,
)
from docprep.engine.classifier import Classifier
from docprep.engine.converter import Converter
from docprep.engine.extractor import Extractor
from docprep.engine.normalizers import NameNormalizer, ExtensionNormalizer
from docprep.utils.paths import find_all_units


def count_units_in_dir(dir_path: Path) -> int:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ UNIT –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π."""
    if not dir_path.exists():
        return 0
    return len(list(dir_path.rglob("UNIT_*")))


def get_cycle_statistics(cycle: int, data_paths: Dict[str, Path]) -> Dict[str, Any]:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ü–∏–∫–ª—É."""
    cycle_paths = get_cycle_paths(
        cycle, data_paths["processing"], data_paths["merge"], data_paths["exceptions"]
    )
    processing_paths = get_processing_paths(cycle, data_paths["processing"])

    stats = {
        "cycle": cycle,
        "processing": {},
        "merge": {},
        "exceptions": {},
    }

    # Processing
    for subdir_name, subdir_path in processing_paths.items():
        if subdir_path.exists():
            count = count_units_in_dir(subdir_path)
            if count > 0:
                stats["processing"][subdir_name] = count

    # –ù–û–í–ê–Ø –°–¢–†–£–ö–¢–£–†–ê v2: Merge/Processed_N/ –≤–º–µ—Å—Ç–æ Merge_N/
    merge_dir = cycle_paths["merge"]  # = Merge/Processed_N/
    if merge_dir.exists():
        for category in ["Converted", "Extracted", "Normalized", "Direct", "Mixed"]:
            category_dir = merge_dir / category
            if category_dir.exists():
                count = count_units_in_dir(category_dir)
                if count > 0:
                    stats["merge"][category] = count

    # –ù–û–í–ê–Ø –°–¢–†–£–ö–¢–£–†–ê v2: Merge/Direct/ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ü–∏–∫–ª–∞ (—Ñ–∞–π–ª—ã –≥–æ—Ç–æ–≤—ã–µ –Ω–∞–ø—Ä—è–º—É—é)
    if cycle == 1:
        direct_dir = data_paths["merge"] / "Direct"
        if direct_dir.exists():
            count = count_units_in_dir(direct_dir)
            if count > 0:
                stats["merge"]["Direct_from_Input"] = count

    # –ù–û–í–ê–Ø –°–¢–†–£–ö–¢–£–†–ê v2: Exceptions/Direct –∏ Exceptions/Processed_N
    # Exceptions/Processed_N –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ü–∏–∫–ª–∞
    exceptions_processed_dir = cycle_paths["exceptions"]  # = Exceptions/Processed_N
    if exceptions_processed_dir.exists():
        for subdir in exceptions_processed_dir.iterdir():
            if subdir.is_dir():
                count = count_units_in_dir(subdir)
                if count > 0:
                    stats["exceptions"][f"Processed_{cycle}/{subdir.name}"] = count

    # Exceptions/Direct –¥–ª—è —Ü–∏–∫–ª–∞ 1 (–∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏)
    if cycle == 1:
        exceptions_direct_dir = data_paths["exceptions"] / "Direct"
        if exceptions_direct_dir.exists():
            for subdir in exceptions_direct_dir.iterdir():
                if subdir.is_dir():
                    count = count_units_in_dir(subdir)
                    if count > 0:
                        stats["exceptions"][f"Direct/{subdir.name}"] = count

    return stats


def print_cycle_header(cycle: int):
    """–ü–µ—á–∞—Ç–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ü–∏–∫–ª–∞."""
    print("\n" + "=" * 80)
    print(f"üîÑ CYCLE {cycle}")
    print("=" * 80)


def print_statistics(stats: Dict[str, Any]):
    """–ü–µ—á–∞—Ç–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
    cycle = stats["cycle"]

    if stats["processing"]:
        print(f"\n‚öôÔ∏è  Processing_{cycle}:")
        total = sum(stats["processing"].values())
        print(f"   –í—Å–µ–≥–æ: {total} units")
        for name, count in stats["processing"].items():
            print(f"   - {name}: {count} units")

    if stats["merge"]:
        # –ù–û–í–ê–Ø –°–¢–†–£–ö–¢–£–†–ê v2: Merge/Processed_N/ –≤–º–µ—Å—Ç–æ Merge_N/
        print(f"\nüîÄ Merge/Processed_{cycle}:")
        total = sum(stats["merge"].values())
        print(f"   –í—Å–µ–≥–æ: {total} units")
        for name, count in stats["merge"].items():
            print(f"   - {name}: {count} units")

    if stats["exceptions"]:
        # –ù–û–í–ê–Ø –°–¢–†–£–ö–¢–£–†–ê v2: Exceptions/Direct –∏–ª–∏ Exceptions/Processed_N
        if cycle == 1:
            print(f"\n‚ö†Ô∏è  Exceptions/Direct & Exceptions/Processed_1:")
        else:
            print(f"\n‚ö†Ô∏è  Exceptions/Processed_{cycle}:")
        total = sum(stats["exceptions"].values())
        print(f"   –í—Å–µ–≥–æ: {total} units")
        for name, count in stats["exceptions"].items():
            print(f"   - {name}: {count} units")


def test_cycle_1(protocol_date: str, verbose: bool = False) -> Dict[str, Any]:
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç Cycle 1: Input -> Processing_1 -> Merge_1.

    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–∏–∫–ª—É
    """
    print_cycle_header(1)

    data_paths = get_data_paths(protocol_date)
    input_dir = data_paths["input"]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ input
    input_units = count_units_in_dir(input_dir)
    print(f"\nüì• Input: {input_units} units")

    if input_units == 0:
        print("‚ùå –ù–µ—Ç units –≤ Input –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏!")
        return {}

    start_time = time.time()

    # 1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    print(f"\nüìã –®–∞–≥ 1/4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
    classifier = Classifier()

    units = find_all_units(input_dir)
    processed = 0
    errors = 0

    for unit_path in units:
        try:
            result = classifier.classify_unit(
                unit_path, cycle=1, protocol_date=protocol_date, dry_run=False
            )
            processed += 1
            if verbose and processed % 100 == 0:
                print(f"   –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {processed}/{len(units)}")
        except Exception as e:
            errors += 1
            if verbose:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ {unit_path.name}: {e}")

    print(f"   ‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {processed} units, –æ—à–∏–±–æ–∫: {errors}")

    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    stats_after_classify = get_cycle_statistics(1, data_paths)

    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ Processing_1
    print(f"\n‚öôÔ∏è  –®–∞–≥ 2/4: –û–±—Ä–∞–±–æ—Ç–∫–∞ Processing_1")

    processing_paths = get_processing_paths(1, data_paths["processing"])
    converter = Converter()
    extractor = Extractor()
    name_normalizer = NameNormalizer()
    extension_normalizer = ExtensionNormalizer()

    # Convert
    convert_dir = processing_paths["Convert"]
    if convert_dir.exists():
        convert_units = find_all_units(convert_dir)
        if convert_units:
            print(f"   üîÑ Convert: {len(convert_units)} units")
            for unit_path in convert_units:
                try:
                    converter.convert_unit(
                        unit_path, cycle=1, protocol_date=protocol_date, dry_run=False
                    )
                except Exception as e:
                    if verbose:
                        print(f"      ‚ùå {unit_path.name}: {e}")

    # Extract
    extract_dir = processing_paths["Extract"]
    if extract_dir.exists():
        extract_units = find_all_units(extract_dir)
        if extract_units:
            print(f"   üì¶ Extract: {len(extract_units)} units")
            for unit_path in extract_units:
                try:
                    extractor.extract_unit(
                        unit_path, cycle=1, protocol_date=protocol_date, dry_run=False
                    )
                except Exception as e:
                    if verbose:
                        print(f"      ‚ùå {unit_path.name}: {e}")

    # Normalize
    normalize_dir = processing_paths["Normalize"]
    if normalize_dir.exists():
        normalize_units = find_all_units(normalize_dir)
        if normalize_units:
            print(f"   ‚ú® Normalize: {len(normalize_units)} units")
            for unit_path in normalize_units:
                try:
                    # Name normalization
                    name_normalizer.normalize_unit(
                        unit_path, cycle=1, protocol_date=protocol_date, dry_run=False
                    )
                    # Extension normalization
                    extension_normalizer.normalize_extensions(
                        unit_path, cycle=1, protocol_date=protocol_date, dry_run=False
                    )
                except Exception as e:
                    if verbose:
                        print(f"      ‚ùå {unit_path.name}: {e}")

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ - –≤—Å–µ –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
    print(f"\nüîç –®–∞–≥ 3/4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    remaining_in_processing = sum(
        count_units_in_dir(p) for p in processing_paths.values()
    )
    print(f"   –û—Å—Ç–∞–ª–æ—Å—å –≤ Processing_1: {remaining_in_processing} units")

    if remaining_in_processing > 0:
        print(f"   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ –≤—Å–µ units –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")

    # 4. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –®–∞–≥ 4/4: –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    stats_final = get_cycle_statistics(1, data_paths)
    print_statistics(stats_final)

    elapsed = time.time() - start_time
    print(f"\n‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è Cycle 1: {elapsed:.1f} —Å–µ–∫—É–Ω–¥")

    return stats_final


def test_cycle_n(cycle: int, protocol_date: str, verbose: bool = False) -> Dict[str, Any]:
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç Cycle N (2, 3, ...): Merge_{N-1} -> Processing_N -> Merge_N.

    Args:
        cycle: –ù–æ–º–µ—Ä —Ü–∏–∫–ª–∞ (2, 3, ...)
        protocol_date: –î–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
        verbose: –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥

    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–∏–∫–ª—É
    """
    print_cycle_header(cycle)

    data_paths = get_data_paths(protocol_date)

    # Input –¥–ª—è —ç—Ç–æ–≥–æ —Ü–∏–∫–ª–∞ = Merge_{cycle-1}
    prev_cycle_paths = get_cycle_paths(
        cycle - 1, data_paths["processing"], data_paths["merge"], data_paths["exceptions"]
    )
    input_dir = prev_cycle_paths["merge"]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ input
    input_units = count_units_in_dir(input_dir)
    print(f"\nüì• Input (Merge_{cycle-1}): {input_units} units")

    if input_units == 0:
        print(f"‚úÖ –ù–µ—Ç units –≤ Merge_{cycle-1}, —Ü–∏–∫–ª {cycle} –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è!")
        return {}

    start_time = time.time()

    # 1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    print(f"\nüìã –®–∞–≥ 1/4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
    classifier = Classifier()

    units = find_all_units(input_dir)
    processed = 0
    errors = 0

    for unit_path in units:
        try:
            result = classifier.classify_unit(
                unit_path, cycle=cycle, protocol_date=protocol_date, dry_run=False
            )
            processed += 1
            if verbose and processed % 50 == 0:
                print(f"   –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {processed}/{len(units)}")
        except Exception as e:
            errors += 1
            if verbose:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ {unit_path.name}: {e}")

    print(f"   ‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: {processed} units, –æ—à–∏–±–æ–∫: {errors}")

    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ Processing_N
    print(f"\n‚öôÔ∏è  –®–∞–≥ 2/4: –û–±—Ä–∞–±–æ—Ç–∫–∞ Processing_{cycle}")

    processing_paths = get_processing_paths(cycle, data_paths["processing"])
    converter = Converter()
    extractor = Extractor()
    name_normalizer = NameNormalizer()
    extension_normalizer = ExtensionNormalizer()

    # Convert
    convert_dir = processing_paths["Convert"]
    if convert_dir.exists():
        convert_units = find_all_units(convert_dir)
        if convert_units:
            print(f"   üîÑ Convert: {len(convert_units)} units")
            for unit_path in convert_units:
                try:
                    converter.convert_unit(
                        unit_path, cycle=cycle, protocol_date=protocol_date, dry_run=False
                    )
                except Exception as e:
                    if verbose:
                        print(f"      ‚ùå {unit_path.name}: {e}")

    # Extract
    extract_dir = processing_paths["Extract"]
    if extract_dir.exists():
        extract_units = find_all_units(extract_dir)
        if extract_units:
            print(f"   üì¶ Extract: {len(extract_units)} units")
            for unit_path in extract_units:
                try:
                    extractor.extract_unit(
                        unit_path, cycle=cycle, protocol_date=protocol_date, dry_run=False
                    )
                except Exception as e:
                    if verbose:
                        print(f"      ‚ùå {unit_path.name}: {e}")

    # Normalize
    normalize_dir = processing_paths["Normalize"]
    if normalize_dir.exists():
        normalize_units = find_all_units(normalize_dir)
        if normalize_units:
            print(f"   ‚ú® Normalize: {len(normalize_units)} units")
            for unit_path in normalize_units:
                try:
                    # Name normalization
                    name_normalizer.normalize_unit(
                        unit_path, cycle=cycle, protocol_date=protocol_date, dry_run=False
                    )
                    # Extension normalization
                    extension_normalizer.normalize_extensions(
                        unit_path, cycle=cycle, protocol_date=protocol_date, dry_run=False
                    )
                except Exception as e:
                    if verbose:
                        print(f"      ‚ùå {unit_path.name}: {e}")

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ - –≤—Å–µ –ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
    print(f"\nüîç –®–∞–≥ 3/4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    remaining_in_processing = sum(
        count_units_in_dir(p) for p in processing_paths.values()
    )
    print(f"   –û—Å—Ç–∞–ª–æ—Å—å –≤ Processing_{cycle}: {remaining_in_processing} units")

    if remaining_in_processing > 0:
        print(f"   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ –≤—Å–µ units –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")

    # 4. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –®–∞–≥ 4/4: –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    stats_final = get_cycle_statistics(cycle, data_paths)
    print_statistics(stats_final)

    elapsed = time.time() - start_time
    print(f"\n‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è Cycle {cycle}: {elapsed:.1f} —Å–µ–∫—É–Ω–¥")

    return stats_final


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö —Ü–∏–∫–ª–æ–≤."""
    import argparse

    parser = argparse.ArgumentParser(
        description="–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–∏–∫–ª–æ–≤ WITHOUT merge2docling"
    )
    parser.add_argument(
        "--date",
        type=str,
        default="2025-12-20",
        help="–î–∞—Ç–∞ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ (YYYY-MM-DD)",
    )
    parser.add_argument(
        "--max-cycles",
        type=int,
        default=3,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏–∫–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥",
    )
    parser.add_argument(
        "--start-from",
        type=int,
        default=1,
        help="–ù–∞—á–∞—Ç—å —Å —Ü–∏–∫–ª–∞ N (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1)",
    )

    args = parser.parse_args()

    print("=" * 80)
    print("üß™ –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –¶–ò–ö–õ–û–í")
    print("=" * 80)
    print(f"–î–∞—Ç–∞: {args.date}")
    print(f"–ú–∞–∫—Å–∏–º—É–º —Ü–∏–∫–ª–æ–≤: {args.max_cycles}")
    print(f"–ù–∞—á–∞–ª–æ —Å —Ü–∏–∫–ª–∞: {args.start_from}")
    print("=" * 80)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    init_directory_structure(date=args.date)

    all_stats = {}
    overall_start = time.time()

    # Cycle 1
    if args.start_from <= 1:
        stats_1 = test_cycle_1(args.date, verbose=args.verbose)
        all_stats[1] = stats_1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ü–∏–∫–ª
        data_paths = get_data_paths(args.date)
        # –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ v2: Processed_N –≤–º–µ—Å—Ç–æ Merge_N
        processed_1_units = count_units_in_dir(data_paths["merge"] / "Processed_1")

        if processed_1_units == 0:
            print("\n" + "=" * 80)
            print("‚úÖ Cycle 1 –∑–∞–≤–µ—Ä—à–µ–Ω. Processed_1 –ø—É—Å—Ç, –¥–∞–ª—å–Ω–µ–π—à–∏–µ —Ü–∏–∫–ª—ã –Ω–µ —Ç—Ä–µ–±—É—é—Ç—Å—è.")
            print("=" * 80)
            return

    # –¶–∏–∫–ª—ã 2+
    for cycle in range(max(2, args.start_from), args.max_cycles + 1):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å
        data_paths = get_data_paths(args.date)
        # –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ v2: Processed_N –≤–º–µ—Å—Ç–æ Merge_N
        prev_processed = data_paths["merge"] / f"Processed_{cycle-1}"
        prev_processed_units = count_units_in_dir(prev_processed)

        if prev_processed_units == 0:
            print("\n" + "=" * 80)
            print(f"‚úÖ Processed_{cycle-1} –ø—É—Å—Ç. Cycle {cycle} –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
            print("=" * 80)
            break

        stats_n = test_cycle_n(cycle, args.date, verbose=args.verbose)
        all_stats[cycle] = stats_n

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–Ω–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ v2: Processed_N)
        current_processed = data_paths["merge"] / f"Processed_{cycle}"
        current_processed_units = count_units_in_dir(current_processed)

        if current_processed_units == 0:
            print("\n" + "=" * 80)
            print(f"‚úÖ Cycle {cycle} –∑–∞–≤–µ—Ä—à–µ–Ω. Processed_{cycle} –ø—É—Å—Ç, –¥–∞–ª—å–Ω–µ–π—à–∏–µ —Ü–∏–∫–ª—ã –Ω–µ —Ç—Ä–µ–±—É—é—Ç—Å—è.")
            print("=" * 80)
            break

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
    overall_elapsed = time.time() - overall_start
    print("\n" + "=" * 80)
    print("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê")
    print("=" * 80)

    total_processed = 0
    total_exceptions = 0

    for cycle, stats in all_stats.items():
        if stats:
            merge_count = sum(stats.get("merge", {}).values())
            exceptions_count = sum(stats.get("exceptions", {}).values())
            total_processed += merge_count
            total_exceptions += exceptions_count

            print(f"\nCycle {cycle}:")
            print(f"  Merge: {merge_count} units")
            print(f"  Exceptions: {exceptions_count} units")

    print(f"\nüìà –ò—Ç–æ–≥–æ:")
    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed} units")
    print(f"  –ò—Å–∫–ª—é—á–µ–Ω–∏–π: {total_exceptions} units")
    print(f"  –û–±—â–µ–µ –≤—Ä–µ–º—è: {overall_elapsed:.1f} —Å–µ–∫—É–Ω–¥")

    print("\n" + "=" * 80)
    print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! Merge2Docling –ù–ï –∑–∞–ø—É—Å–∫–∞–ª—Å—è.")
    print("=" * 80)


if __name__ == "__main__":
    main()
